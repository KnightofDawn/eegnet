{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from six.moves import cPickle as pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import timeline\n",
    "\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"import done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load pickled dataset into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "name_pickle = './data/trainsh1.pickle'\n",
    "\n",
    "with open(name_pickle, 'rb') as f:\n",
    "    print('Unpickling ' + name_pickle)\n",
    "    load = pickle.load(f)\n",
    "    dataset = load['data']\n",
    "    labels = load['labels']\n",
    "    del load\n",
    "    print('dataset shape:', dataset.shape)\n",
    "    print('labels shape:', labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reformat data for training\n",
    "- Divide each file with 240000 samples into smaller batch_samples ~= size of receptive field of eegnet\n",
    "- Keep valid_dataset nr of samples intact for proper validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_normalize_data_labels(data, labels, sigma=1.0):\n",
    "    data_tmp = list()\n",
    "    labels_tmp = list()\n",
    "    for idx, d in enumerate(data):\n",
    "        if (np.count_nonzero(d) < 10) or (np.any(np.std(d, axis=0) < sigma)):\n",
    "            continue\n",
    "        # Normalize mean=0 and sigma=1: axis=0 is along columns, vertical lines.\n",
    "        d -= np.mean(d, axis=0) \n",
    "        d /= np.amax(np.absolute(d), axis=0)\n",
    "        data_tmp.append(d)\n",
    "        labels_tmp.append(labels[idx])\n",
    "    return np.asarray(data_tmp), np.asarray(labels_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Output size of the layer\n",
    "num_labels = 2\n",
    "\n",
    "#60% for train and 40% for validation\n",
    "split_idx = int(dataset.shape[0]*0.8)\n",
    "#nr of splits\n",
    "nrOfSplits = 300\n",
    "\n",
    "def format_data(data, labels, nr_splits):\n",
    "    shape = data.shape\n",
    "    # stack 3D array into 2D\n",
    "    data = np.reshape(data, (shape[0]*shape[1], shape[2]))\n",
    "    # 3D array from 2D array by splitting 2D array into the desired smaller chuncks\n",
    "    data = np.asarray(np.split(data, shape[0]*nr_splits, axis=0))\n",
    "    # labels are obtained by repeating original labels nr_splits times\n",
    "    labels = np.repeat((np.arange(num_labels) == labels[:,None]).astype(np.float32), nr_splits, axis=0)\n",
    "    # normalize and eliminate batches that only contain drop-outs\n",
    "    data, labels = clean_normalize_data_labels(data, labels, 0.01)\n",
    "    # data has to be 4D for tensorflow (insert an empty dimension)\n",
    "    data = data[:,None,:,:]\n",
    "    # shuffle data and labels mantaining relation between them\n",
    "    shuffle_idx = np.random.permutation(data.shape[0])\n",
    "    data = data[shuffle_idx,:,:,:]\n",
    "    labels = labels[shuffle_idx]\n",
    "    return data, labels\n",
    "\n",
    "# shuffle file data\n",
    "shuffle_idx = np.random.permutation(dataset.shape[0])\n",
    "dataset = dataset[shuffle_idx,:,:]\n",
    "labels = labels[shuffle_idx]\n",
    "# format and split data into smaller chunks\n",
    "train_dataset, train_labels = format_data(dataset[:split_idx], labels[:split_idx], nrOfSplits)\n",
    "valid_dataset, valid_labels = format_data(dataset[split_idx:-1], labels[split_idx:-1], nrOfSplits)\n",
    "del dataset, labels\n",
    "\n",
    "train_dataset = train_dataset[:,:,:,0]\n",
    "train_dataset = train_dataset[:,:,:,None]\n",
    "\n",
    "valid_dataset = valid_dataset[:200,:,:,0]\n",
    "valid_dataset = valid_dataset[:,:,:,None]\n",
    "valid_labels = valid_labels[:200]\n",
    "\n",
    "print('train_dataset shape:', train_dataset.shape, 'train_labels shape:', train_labels.shape, \n",
    "      'mix:', float(np.count_nonzero(train_labels[:,1], axis=0))/train_labels.shape[0])\n",
    "print('valid_dataset shape:', valid_dataset.shape, 'valid_labels shape:', valid_labels.shape, \n",
    "      'mix:', float(np.count_nonzero(valid_labels[:,1], axis=0))/valid_labels.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot some data to have an idea of how data looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.subplot(211)\n",
    "plt.plot(train_dataset[1,0,:,0])\n",
    "plt.plot(train_dataset[3,0,:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quantize signal\n",
    "\n",
    "Based on wavenet paper, raw signal is first mu-law transformed and then quantized to 256 possible values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "quantization_levels = 256\n",
    "\n",
    "def quantize_data(data, levels):\n",
    "    levels -= 1\n",
    "    # mu law companding transformation\n",
    "    data = np.sign(data) * (np.log(1.0 + levels * np.absolute(data)) / np.log(1.0 + levels))\n",
    "    # Normalize mean=0 and sigma=0.5: axis = 0 is along columns, vertical lines.\n",
    "    data -= np.mean(data, axis=0) \n",
    "    data /= np.amax(np.absolute(data), axis=0)\n",
    "    return data\n",
    "\n",
    "    ## Quantization to uint8: [(make data > 0) * levels] -> cast uin8\n",
    "    #data += np.absolute(np.amin(data, axis=0))\n",
    "    #data /= np.amax(data, axis=0)\n",
    "    #data = (data*levels).astype(np.uint8)\n",
    "    #return (data*levels).astype(np.uint8)\n",
    "\n",
    "train_dataset = quantize_data(train_dataset, quantization_levels)\n",
    "valid_dataset = quantize_data(valid_dataset, quantization_levels)\n",
    "\n",
    "print('train_dataset shape:', train_dataset.shape)\n",
    "print('valid_dataset shape:', valid_dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot data after quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.subplot(211)\n",
    "plt.plot(train_dataset[1,0,:,0])\n",
    "plt.plot(train_dataset[2,0,:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EEGNET implementation\n",
    "\n",
    "Ideas\n",
    "  - Condition classification based on sensor?\n",
    "  \n",
    "Part of https://arxiv.org/pdf/1609.03499.pdf that most concerns classification:\n",
    "\"As a last experiment we looked at speech recognition with WaveNets on the TIMIT (Garofolo et al., 1993) dataset. For this task we added a mean-pooling layer after the dilation convolutions that aggregated the activations to coarser frames spanning 10 milliseconds (160 x downsampling). The pooling layer was followed by a few non-causal convolutions. We trained WaveNet with two loss terms, one to predict the next sample and one to classify the frame, the model generalized better than with a single loss and achieved 18.8 PER on the test set, which is to our knowledge the best score obtained from a model trained directly on raw audio on TIMIT.\"\n",
    "\n",
    "Look into: http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43290.pdf\n",
    "\"Input: This layer extracts 275 ms waveform segments from each of M input microphones. Successive inputs are hopped by 10ms. At the 16kHz sampling rate used in our experiments each segment contains M X 4401 dimensions.\"\n",
    "...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#How many files are supplied per batch.\n",
    "batch_size=64\n",
    "#Number of samples in each batch entry\n",
    "batch_samples=train_dataset.shape[2]\n",
    "#How many filters to learn for the input.\n",
    "input_channels=1\n",
    "#How many filters to learn for the residual.\n",
    "residual_channels=16\n",
    "# size after pooling layer\n",
    "pool_size = 600\n",
    "#number of steps after which learning rate is decayed\n",
    "decay_steps=100\n",
    "\n",
    "filter_width=5\n",
    "\n",
    "slim = tf.contrib.slim\n",
    "\n",
    "#Construct computation graph\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    \n",
    "    # Input data\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, 1, batch_samples, input_channels))\n",
    "    tf_train_labels = tf.placeholder(tf.uint8, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset, dtype=tf.float32)\n",
    "    tf_valid_labels = tf.constant(valid_labels, dtype=tf.float32)\n",
    "    \n",
    "    def accuracy(logits, labels):\n",
    "        return tf.div(\n",
    "            tf.mul(\n",
    "                tf.to_float(\n",
    "                tf.reduce_sum(tf.to_int32(tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))))\n",
    "                ), \n",
    "                100), \n",
    "                tf.to_float(tf.shape(logits)[0]))\n",
    "    \n",
    "    def network(batch_data, reuse=False, is_training=True):\n",
    "        with tf.variable_scope('eegnet_network', reuse=reuse):\n",
    "            with slim.arg_scope([slim.batch_norm], \n",
    "                                is_training=is_training):\n",
    "                with slim.arg_scope([slim.conv2d, slim.fully_connected], \n",
    "                                    activation_fn=tf.nn.relu, \n",
    "                                    weights_initializer=slim.xavier_initializer(), \n",
    "                                    normalizer_fn=slim.batch_norm):\n",
    "                    with tf.variable_scope('input_layer'):\n",
    "                        hidden = slim.conv2d(batch_data, residual_channels, [1, filter_width], stride=1, rate=1, scope='conv1')\n",
    "                        skip = hidden\n",
    "\n",
    "                    with tf.variable_scope('hidden_layers'):\n",
    "                        hidden = slim.conv2d(hidden, residual_channels, [1, filter_width], stride=1, rate=2, scope='conv1')\n",
    "                        skip = tf.add(skip, hidden)\n",
    "                        hidden = slim.conv2d(hidden, residual_channels, [1, filter_width], stride=1, rate=4, scope='conv2')\n",
    "                        skip = tf.add(skip, hidden)\n",
    "                        \n",
    "                    with tf.variable_scope('skip_processing'):\n",
    "                        hidden = slim.conv2d(skip, residual_channels, 1, scope='1x1conv1')\n",
    "\n",
    "                    with tf.variable_scope('prediction'):\n",
    "                        predc = slim.conv2d(hidden, quantization_levels, 1, scope='1x1conv1')\n",
    "\n",
    "                    with tf.variable_scope('classification'):\n",
    "                        hidden = slim.conv2d(hidden, 4, 1, scope='1x1conv1')\n",
    "                        hidden = slim.avg_pool2d(hidden, [1, batch_samples*2//pool_size], [1, batch_samples//pool_size])\n",
    "                        shape = hidden.get_shape().as_list()\n",
    "                        hidden = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "                        classf = slim.fully_connected(hidden, num_labels, scope='fc1')\n",
    "        return predc, classf \n",
    "\n",
    "    with tf.name_scope('eegnet_handling'):\n",
    "        with tf.name_scope('network'):\n",
    "            predictions, classification = network(tf_train_dataset)\n",
    "        with tf.name_scope('classification_loss'):\n",
    "            loss_class = slim.losses.softmax_cross_entropy(classification, tf_train_labels, scope='classification_loss')\n",
    "            tf.scalar_summary('classification_loss', loss_class)\n",
    "        with tf.name_scope('prediction_loss'):\n",
    "            # remove last predicted point\n",
    "            predictions = tf.slice(predictions, [0, 0, 0, 0], [-1, -1, batch_samples - 1, -1])\n",
    "            predictions = tf.reshape(predictions, [batch_size * (batch_samples - 1), quantization_levels])\n",
    "            # remove first training sample\n",
    "            shift_data = tf.slice(tf_train_dataset, [0, 0, 1, 0], [-1, -1, -1, -1])\n",
    "            shift_data = tf.reshape(shift_data, [batch_size, (batch_samples - 1) * input_channels])\n",
    "            # quantization to uint8: [(make data > 0) * levels] -> cast uin8\n",
    "            #data += np.absolute(np.amin(data, axis=0))\n",
    "            #data /= np.amax(data, axis=0)\n",
    "            #data = (data*levels).astype(np.uint8)\n",
    "            shift_data = tf.add(shift_data, tf.abs(tf.reduce_min(shift_data, reduction_indices=0)))\n",
    "            shift_data = tf.div(shift_data, tf.reduce_max(shift_data, reduction_indices=0))\n",
    "            shift_data = tf.mul(shift_data, quantization_levels-1)\n",
    "            shift_data = tf.cast(shift_data, tf.uint8) #one hot requires integer indices\n",
    "            # one hot encoding training samples            \n",
    "            shift_data = tf.one_hot(shift_data, quantization_levels)\n",
    "            shift_data = tf.reshape(shift_data, [batch_size * (batch_samples - 1), quantization_levels])\n",
    "            # loss\n",
    "            loss_pred = slim.losses.softmax_cross_entropy(predictions, shift_data, scope='prediction_loss')\n",
    "            tf.scalar_summary('prediction_loss', loss_pred)\n",
    "        with tf.name_scope('total_loss'):\n",
    "            total_loss = slim.losses.get_total_loss(add_regularization_losses=False)\n",
    "            tf.scalar_summary('total_loss', total_loss)\n",
    "        with tf.name_scope('optimizer'):\n",
    "            global_step = tf.Variable(0)\n",
    "            learning_rate = tf.train.exponential_decay(1e-3, global_step, decay_steps, 0.96, staircase=True)\n",
    "            #optimizer = tf.train.RMSPropOptimizer(learning_rate, 0.9).minimize(loss, global_step=global_step)\n",
    "            optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(total_loss, global_step=global_step)\n",
    "            tf.scalar_summary('learning_rate', learning_rate)\n",
    "        with tf.name_scope('accuracy'):\n",
    "            train_predictions = tf.nn.softmax(classification)\n",
    "            valid_predictions = tf.nn.softmax(network(tf_valid_dataset, True, True)[1])\n",
    "            train_accuracy = accuracy(train_predictions, tf_train_labels)\n",
    "            valid_accuracy = accuracy(valid_predictions, tf_valid_labels)\n",
    "\n",
    "    # Add histograms for trainable variables.\n",
    "    for var in tf.trainable_variables():\n",
    "        tf.histogram_summary(var.op.name, var)\n",
    "        \n",
    "    # Add summaries for activations {working only for tf rev >= 0.11}\n",
    "    slim.summarize_activations()\n",
    "    \n",
    "    #Merge all summaries and write to a folder\n",
    "    merged = tf.merge_all_summaries()\n",
    "    results_writer = tf.train.SummaryWriter('./results', graph)\n",
    "    \n",
    "    # Add ops to save and restore all the variables.\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    #tracing for timeline\n",
    "    run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "    run_metadata = tf.RunMetadata()    \n",
    "    \n",
    "print('computational graph created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_steps = 4001\n",
    "\n",
    "trace_file = open('./tracing/timeline.json', 'w')\n",
    "save_path = './checkpoints/model.ckpt'\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    tf.initialize_local_variables().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_dataset.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels}\n",
    "        _, tloss, lrate, trpred, traccu, summary = session.run(\n",
    "            [optimizer, total_loss, learning_rate, train_predictions, train_accuracy, merged], \n",
    "            feed_dict=feed_dict, \n",
    "            options=run_options,\n",
    "            run_metadata=run_metadata)\n",
    "        results_writer.add_summary(summary, step)\n",
    "        if (step % 15 == 0):\n",
    "            print('Minibatch total loss at step %d: %f' % (step, tloss), 'Learning rate:', lrate)\n",
    "            print('Minibatch accuracy:', traccu)\n",
    "            print('Predictions | Labels:\\n', np.concatenate((trpred[:2], batch_labels[:2]), axis=1))\n",
    "        if (step % 20 == 0):\n",
    "            print('Validation accuracy:', valid_accuracy.eval())\n",
    "            \n",
    "    # Save tracing into disl\n",
    "    trace = timeline.Timeline(step_stats=run_metadata.step_stats)\n",
    "    trace_file.write(trace.generate_chrome_trace_format(show_memory=True))\n",
    "            \n",
    "    # Save the variables to disk.\n",
    "    saver.save(session, save_path)\n",
    "    print(\"Model saved in file: %s\" % save_path)\n",
    "            \n",
    "    results_writer.flush()\n",
    "    results_writer.close()\n",
    "\n",
    "    print('Finished training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "valid_batch_size = 1\n",
    "\n",
    "def accuracy_notpercent(predictions, labels):\n",
    "  return np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    saver.restore(session, save_path)\n",
    "    print('Model Loaded')\n",
    "    data_split = np.array_split(valid_dataset, valid_dataset.shape[0]//valid_batch_size, axis=0)\n",
    "    labels_split = np.array_split(valid_labels, valid_labels.shape[0]//valid_batch_size, axis=0)\n",
    "    correct_predictions = 0\n",
    "    for idx, batch_data in enumerate(data_split):\n",
    "        correct_predictions += accuracy_notpercent(\n",
    "            train_prediction.eval(feed_dict={tf_train_dataset: batch_data}), \n",
    "            labels_split[idx])\n",
    "        print('accuracy:', (100.0*correct_predictions)/((idx+1)*valid_batch_size))\n",
    "        \n",
    "        \n",
    "    print('Finished validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
