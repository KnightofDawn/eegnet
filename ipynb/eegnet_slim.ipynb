{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import done\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from six.moves import cPickle as pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import timeline\n",
    "\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"import done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load pickled dataset into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unpickling ./data/trainsh1.pickle\n",
      "dataset shape: (29, 240000, 16)\n",
      "labels shape: (29,)\n"
     ]
    }
   ],
   "source": [
    "name_pickle = './data/trainsh1.pickle'\n",
    "\n",
    "with open(name_pickle, 'rb') as f:\n",
    "    print('Unpickling ' + name_pickle)\n",
    "    load = pickle.load(f)\n",
    "    dataset = load['data']\n",
    "    labels = load['labels']\n",
    "    del load\n",
    "    print('dataset shape:', dataset.shape)\n",
    "    print('labels shape:', labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reformat data for training\n",
    "- Divide each file with 240000 samples into smaller batch_samples ~= size of receptive field of eegnet\n",
    "- Keep valid_dataset nr of samples intact for proper validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset shape: (1735, 1, 3000, 1) train_labels shape: (1735, 1) mix: 0.547550432277\n",
      "valid_dataset shape: (150, 1, 3000, 1) valid_labels shape: (150, 1) mix: 0.553333333333\n"
     ]
    }
   ],
   "source": [
    "#Output size of the layer\n",
    "num_labels = 1\n",
    "\n",
    "#60% for train and 40% for validation\n",
    "split_idx = int(dataset.shape[0]*0.8)\n",
    "#nr of splits\n",
    "nrOfSplits = 80\n",
    "\n",
    "def format_data(data, labels, nr_splits):\n",
    "    shape = data.shape\n",
    "    # stack 3D array into 2D\n",
    "    data = np.reshape(data, (shape[0]*shape[1], shape[2]))\n",
    "    # 3D array from 2D array by splitting 2D array into the desired smaller chuncks\n",
    "    data = np.asarray(np.split(data, shape[0]*nr_splits, axis=0))\n",
    "    # labels are obtaining by repeating original labels nr_splits times\n",
    "    labels = np.repeat((np.arange(num_labels) == labels[:,None]).astype(np.float32), nr_splits, axis=0)\n",
    "    # eliminate batches that only contain drop-outs\n",
    "    data_tmp = list()\n",
    "    labels_tmp = list()\n",
    "    for idx, d in enumerate(data):\n",
    "        if (np.count_nonzero(d) < 10) or (np.std(d) < 0.01):\n",
    "            continue\n",
    "        data_tmp.append(d)\n",
    "        labels_tmp.append(labels[idx])\n",
    "    data = np.asarray(data_tmp)\n",
    "    labels = np.asarray(labels_tmp)\n",
    "    # data has to be 4D for tensorflow (insert an empty dimension)\n",
    "    data = data[:,None,:,:]\n",
    "    # shuffle data and labels mantaining relation between them\n",
    "    shuffle_idx = np.random.permutation(data.shape[0])\n",
    "    data = data[shuffle_idx,:,:,:]\n",
    "    labels = labels[shuffle_idx]\n",
    "    return data, labels\n",
    "\n",
    "# shuffle file data\n",
    "shuffle_idx = np.random.permutation(dataset.shape[0])\n",
    "dataset = dataset[shuffle_idx,:,:]\n",
    "labels = labels[shuffle_idx]\n",
    "# format and split data into smaller chunks\n",
    "train_dataset, train_labels = format_data(dataset[:split_idx], labels[:split_idx], nrOfSplits)\n",
    "valid_dataset, valid_labels = format_data(dataset[split_idx:-1], labels[split_idx:-1], nrOfSplits)\n",
    "del dataset, labels\n",
    "\n",
    "train_dataset = train_dataset[:,:,:,0]\n",
    "train_dataset = train_dataset[:,:,:,None]\n",
    "\n",
    "valid_dataset = valid_dataset[:150,:,:,0]\n",
    "valid_dataset = valid_dataset[:,:,:,None]\n",
    "valid_labels = valid_labels[:150]\n",
    "\n",
    "print('train_dataset shape:', train_dataset.shape, 'train_labels shape:', train_labels.shape, \n",
    "      'mix:', float(np.count_nonzero(train_labels, axis=0))/train_labels.shape[0])\n",
    "print('valid_dataset shape:', valid_dataset.shape, 'valid_labels shape:', valid_labels.shape, \n",
    "      'mix:', float(np.count_nonzero(valid_labels, axis=0))/valid_labels.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot some data to have an idea of how data looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fd80cc89690>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAACGCAYAAADemTEBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFfEOdBmFe11hlu0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd80cc77710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAACGCAYAAADemTEBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nfMdxnFmCK3zHcZxZgit8x3GcWcL/AbmoPsakxRuaAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd80ac4ec50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplot(211)\n",
    "plt.plot(train_dataset[1,0,:,0])\n",
    "plt.plot(train_dataset[2,0,:,0])\n",
    "plt.plot(train_dataset[3,0,:,0])\n",
    "plt.plot(train_dataset[4,0,:,0])\n",
    "\n",
    "plt.figure(2)\n",
    "plt.subplot(211)\n",
    "plt.plot(valid_dataset[4,0,:,0])\n",
    "plt.plot(valid_dataset[5,0,:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EEGNET implementation\n",
    "\n",
    "Ideas\n",
    "  - Condition classification based on sensor?\n",
    "  \n",
    "Part of https://arxiv.org/pdf/1609.03499.pdf that most concerns classification:\n",
    "\"As a last experiment we looked at speech recognition with WaveNets on the TIMIT (Garofolo et al., 1993) dataset. For this task we added a mean-pooling layer after the dilation convolutions that aggregated the activations to coarser frames spanning 10 milliseconds (160 x downsampling). The pooling layer was followed by a few non-causal convolutions. We trained WaveNet with two loss terms, one to predict the next sample and one to classify the frame, the model generalized better than with a single loss and achieved 18.8 PER on the test set, which is to our knowledge the best score obtained from a model trained directly on raw audio on TIMIT.\"\n",
    "\n",
    "Look into: http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43290.pdf\n",
    "\"Input: This layer extracts 275 ms waveform segments from each of M input microphones. Successive inputs are hopped by 10ms. At the 16kHz sampling rate used in our experiments each segment contains M X 4401 dimensions.\"\n",
    "...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum((predictions > 0.5) == labels)\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computational graph created\n"
     ]
    }
   ],
   "source": [
    "#How many files are supplied per batch.\n",
    "batch_size=16\n",
    "#Number of samples in each batch entry\n",
    "batch_samples=train_dataset.shape[2]\n",
    "#How many filters to learn for the input.\n",
    "input_channels=1\n",
    "#How many filters to learn for the residual.\n",
    "residual_channels=16\n",
    "#Hidden layer size for fully connected layer\n",
    "hidden_size=64\n",
    "#number of steps after which learning rate is decayed\n",
    "decay_steps=150\n",
    "# size after pooling layer\n",
    "pool_size = 600\n",
    "\n",
    "filter_width=5\n",
    "\n",
    "slim = tf.contrib.slim\n",
    "\n",
    "#Construct computation graph\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    \n",
    "    # Input data\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, 1, train_dataset.shape[2], train_dataset.shape[3]))\n",
    "    tf_train_labels = tf.placeholder(tf.int32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "            \n",
    "    def network(batch_data, reuse=False):\n",
    "        with tf.variable_scope('eegnet', reuse=reuse):\n",
    "            with slim.arg_scope([slim.conv2d, slim.fully_connected], \n",
    "                                activation_fn=tf.nn.elu, \n",
    "                                weights_initializer=tf.contrib.layers.xavier_initializer()):\n",
    "                with tf.variable_scope('input_layer'):\n",
    "                    hidden = slim.conv2d(batch_data, residual_channels, [1, filter_width], stride=1, rate=1, scope='conv1')\n",
    "                    skip = hidden\n",
    "\n",
    "                with tf.variable_scope('hidden_layers'):\n",
    "                    hidden = slim.conv2d(hidden, residual_channels, [1, filter_width], stride=1, rate=2, scope='conv1')\n",
    "                    skip = tf.add(skip, hidden)\n",
    "                    hidden = slim.conv2d(hidden, residual_channels, [1, filter_width], stride=1, rate=4, scope='conv2')\n",
    "                    skip = tf.add(skip, hidden)\n",
    "                    # Residual connection\n",
    "                    #hidden = tf.add(hidden, batch_data)\n",
    "                    \n",
    "                with tf.variable_scope('skip_processing'):\n",
    "                    hidden = slim.conv2d(skip, residual_channels, [1, 1], stride=1, rate=1, scope='conv1')\n",
    "                    hidden = slim.conv2d(hidden, 4, [1, 1], stride=1, rate=1, scope='conv2')\n",
    "\n",
    "                with tf.variable_scope('output'):\n",
    "                    hidden = slim.avg_pool2d(hidden, [1, batch_samples*2//pool_size], [1, batch_samples//pool_size])\n",
    "                    shape = hidden.get_shape().as_list()\n",
    "                    hidden = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "                    hidden = slim.fully_connected(hidden, hidden_size, scope='fc1')\n",
    "                    return slim.fully_connected(hidden, num_labels, scope='fc2')\n",
    "\n",
    "    with tf.name_scope('eegnet'):\n",
    "        logits = network(tf_train_dataset)\n",
    "        with tf.name_scope('loss'):\n",
    "            loss = slim.losses.sigmoid_cross_entropy(logits, tf_train_labels)\n",
    "            tf.scalar_summary('loss', loss)\n",
    "        with tf.name_scope('optimizer'):\n",
    "            global_step = tf.Variable(0)\n",
    "            learning_rate = tf.train.exponential_decay(1e-2, global_step, decay_steps, 0.96, staircase=True)\n",
    "            #optimizer = tf.train.RMSPropOptimizer(learning_rate, 0.9).minimize(loss, global_step=global_step)\n",
    "            optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "            tf.scalar_summary('learning_rate', learning_rate)\n",
    "        with tf.name_scope('accuracy'):\n",
    "            #train_accuracy, _ = slim.metrics.streaming_auc(logits, tf_train_labels)\n",
    "            train_predictions = tf.nn.sigmoid(logits)\n",
    "            valid_predictions = tf.nn.sigmoid(network(tf_valid_dataset, True))\n",
    "\n",
    "    # Add histograms for trainable variables.\n",
    "    for var in tf.trainable_variables():\n",
    "        tf.histogram_summary(var.op.name, var)\n",
    "    \n",
    "    #Merge all summaries and write to a folder\n",
    "    merged = tf.merge_all_summaries()\n",
    "    results_writer = tf.train.SummaryWriter('./results', graph)\n",
    "    \n",
    "    # Add ops to save and restore all the variables.\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    #tracing for timeline\n",
    "    run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "    run_metadata = tf.RunMetadata()    \n",
    "    \n",
    "print('computational graph created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 0.670082 Learning rate: 0.01\n",
      "Minibatch accuracy: 62.5\n",
      "Minibatch accuracy: 56.6666666667\n",
      "Predictions | Labels:\n",
      " [[ 0.46465212  1.        ]\n",
      " [ 0.47929481  0.        ]]\n",
      "Minibatch loss at step 15: 0.738399 Learning rate: 0.01\n",
      "Minibatch accuracy: 62.5\n",
      "Minibatch accuracy: 54.6666666667\n",
      "Predictions | Labels:\n",
      " [[ 0.5221132   0.        ]\n",
      " [ 0.42920274  0.        ]]\n",
      "Minibatch loss at step 30: 0.733485 Learning rate: 0.01\n",
      "Minibatch accuracy: 43.75\n",
      "Minibatch accuracy: 56.0\n",
      "Predictions | Labels:\n",
      " [[ 0.60366935  1.        ]\n",
      " [ 0.46171281  0.        ]]\n",
      "Minibatch loss at step 45: 0.683214 Learning rate: 0.01\n",
      "Minibatch accuracy: 62.5\n",
      "Minibatch accuracy: 54.6666666667\n",
      "Predictions | Labels:\n",
      " [[ 0.63184416  1.        ]\n",
      " [ 0.46751934  1.        ]]\n",
      "Minibatch loss at step 60: 0.725151 Learning rate: 0.01\n",
      "Minibatch accuracy: 56.25\n",
      "Minibatch accuracy: 54.0\n",
      "Predictions | Labels:\n",
      " [[ 0.51556563  1.        ]\n",
      " [ 0.63107693  1.        ]]\n",
      "Minibatch loss at step 75: 0.648149 Learning rate: 0.01\n",
      "Minibatch accuracy: 56.25\n",
      "Minibatch accuracy: 53.3333333333\n",
      "Predictions | Labels:\n",
      " [[ 0.61969054  0.        ]\n",
      " [ 0.50608921  1.        ]]\n",
      "Minibatch loss at step 90: 0.710216 Learning rate: 0.01\n",
      "Minibatch accuracy: 43.75\n",
      "Minibatch accuracy: 54.0\n",
      "Predictions | Labels:\n",
      " [[ 0.55714524  0.        ]\n",
      " [ 0.57658452  1.        ]]\n",
      "Minibatch loss at step 105: 0.642405 Learning rate: 0.01\n",
      "Minibatch accuracy: 62.5\n",
      "Minibatch accuracy: 54.0\n",
      "Predictions | Labels:\n",
      " [[ 0.57208246  1.        ]\n",
      " [ 0.47598758  1.        ]]\n",
      "Minibatch loss at step 120: 0.671629 Learning rate: 0.01\n",
      "Minibatch accuracy: 56.25\n",
      "Minibatch accuracy: 54.0\n",
      "Predictions | Labels:\n",
      " [[ 0.48983091  1.        ]\n",
      " [ 0.46273729  0.        ]]\n",
      "Minibatch loss at step 135: 0.733121 Learning rate: 0.01\n",
      "Minibatch accuracy: 43.75\n",
      "Minibatch accuracy: 56.6666666667\n",
      "Predictions | Labels:\n",
      " [[ 0.43533769  0.        ]\n",
      " [ 0.45173118  0.        ]]\n",
      "Minibatch loss at step 150: 0.689305 Learning rate: 0.0096\n",
      "Minibatch accuracy: 56.25\n",
      "Minibatch accuracy: 57.3333333333\n",
      "Predictions | Labels:\n",
      " [[ 0.47094584  1.        ]\n",
      " [ 0.69080794  0.        ]]\n",
      "Minibatch loss at step 165: 0.768576 Learning rate: 0.0096\n",
      "Minibatch accuracy: 18.75\n",
      "Minibatch accuracy: 55.3333333333\n",
      "Predictions | Labels:\n",
      " [[ 0.53642023  0.        ]\n",
      " [ 0.50196689  0.        ]]\n",
      "Minibatch loss at step 180: 0.723807 Learning rate: 0.0096\n",
      "Minibatch accuracy: 43.75\n",
      "Minibatch accuracy: 52.6666666667\n",
      "Predictions | Labels:\n",
      " [[ 0.4717496   0.        ]\n",
      " [ 0.53654909  1.        ]]\n",
      "Minibatch loss at step 195: 0.707338 Learning rate: 0.0096\n",
      "Minibatch accuracy: 62.5\n",
      "Minibatch accuracy: 52.0\n",
      "Predictions | Labels:\n",
      " [[ 0.77074987  0.        ]\n",
      " [ 0.42440099  1.        ]]\n",
      "Minibatch loss at step 210: 0.722222 Learning rate: 0.0096\n",
      "Minibatch accuracy: 43.75\n",
      "Minibatch accuracy: 52.6666666667\n",
      "Predictions | Labels:\n",
      " [[ 0.56829691  1.        ]\n",
      " [ 0.46727651  1.        ]]\n",
      "Minibatch loss at step 225: 0.633153 Learning rate: 0.0096\n",
      "Minibatch accuracy: 56.25\n",
      "Minibatch accuracy: 56.0\n",
      "Predictions | Labels:\n",
      " [[ 0.57289833  0.        ]\n",
      " [ 0.50611317  0.        ]]\n",
      "Minibatch loss at step 240: 0.723600 Learning rate: 0.0096\n",
      "Minibatch accuracy: 37.5\n",
      "Minibatch accuracy: 56.0\n",
      "Predictions | Labels:\n",
      " [[ 0.65611291  0.        ]\n",
      " [ 0.43410048  0.        ]]\n",
      "Minibatch loss at step 255: 0.579596 Learning rate: 0.0096\n",
      "Minibatch accuracy: 75.0\n",
      "Minibatch accuracy: 59.3333333333\n",
      "Predictions | Labels:\n",
      " [[ 0.70082027  1.        ]\n",
      " [ 0.64240104  1.        ]]\n",
      "Minibatch loss at step 270: 0.586115 Learning rate: 0.0096\n",
      "Minibatch accuracy: 62.5\n",
      "Minibatch accuracy: 60.0\n",
      "Predictions | Labels:\n",
      " [[ 0.69576073  1.        ]\n",
      " [ 0.48274645  0.        ]]\n",
      "Minibatch loss at step 285: 0.699284 Learning rate: 0.0096\n",
      "Minibatch accuracy: 68.75\n",
      "Minibatch accuracy: 54.6666666667\n",
      "Predictions | Labels:\n",
      " [[ 0.72220898  0.        ]\n",
      " [ 0.6377421   1.        ]]\n",
      "Minibatch loss at step 300: 0.687361 Learning rate: 0.009216\n",
      "Minibatch accuracy: 50.0\n",
      "Minibatch accuracy: 54.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5703302   1.        ]\n",
      " [ 0.47584781  1.        ]]\n",
      "Minibatch loss at step 315: 0.637336 Learning rate: 0.009216\n",
      "Minibatch accuracy: 62.5\n",
      "Minibatch accuracy: 54.0\n",
      "Predictions | Labels:\n",
      " [[ 0.57277954  1.        ]\n",
      " [ 0.42794836  0.        ]]\n",
      "Minibatch loss at step 330: 0.743633 Learning rate: 0.009216\n",
      "Minibatch accuracy: 43.75\n",
      "Minibatch accuracy: 51.3333333333\n",
      "Predictions | Labels:\n",
      " [[ 0.6673584   0.        ]\n",
      " [ 0.48918495  0.        ]]\n",
      "Minibatch loss at step 345: 0.585064 Learning rate: 0.009216\n",
      "Minibatch accuracy: 68.75\n",
      "Minibatch accuracy: 54.0\n",
      "Predictions | Labels:\n",
      " [[ 0.59778821  1.        ]\n",
      " [ 0.47711372  1.        ]]\n",
      "Minibatch loss at step 360: 0.631030 Learning rate: 0.009216\n",
      "Minibatch accuracy: 56.25\n",
      "Minibatch accuracy: 59.3333333333\n",
      "Predictions | Labels:\n",
      " [[ 0.65262318  1.        ]\n",
      " [ 0.72226799  1.        ]]\n",
      "Minibatch loss at step 375: 0.653965 Learning rate: 0.009216\n",
      "Minibatch accuracy: 62.5\n",
      "Minibatch accuracy: 60.0\n",
      "Predictions | Labels:\n",
      " [[ 0.53979486  0.        ]\n",
      " [ 0.54876649  0.        ]]\n",
      "Minibatch loss at step 390: 0.703512 Learning rate: 0.009216\n",
      "Minibatch accuracy: 50.0\n",
      "Minibatch accuracy: 57.3333333333\n",
      "Predictions | Labels:\n",
      " [[ 0.57561123  1.        ]\n",
      " [ 0.64710164  0.        ]]\n",
      "Minibatch loss at step 405: 0.631627 Learning rate: 0.009216\n",
      "Minibatch accuracy: 62.5\n",
      "Minibatch accuracy: 54.6666666667\n",
      "Predictions | Labels:\n",
      " [[ 0.51306504  0.        ]\n",
      " [ 0.51889294  0.        ]]\n",
      "Minibatch loss at step 420: 0.733865 Learning rate: 0.009216\n",
      "Minibatch accuracy: 43.75\n",
      "Minibatch accuracy: 56.0\n",
      "Predictions | Labels:\n",
      " [[ 0.49127176  1.        ]\n",
      " [ 0.55195481  0.        ]]\n",
      "Minibatch loss at step 435: 0.647341 Learning rate: 0.009216\n",
      "Minibatch accuracy: 56.25\n",
      "Minibatch accuracy: 55.3333333333\n",
      "Predictions | Labels:\n",
      " [[ 0.49650142  0.        ]\n",
      " [ 0.43001232  0.        ]]\n",
      "Minibatch loss at step 450: 0.694779 Learning rate: 0.00884736\n",
      "Minibatch accuracy: 56.25\n",
      "Minibatch accuracy: 57.3333333333\n",
      "Predictions | Labels:\n",
      " [[ 0.52369463  0.        ]\n",
      " [ 0.43697366  1.        ]]\n",
      "Minibatch loss at step 465: 0.604092 Learning rate: 0.00884736\n",
      "Minibatch accuracy: 50.0\n",
      "Minibatch accuracy: 56.0\n",
      "Predictions | Labels:\n",
      " [[ 0.75622147  1.        ]\n",
      " [ 0.71575415  1.        ]]\n",
      "Minibatch loss at step 480: 0.637152 Learning rate: 0.00884736\n",
      "Minibatch accuracy: 62.5\n",
      "Minibatch accuracy: 60.0\n",
      "Predictions | Labels:\n",
      " [[ 0.68586171  1.        ]\n",
      " [ 0.57673675  1.        ]]\n",
      "Minibatch loss at step 495: 0.551341 Learning rate: 0.00884736\n",
      "Minibatch accuracy: 75.0\n",
      "Minibatch accuracy: 56.6666666667\n",
      "Predictions | Labels:\n",
      " [[ 0.43190911  0.        ]\n",
      " [ 0.54374063  1.        ]]\n",
      "Minibatch loss at step 510: 0.650879 Learning rate: 0.00884736\n",
      "Minibatch accuracy: 56.25\n",
      "Minibatch accuracy: 59.3333333333\n",
      "Predictions | Labels:\n",
      " [[ 0.61912131  0.        ]\n",
      " [ 0.55322915  1.        ]]\n",
      "Minibatch loss at step 525: 0.570138 Learning rate: 0.00884736\n",
      "Minibatch accuracy: 81.25\n",
      "Minibatch accuracy: 59.3333333333\n",
      "Predictions | Labels:\n",
      " [[ 0.58908814  1.        ]\n",
      " [ 0.41504413  0.        ]]\n",
      "Minibatch loss at step 540: 0.630839 Learning rate: 0.00884736\n",
      "Minibatch accuracy: 62.5\n",
      "Minibatch accuracy: 59.3333333333\n",
      "Predictions | Labels:\n",
      " [[ 0.69580656  1.        ]\n",
      " [ 0.47211403  1.        ]]\n",
      "Minibatch loss at step 555: 0.621113 Learning rate: 0.00884736\n",
      "Minibatch accuracy: 62.5\n",
      "Minibatch accuracy: 60.0\n",
      "Predictions | Labels:\n",
      " [[ 0.43533668  0.        ]\n",
      " [ 0.70945448  1.        ]]\n",
      "Minibatch loss at step 570: 0.559935 Learning rate: 0.00884736\n",
      "Minibatch accuracy: 81.25\n",
      "Minibatch accuracy: 56.0\n",
      "Predictions | Labels:\n",
      " [[ 0.59136814  1.        ]\n",
      " [ 0.58115345  1.        ]]\n",
      "Minibatch loss at step 585: 0.521848 Learning rate: 0.00884736\n",
      "Minibatch accuracy: 68.75\n",
      "Minibatch accuracy: 58.6666666667\n",
      "Predictions | Labels:\n",
      " [[ 0.61658484  0.        ]\n",
      " [ 0.75828278  1.        ]]\n",
      "Minibatch loss at step 600: 0.773757 Learning rate: 0.00849347\n",
      "Minibatch accuracy: 43.75\n",
      "Minibatch accuracy: 56.6666666667\n",
      "Predictions | Labels:\n",
      " [[ 0.32550809  0.        ]\n",
      " [ 0.65831006  0.        ]]\n",
      "Minibatch loss at step 615: 0.801651 Learning rate: 0.00849347\n",
      "Minibatch accuracy: 50.0\n",
      "Minibatch accuracy: 58.6666666667\n",
      "Predictions | Labels:\n",
      " [[ 0.31345227  0.        ]\n",
      " [ 0.39852443  0.        ]]\n",
      "Minibatch loss at step 630: 0.685807 Learning rate: 0.00849347\n",
      "Minibatch accuracy: 62.5\n",
      "Minibatch accuracy: 60.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5247364   1.        ]\n",
      " [ 0.37360364  1.        ]]\n",
      "Minibatch loss at step 645: 0.671797 Learning rate: 0.00849347\n",
      "Minibatch accuracy: 68.75\n",
      "Minibatch accuracy: 58.6666666667\n",
      "Predictions | Labels:\n",
      " [[ 0.75652039  1.        ]\n",
      " [ 0.61671835  1.        ]]\n",
      "Minibatch loss at step 660: 0.453886 Learning rate: 0.00849347\n",
      "Minibatch accuracy: 93.75\n",
      "Minibatch accuracy: 59.3333333333\n",
      "Predictions | Labels:\n",
      " [[ 0.74632907  1.        ]\n",
      " [ 0.63090324  1.        ]]\n",
      "Minibatch loss at step 675: 0.536531 Learning rate: 0.00849347\n",
      "Minibatch accuracy: 81.25\n",
      "Minibatch accuracy: 56.0\n",
      "Predictions | Labels:\n",
      " [[ 0.64380503  1.        ]\n",
      " [ 0.55103385  1.        ]]\n",
      "Minibatch loss at step 690: 0.568420 Learning rate: 0.00849347\n",
      "Minibatch accuracy: 62.5\n",
      "Minibatch accuracy: 57.3333333333\n",
      "Predictions | Labels:\n",
      " [[ 0.73518574  1.        ]\n",
      " [ 0.54225641  0.        ]]\n",
      "Minibatch loss at step 705: 0.500683 Learning rate: 0.00849347\n",
      "Minibatch accuracy: 62.5\n",
      "Minibatch accuracy: 57.3333333333\n",
      "Predictions | Labels:\n",
      " [[ 0.74557656  1.        ]\n",
      " [ 0.53254271  0.        ]]\n",
      "Minibatch loss at step 720: 0.554585 Learning rate: 0.00849347\n",
      "Minibatch accuracy: 68.75\n",
      "Minibatch accuracy: 58.6666666667\n",
      "Predictions | Labels:\n",
      " [[ 0.52770877  0.        ]\n",
      " [ 0.33452052  0.        ]]\n",
      "Minibatch loss at step 735: 0.669666 Learning rate: 0.00849347\n",
      "Minibatch accuracy: 56.25\n",
      "Minibatch accuracy: 58.0\n",
      "Predictions | Labels:\n",
      " [[ 0.54760772  0.        ]\n",
      " [ 0.49827671  0.        ]]\n",
      "Minibatch loss at step 750: 0.477052 Learning rate: 0.00815373\n",
      "Minibatch accuracy: 87.5\n",
      "Minibatch accuracy: 59.3333333333\n",
      "Predictions | Labels:\n",
      " [[ 0.34980103  0.        ]\n",
      " [ 0.70715034  1.        ]]\n",
      "Minibatch loss at step 765: 0.607541 Learning rate: 0.00815373\n",
      "Minibatch accuracy: 62.5\n",
      "Minibatch accuracy: 57.3333333333\n",
      "Predictions | Labels:\n",
      " [[ 0.66812307  1.        ]\n",
      " [ 0.52995908  1.        ]]\n",
      "Minibatch loss at step 780: 0.637491 Learning rate: 0.00815373\n",
      "Minibatch accuracy: 62.5\n",
      "Minibatch accuracy: 56.0\n",
      "Predictions | Labels:\n",
      " [[ 0.64021993  1.        ]\n",
      " [ 0.61018729  0.        ]]\n",
      "Minibatch loss at step 795: 0.536969 Learning rate: 0.00815373\n",
      "Minibatch accuracy: 81.25\n",
      "Minibatch accuracy: 57.3333333333\n",
      "Predictions | Labels:\n",
      " [[ 0.36603037  0.        ]\n",
      " [ 0.55442172  1.        ]]\n",
      "Minibatch loss at step 810: 0.567247 Learning rate: 0.00815373\n",
      "Minibatch accuracy: 68.75\n",
      "Minibatch accuracy: 58.6666666667\n",
      "Predictions | Labels:\n",
      " [[ 0.37595126  0.        ]\n",
      " [ 0.79205096  1.        ]]\n",
      "Minibatch loss at step 825: 0.599434 Learning rate: 0.00815373\n",
      "Minibatch accuracy: 75.0\n",
      "Minibatch accuracy: 58.6666666667\n",
      "Predictions | Labels:\n",
      " [[ 0.34341654  1.        ]\n",
      " [ 0.42568213  0.        ]]\n",
      "Minibatch loss at step 840: 0.556365 Learning rate: 0.00815373\n",
      "Minibatch accuracy: 75.0\n",
      "Minibatch accuracy: 56.0\n",
      "Predictions | Labels:\n",
      " [[ 0.4695895   1.        ]\n",
      " [ 0.40461013  0.        ]]\n",
      "Minibatch loss at step 855: 0.652627 Learning rate: 0.00815373\n",
      "Minibatch accuracy: 68.75\n",
      "Minibatch accuracy: 58.0\n",
      "Predictions | Labels:\n",
      " [[ 0.59907025  1.        ]\n",
      " [ 0.53702837  0.        ]]\n",
      "Minibatch loss at step 870: 0.578859 Learning rate: 0.00815373\n",
      "Minibatch accuracy: 62.5\n",
      "Minibatch accuracy: 58.0\n",
      "Predictions | Labels:\n",
      " [[ 0.52545911  0.        ]\n",
      " [ 0.40667453  0.        ]]\n",
      "Minibatch loss at step 885: 0.664334 Learning rate: 0.00815373\n",
      "Minibatch accuracy: 75.0\n",
      "Minibatch accuracy: 55.3333333333\n",
      "Predictions | Labels:\n",
      " [[ 0.45434821  1.        ]\n",
      " [ 0.86236972  1.        ]]\n",
      "Minibatch loss at step 900: 0.526309 Learning rate: 0.00782758\n",
      "Minibatch accuracy: 81.25\n",
      "Minibatch accuracy: 56.6666666667\n",
      "Predictions | Labels:\n",
      " [[ 0.58777457  1.        ]\n",
      " [ 0.53297108  1.        ]]\n",
      "Minibatch loss at step 915: 0.620802 Learning rate: 0.00782758\n",
      "Minibatch accuracy: 68.75\n",
      "Minibatch accuracy: 58.0\n",
      "Predictions | Labels:\n",
      " [[ 0.39582106  0.        ]\n",
      " [ 0.66409022  1.        ]]\n",
      "Minibatch loss at step 930: 0.606749 Learning rate: 0.00782758\n",
      "Minibatch accuracy: 68.75\n",
      "Minibatch accuracy: 58.0\n",
      "Predictions | Labels:\n",
      " [[ 0.46963987  0.        ]\n",
      " [ 0.8451488   1.        ]]\n",
      "Minibatch loss at step 945: 0.549888 Learning rate: 0.00782758\n",
      "Minibatch accuracy: 62.5\n",
      "Minibatch accuracy: 56.6666666667\n",
      "Predictions | Labels:\n",
      " [[ 0.53918403  0.        ]\n",
      " [ 0.88537431  1.        ]]\n",
      "Minibatch loss at step 960: 0.540592 Learning rate: 0.00782758\n",
      "Minibatch accuracy: 81.25\n",
      "Minibatch accuracy: 58.0\n",
      "Predictions | Labels:\n",
      " [[ 0.28916371  1.        ]\n",
      " [ 0.30791426  0.        ]]\n",
      "Minibatch loss at step 975: 0.524291 Learning rate: 0.00782758\n",
      "Minibatch accuracy: 68.75\n",
      "Minibatch accuracy: 58.6666666667\n",
      "Predictions | Labels:\n",
      " [[ 0.63379717  0.        ]\n",
      " [ 0.49348626  1.        ]]\n",
      "Minibatch loss at step 990: 0.497839 Learning rate: 0.00782758\n",
      "Minibatch accuracy: 75.0\n",
      "Minibatch accuracy: 57.3333333333\n",
      "Predictions | Labels:\n",
      " [[ 0.60654742  1.        ]\n",
      " [ 0.8773216   1.        ]]\n",
      "Minibatch loss at step 1005: 0.506589 Learning rate: 0.00782758\n",
      "Minibatch accuracy: 75.0\n",
      "Minibatch accuracy: 57.3333333333\n",
      "Predictions | Labels:\n",
      " [[ 0.31544176  0.        ]\n",
      " [ 0.59744328  1.        ]]\n",
      "Minibatch loss at step 1020: 0.594920 Learning rate: 0.00782758\n",
      "Minibatch accuracy: 56.25\n",
      "Minibatch accuracy: 57.3333333333\n",
      "Predictions | Labels:\n",
      " [[ 0.85612488  1.        ]\n",
      " [ 0.83902568  1.        ]]\n",
      "Minibatch loss at step 1035: 0.553747 Learning rate: 0.00782758\n",
      "Minibatch accuracy: 75.0\n",
      "Minibatch accuracy: 57.3333333333\n",
      "Predictions | Labels:\n",
      " [[ 0.8328408   1.        ]\n",
      " [ 0.94730294  1.        ]]\n",
      "Minibatch loss at step 1050: 0.552406 Learning rate: 0.00751447\n",
      "Minibatch accuracy: 81.25\n",
      "Minibatch accuracy: 57.3333333333\n",
      "Predictions | Labels:\n",
      " [[ 0.32807717  0.        ]\n",
      " [ 0.41450575  0.        ]]\n",
      "Minibatch loss at step 1065: 0.542949 Learning rate: 0.00751447\n",
      "Minibatch accuracy: 81.25\n",
      "Minibatch accuracy: 56.0\n",
      "Predictions | Labels:\n",
      " [[ 0.36991996  0.        ]\n",
      " [ 0.37662339  0.        ]]\n",
      "Minibatch loss at step 1080: 0.524236 Learning rate: 0.00751447\n",
      "Minibatch accuracy: 81.25\n",
      "Minibatch accuracy: 58.0\n",
      "Predictions | Labels:\n",
      " [[ 0.60798371  1.        ]\n",
      " [ 0.62964267  0.        ]]\n",
      "Minibatch loss at step 1095: 0.535152 Learning rate: 0.00751447\n",
      "Minibatch accuracy: 75.0\n",
      "Minibatch accuracy: 51.3333333333\n",
      "Predictions | Labels:\n",
      " [[ 0.6421538   1.        ]\n",
      " [ 0.88314402  1.        ]]\n",
      "Minibatch loss at step 1110: 0.647742 Learning rate: 0.00751447\n",
      "Minibatch accuracy: 50.0\n",
      "Minibatch accuracy: 56.6666666667\n",
      "Predictions | Labels:\n",
      " [[ 0.39975041  1.        ]\n",
      " [ 0.8223998   0.        ]]\n",
      "Minibatch loss at step 1125: 0.530331 Learning rate: 0.00751447\n",
      "Minibatch accuracy: 68.75\n",
      "Minibatch accuracy: 59.3333333333\n",
      "Predictions | Labels:\n",
      " [[ 0.86312991  1.        ]\n",
      " [ 0.85318339  1.        ]]\n",
      "Minibatch loss at step 1140: 0.686191 Learning rate: 0.00751447\n",
      "Minibatch accuracy: 62.5\n",
      "Minibatch accuracy: 54.6666666667\n",
      "Predictions | Labels:\n",
      " [[ 0.31064841  0.        ]\n",
      " [ 0.44736943  0.        ]]\n",
      "Minibatch loss at step 1155: 0.592913 Learning rate: 0.00751447\n",
      "Minibatch accuracy: 68.75\n",
      "Minibatch accuracy: 54.0\n",
      "Predictions | Labels:\n",
      " [[ 0.81838095  1.        ]\n",
      " [ 0.76882553  0.        ]]\n",
      "Minibatch loss at step 1170: 0.495132 Learning rate: 0.00751447\n",
      "Minibatch accuracy: 81.25\n",
      "Minibatch accuracy: 59.3333333333\n",
      "Predictions | Labels:\n",
      " [[ 0.78223246  1.        ]\n",
      " [ 0.50524265  0.        ]]\n",
      "Minibatch loss at step 1185: 0.488954 Learning rate: 0.00751447\n",
      "Minibatch accuracy: 75.0\n",
      "Minibatch accuracy: 59.3333333333\n",
      "Predictions | Labels:\n",
      " [[ 0.74844486  1.        ]\n",
      " [ 0.66245031  1.        ]]\n",
      "Minibatch loss at step 1200: 0.576122 Learning rate: 0.00721389\n",
      "Minibatch accuracy: 81.25\n",
      "Minibatch accuracy: 56.0\n",
      "Predictions | Labels:\n",
      " [[ 0.42315474  0.        ]\n",
      " [ 0.61549443  1.        ]]\n",
      "Minibatch loss at step 1215: 0.490920 Learning rate: 0.00721389\n",
      "Minibatch accuracy: 87.5\n",
      "Minibatch accuracy: 56.0\n",
      "Predictions | Labels:\n",
      " [[ 0.72188544  1.        ]\n",
      " [ 0.3480607   1.        ]]\n",
      "Minibatch loss at step 1230: 0.472922 Learning rate: 0.00721389\n",
      "Minibatch accuracy: 87.5\n",
      "Minibatch accuracy: 60.0\n",
      "Predictions | Labels:\n",
      " [[ 0.31180498  1.        ]\n",
      " [ 0.63904798  1.        ]]\n",
      "Minibatch loss at step 1245: 0.612624 Learning rate: 0.00721389\n",
      "Minibatch accuracy: 75.0\n",
      "Minibatch accuracy: 52.6666666667\n",
      "Predictions | Labels:\n",
      " [[ 0.38387731  0.        ]\n",
      " [ 0.29287055  1.        ]]\n",
      "Minibatch loss at step 1260: 0.617937 Learning rate: 0.00721389\n",
      "Minibatch accuracy: 75.0\n",
      "Minibatch accuracy: 52.0\n",
      "Predictions | Labels:\n",
      " [[ 0.45355251  1.        ]\n",
      " [ 0.51425785  0.        ]]\n",
      "Minibatch loss at step 1275: 0.568792 Learning rate: 0.00721389\n",
      "Minibatch accuracy: 75.0\n",
      "Minibatch accuracy: 57.3333333333\n",
      "Predictions | Labels:\n",
      " [[ 0.30822533  1.        ]\n",
      " [ 0.72037828  1.        ]]\n",
      "Minibatch loss at step 1290: 0.645137 Learning rate: 0.00721389\n",
      "Minibatch accuracy: 62.5\n",
      "Minibatch accuracy: 54.6666666667\n",
      "Predictions | Labels:\n",
      " [[ 0.28618294  1.        ]\n",
      " [ 0.81635433  1.        ]]\n",
      "Minibatch loss at step 1305: 0.292054 Learning rate: 0.00721389\n",
      "Minibatch accuracy: 93.75\n",
      "Minibatch accuracy: 59.3333333333\n",
      "Predictions | Labels:\n",
      " [[ 0.44915918  0.        ]\n",
      " [ 0.90835899  1.        ]]\n",
      "Minibatch loss at step 1320: 0.454941 Learning rate: 0.00721389\n",
      "Minibatch accuracy: 87.5\n",
      "Minibatch accuracy: 55.3333333333\n",
      "Predictions | Labels:\n",
      " [[ 0.42914718  0.        ]\n",
      " [ 0.52111208  1.        ]]\n",
      "Minibatch loss at step 1335: 0.459374 Learning rate: 0.00721389\n",
      "Minibatch accuracy: 81.25\n",
      "Minibatch accuracy: 58.6666666667\n",
      "Predictions | Labels:\n",
      " [[ 0.48249084  1.        ]\n",
      " [ 0.98236036  1.        ]]\n",
      "Minibatch loss at step 1350: 0.427951 Learning rate: 0.00692534\n",
      "Minibatch accuracy: 87.5\n",
      "Minibatch accuracy: 56.6666666667\n",
      "Predictions | Labels:\n",
      " [[ 0.34574169  1.        ]\n",
      " [ 0.28110328  1.        ]]\n",
      "Minibatch loss at step 1365: 0.554564 Learning rate: 0.00692534\n",
      "Minibatch accuracy: 68.75\n",
      "Minibatch accuracy: 52.6666666667\n",
      "Predictions | Labels:\n",
      " [[ 0.72417891  0.        ]\n",
      " [ 0.32666385  0.        ]]\n",
      "Minibatch loss at step 1380: 0.521942 Learning rate: 0.00692534\n",
      "Minibatch accuracy: 75.0\n",
      "Minibatch accuracy: 56.0\n",
      "Predictions | Labels:\n",
      " [[ 0.33997509  1.        ]\n",
      " [ 0.89767867  1.        ]]\n",
      "Minibatch loss at step 1395: 0.381698 Learning rate: 0.00692534\n",
      "Minibatch accuracy: 93.75\n",
      "Minibatch accuracy: 58.0\n",
      "Predictions | Labels:\n",
      " [[ 0.71126658  1.        ]\n",
      " [ 0.71042216  1.        ]]\n",
      "Minibatch loss at step 1410: 0.539008 Learning rate: 0.00692534\n",
      "Minibatch accuracy: 81.25\n",
      "Minibatch accuracy: 60.0\n",
      "Predictions | Labels:\n",
      " [[ 0.80844116  1.        ]\n",
      " [ 0.60279393  1.        ]]\n",
      "Minibatch loss at step 1425: 0.548052 Learning rate: 0.00692534\n",
      "Minibatch accuracy: 75.0\n",
      "Minibatch accuracy: 54.0\n",
      "Predictions | Labels:\n",
      " [[ 0.88732946  1.        ]\n",
      " [ 0.84930325  1.        ]]\n",
      "Minibatch loss at step 1440: 0.404619 Learning rate: 0.00692534\n",
      "Minibatch accuracy: 87.5\n",
      "Minibatch accuracy: 56.6666666667\n",
      "Predictions | Labels:\n",
      " [[ 0.64484388  0.        ]\n",
      " [ 0.31307536  0.        ]]\n",
      "Minibatch loss at step 1455: 0.577249 Learning rate: 0.00692534\n",
      "Minibatch accuracy: 87.5\n",
      "Minibatch accuracy: 59.3333333333\n",
      "Predictions | Labels:\n",
      " [[ 0.36101195  0.        ]\n",
      " [ 0.45695481  0.        ]]\n",
      "Minibatch loss at step 1470: 0.445890 Learning rate: 0.00692534\n",
      "Minibatch accuracy: 87.5\n",
      "Minibatch accuracy: 56.0\n",
      "Predictions | Labels:\n",
      " [[ 0.54427528  1.        ]\n",
      " [ 0.30068013  0.        ]]\n",
      "Minibatch loss at step 1485: 0.351325 Learning rate: 0.00692534\n",
      "Minibatch accuracy: 87.5\n",
      "Minibatch accuracy: 54.0\n",
      "Predictions | Labels:\n",
      " [[ 0.7856127   1.        ]\n",
      " [ 0.89208311  1.        ]]\n",
      "Minibatch loss at step 1500: 0.392067 Learning rate: 0.00664832\n",
      "Minibatch accuracy: 93.75\n",
      "Minibatch accuracy: 58.6666666667\n",
      "Predictions | Labels:\n",
      " [[ 0.34682426  0.        ]\n",
      " [ 0.40283528  0.        ]]\n",
      "Minibatch loss at step 1515: 0.382433 Learning rate: 0.00664832\n",
      "Minibatch accuracy: 87.5\n",
      "Minibatch accuracy: 59.3333333333\n",
      "Predictions | Labels:\n",
      " [[ 0.84350032  1.        ]\n",
      " [ 0.75974703  1.        ]]\n",
      "Minibatch loss at step 1530: 0.515230 Learning rate: 0.00664832\n",
      "Minibatch accuracy: 81.25\n",
      "Minibatch accuracy: 56.0\n",
      "Predictions | Labels:\n",
      " [[ 0.69069773  1.        ]\n",
      " [ 0.88821685  0.        ]]\n",
      "Minibatch loss at step 1545: 0.378893 Learning rate: 0.00664832\n",
      "Minibatch accuracy: 81.25\n",
      "Minibatch accuracy: 61.3333333333\n",
      "Predictions | Labels:\n",
      " [[ 0.42370999  0.        ]\n",
      " [ 0.93930173  1.        ]]\n",
      "Minibatch loss at step 1560: 0.397747 Learning rate: 0.00664832\n",
      "Minibatch accuracy: 93.75\n",
      "Minibatch accuracy: 60.6666666667\n",
      "Predictions | Labels:\n",
      " [[ 0.48135367  0.        ]\n",
      " [ 0.94602746  1.        ]]\n",
      "Minibatch loss at step 1575: 0.535098 Learning rate: 0.00664832\n",
      "Minibatch accuracy: 68.75\n",
      "Minibatch accuracy: 58.6666666667\n",
      "Predictions | Labels:\n",
      " [[ 0.28931662  1.        ]\n",
      " [ 0.30170318  0.        ]]\n",
      "Minibatch loss at step 1590: 0.285252 Learning rate: 0.00664832\n",
      "Minibatch accuracy: 93.75\n",
      "Minibatch accuracy: 56.6666666667\n",
      "Predictions | Labels:\n",
      " [[ 0.87008905  1.        ]\n",
      " [ 0.73547083  1.        ]]\n",
      "Minibatch loss at step 1605: 0.354508 Learning rate: 0.00664832\n",
      "Minibatch accuracy: 93.75\n",
      "Minibatch accuracy: 57.3333333333\n",
      "Predictions | Labels:\n",
      " [[ 0.35818556  0.        ]\n",
      " [ 0.27470139  0.        ]]\n",
      "Minibatch loss at step 1620: 0.395914 Learning rate: 0.00664832\n",
      "Minibatch accuracy: 87.5\n",
      "Minibatch accuracy: 58.6666666667\n",
      "Predictions | Labels:\n",
      " [[ 0.27663708  1.        ]\n",
      " [ 0.73408347  1.        ]]\n",
      "Minibatch loss at step 1635: 0.329943 Learning rate: 0.00664832\n",
      "Minibatch accuracy: 93.75\n",
      "Minibatch accuracy: 56.6666666667\n",
      "Predictions | Labels:\n",
      " [[ 0.97421527  1.        ]\n",
      " [ 0.43288276  0.        ]]\n",
      "Minibatch loss at step 1650: 0.389102 Learning rate: 0.00638239\n",
      "Minibatch accuracy: 93.75\n",
      "Minibatch accuracy: 60.6666666667\n",
      "Predictions | Labels:\n",
      " [[ 0.67843723  1.        ]\n",
      " [ 0.99595094  1.        ]]\n",
      "Minibatch loss at step 1665: 0.523125 Learning rate: 0.00638239\n",
      "Minibatch accuracy: 56.25\n",
      "Minibatch accuracy: 58.0\n",
      "Predictions | Labels:\n",
      " [[ 0.87315083  1.        ]\n",
      " [ 0.61462975  0.        ]]\n",
      "Minibatch loss at step 1680: 0.524398 Learning rate: 0.00638239\n",
      "Minibatch accuracy: 62.5\n",
      "Minibatch accuracy: 55.3333333333\n",
      "Predictions | Labels:\n",
      " [[ 0.50612003  0.        ]\n",
      " [ 0.66055936  0.        ]]\n",
      "Minibatch loss at step 1695: 0.437043 Learning rate: 0.00638239\n",
      "Minibatch accuracy: 87.5\n",
      "Minibatch accuracy: 60.0\n",
      "Predictions | Labels:\n",
      " [[ 0.62477648  1.        ]\n",
      " [ 0.29387087  0.        ]]\n",
      "Minibatch loss at step 1710: 0.361064 Learning rate: 0.00638239\n",
      "Minibatch accuracy: 93.75\n",
      "Minibatch accuracy: 58.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5893625   1.        ]\n",
      " [ 0.35503966  0.        ]]\n",
      "Minibatch loss at step 1725: 0.293344 Learning rate: 0.00638239\n",
      "Minibatch accuracy: 93.75\n",
      "Minibatch accuracy: 58.6666666667\n",
      "Predictions | Labels:\n",
      " [[ 0.78951603  1.        ]\n",
      " [ 0.98925215  1.        ]]\n",
      "Minibatch loss at step 1740: 0.346568 Learning rate: 0.00638239\n",
      "Minibatch accuracy: 81.25\n",
      "Minibatch accuracy: 58.6666666667\n",
      "Predictions | Labels:\n",
      " [[ 0.52246231  0.        ]\n",
      " [ 0.96013695  1.        ]]\n",
      "Minibatch loss at step 1755: 0.393513 Learning rate: 0.00638239\n",
      "Minibatch accuracy: 81.25\n",
      "Minibatch accuracy: 61.3333333333\n",
      "Predictions | Labels:\n",
      " [[ 0.34515572  0.        ]\n",
      " [ 0.88149768  1.        ]]\n",
      "Minibatch loss at step 1770: 0.394106 Learning rate: 0.00638239\n",
      "Minibatch accuracy: 93.75\n",
      "Minibatch accuracy: 61.3333333333\n",
      "Predictions | Labels:\n",
      " [[ 0.73576498  1.        ]\n",
      " [ 0.82401574  1.        ]]\n",
      "Minibatch loss at step 1785: 0.476101 Learning rate: 0.00638239\n",
      "Minibatch accuracy: 75.0\n",
      "Minibatch accuracy: 59.3333333333\n",
      "Predictions | Labels:\n",
      " [[ 0.27419952  1.        ]\n",
      " [ 0.60720897  1.        ]]\n",
      "Minibatch loss at step 1800: 0.348252 Learning rate: 0.0061271\n",
      "Minibatch accuracy: 93.75\n",
      "Minibatch accuracy: 57.3333333333\n",
      "Predictions | Labels:\n",
      " [[ 0.28661358  0.        ]\n",
      " [ 0.7053504   1.        ]]\n",
      "Minibatch loss at step 1815: 0.272413 Learning rate: 0.0061271\n",
      "Minibatch accuracy: 93.75\n",
      "Minibatch accuracy: 59.3333333333\n",
      "Predictions | Labels:\n",
      " [[ 0.33387673  0.        ]\n",
      " [ 0.33687851  0.        ]]\n",
      "Minibatch loss at step 1830: 0.361944 Learning rate: 0.0061271\n",
      "Minibatch accuracy: 93.75\n",
      "Minibatch accuracy: 60.6666666667\n",
      "Predictions | Labels:\n",
      " [[ 0.8945958  1.       ]\n",
      " [ 0.7217893  1.       ]]\n",
      "Minibatch loss at step 1845: 0.407036 Learning rate: 0.0061271\n",
      "Minibatch accuracy: 93.75\n",
      "Minibatch accuracy: 57.3333333333\n",
      "Predictions | Labels:\n",
      " [[ 0.28247234  0.        ]\n",
      " [ 0.28984499  0.        ]]\n",
      "Minibatch loss at step 1860: 0.278825 Learning rate: 0.0061271\n",
      "Minibatch accuracy: 93.75\n",
      "Minibatch accuracy: 60.0\n",
      "Predictions | Labels:\n",
      " [[ 0.31432667  0.        ]\n",
      " [ 0.70409834  1.        ]]\n",
      "Minibatch loss at step 1875: 0.271267 Learning rate: 0.0061271\n",
      "Minibatch accuracy: 100.0\n",
      "Minibatch accuracy: 60.0\n",
      "Predictions | Labels:\n",
      " [[ 0.98985565  1.        ]\n",
      " [ 0.27458516  0.        ]]\n",
      "Minibatch loss at step 1890: 0.345364 Learning rate: 0.0061271\n",
      "Minibatch accuracy: 87.5\n",
      "Minibatch accuracy: 54.6666666667\n",
      "Predictions | Labels:\n",
      " [[ 0.29160452  0.        ]\n",
      " [ 0.30139542  0.        ]]\n",
      "Minibatch loss at step 1905: 0.409769 Learning rate: 0.0061271\n",
      "Minibatch accuracy: 87.5\n",
      "Minibatch accuracy: 54.0\n",
      "Predictions | Labels:\n",
      " [[ 0.86039263  1.        ]\n",
      " [ 0.34410244  0.        ]]\n",
      "Minibatch loss at step 1920: 0.295008 Learning rate: 0.0061271\n",
      "Minibatch accuracy: 100.0\n",
      "Minibatch accuracy: 60.6666666667\n",
      "Predictions | Labels:\n",
      " [[ 0.85572398  1.        ]\n",
      " [ 0.92135215  1.        ]]\n",
      "Minibatch loss at step 1935: 0.528304 Learning rate: 0.0061271\n",
      "Minibatch accuracy: 81.25\n",
      "Minibatch accuracy: 55.3333333333\n",
      "Predictions | Labels:\n",
      " [[ 0.32939491  0.        ]\n",
      " [ 0.27554575  0.        ]]\n",
      "Minibatch loss at step 1950: 0.184066 Learning rate: 0.00588201\n",
      "Minibatch accuracy: 100.0\n",
      "Minibatch accuracy: 62.6666666667\n",
      "Predictions | Labels:\n",
      " [[ 0.3281256   0.        ]\n",
      " [ 0.98857701  1.        ]]\n",
      "Minibatch loss at step 1965: 0.271802 Learning rate: 0.00588201\n",
      "Minibatch accuracy: 93.75\n",
      "Minibatch accuracy: 59.3333333333\n",
      "Predictions | Labels:\n",
      " [[ 0.28752357  0.        ]\n",
      " [ 0.96445644  1.        ]]\n",
      "Minibatch loss at step 1980: 0.264572 Learning rate: 0.00588201\n",
      "Minibatch accuracy: 93.75\n",
      "Minibatch accuracy: 60.6666666667\n",
      "Predictions | Labels:\n",
      " [[ 0.97380257  1.        ]\n",
      " [ 0.98267579  1.        ]]\n",
      "Minibatch loss at step 1995: 0.271752 Learning rate: 0.00588201\n",
      "Minibatch accuracy: 100.0\n",
      "Minibatch accuracy: 54.0\n",
      "Predictions | Labels:\n",
      " [[ 0.95762277  1.        ]\n",
      " [ 0.27183098  0.        ]]\n",
      "Minibatch loss at step 2010: 0.393835 Learning rate: 0.00588201\n",
      "Minibatch accuracy: 87.5\n",
      "Minibatch accuracy: 57.3333333333\n",
      "Predictions | Labels:\n",
      " [[ 0.98273438  1.        ]\n",
      " [ 0.99335587  1.        ]]\n",
      "Minibatch loss at step 2025: 0.349288 Learning rate: 0.00588201\n",
      "Minibatch accuracy: 87.5\n",
      "Minibatch accuracy: 61.3333333333\n",
      "Predictions | Labels:\n",
      " [[ 0.99341655  1.        ]\n",
      " [ 0.81879079  1.        ]]\n",
      "Minibatch loss at step 2040: 0.318009 Learning rate: 0.00588201\n",
      "Minibatch accuracy: 100.0\n",
      "Minibatch accuracy: 60.0\n",
      "Predictions | Labels:\n",
      " [[ 0.27154052  0.        ]\n",
      " [ 0.89218497  1.        ]]\n",
      "Minibatch loss at step 2055: 0.322039 Learning rate: 0.00588201\n",
      "Minibatch accuracy: 93.75\n",
      "Minibatch accuracy: 60.6666666667\n",
      "Predictions | Labels:\n",
      " [[ 0.42149314  0.        ]\n",
      " [ 0.43312413  0.        ]]\n",
      "Minibatch loss at step 2070: 0.387755 Learning rate: 0.00588201\n",
      "Minibatch accuracy: 87.5\n",
      "Minibatch accuracy: 60.6666666667\n",
      "Predictions | Labels:\n",
      " [[ 0.85502011  1.        ]\n",
      " [ 0.28760865  0.        ]]\n",
      "Minibatch loss at step 2085: 0.284037 Learning rate: 0.00588201\n",
      "Minibatch accuracy: 93.75\n",
      "Minibatch accuracy: 62.0\n",
      "Predictions | Labels:\n",
      " [[ 0.88017589  1.        ]\n",
      " [ 0.91114265  1.        ]]\n",
      "Minibatch loss at step 2100: 0.385664 Learning rate: 0.00564673\n",
      "Minibatch accuracy: 87.5\n",
      "Minibatch accuracy: 60.6666666667\n",
      "Predictions | Labels:\n",
      " [[ 0.34791061  0.        ]\n",
      " [ 0.99444628  1.        ]]\n",
      "Minibatch loss at step 2115: 0.230364 Learning rate: 0.00564673\n",
      "Minibatch accuracy: 93.75\n",
      "Minibatch accuracy: 58.6666666667\n",
      "Predictions | Labels:\n",
      " [[ 0.99406105  1.        ]\n",
      " [ 0.3171258   0.        ]]\n",
      "Minibatch loss at step 2130: 0.319548 Learning rate: 0.00564673\n",
      "Minibatch accuracy: 81.25\n",
      "Minibatch accuracy: 60.6666666667\n",
      "Predictions | Labels:\n",
      " [[ 0.46779498  1.        ]\n",
      " [ 0.27272797  1.        ]]\n",
      "Minibatch loss at step 2145: 0.246614 Learning rate: 0.00564673\n",
      "Minibatch accuracy: 100.0\n",
      "Minibatch accuracy: 60.6666666667\n",
      "Predictions | Labels:\n",
      " [[ 0.34655353  0.        ]\n",
      " [ 0.9672175   1.        ]]\n",
      "Minibatch loss at step 2160: 0.261905 Learning rate: 0.00564673\n",
      "Minibatch accuracy: 93.75\n",
      "Minibatch accuracy: 63.3333333333\n",
      "Predictions | Labels:\n",
      " [[ 0.93907535  1.        ]\n",
      " [ 0.89487785  1.        ]]\n",
      "Minibatch loss at step 2175: 0.179562 Learning rate: 0.00564673\n",
      "Minibatch accuracy: 100.0\n",
      "Minibatch accuracy: 60.0\n",
      "Predictions | Labels:\n",
      " [[ 0.26964203  0.        ]\n",
      " [ 0.27346036  0.        ]]\n",
      "Minibatch loss at step 2190: 0.225309 Learning rate: 0.00564673\n",
      "Minibatch accuracy: 93.75\n",
      "Minibatch accuracy: 61.3333333333\n",
      "Predictions | Labels:\n",
      " [[ 0.99070263  1.        ]\n",
      " [ 0.32771477  0.        ]]\n",
      "Minibatch loss at step 2205: 0.204438 Learning rate: 0.00564673\n",
      "Minibatch accuracy: 100.0\n",
      "Minibatch accuracy: 62.0\n",
      "Predictions | Labels:\n",
      " [[ 0.33878151  0.        ]\n",
      " [ 0.93459326  1.        ]]\n",
      "Minibatch loss at step 2220: 0.285532 Learning rate: 0.00564673\n",
      "Minibatch accuracy: 93.75\n",
      "Minibatch accuracy: 61.3333333333\n",
      "Predictions | Labels:\n",
      " [[ 0.95149446  1.        ]\n",
      " [ 0.2765843   1.        ]]\n",
      "Minibatch loss at step 2235: 0.289190 Learning rate: 0.00564673\n",
      "Minibatch accuracy: 93.75\n",
      "Minibatch accuracy: 58.0\n",
      "Predictions | Labels:\n",
      " [[ 0.2776024   0.        ]\n",
      " [ 0.99404317  1.        ]]\n",
      "Minibatch loss at step 2250: 0.321438 Learning rate: 0.00542086\n",
      "Minibatch accuracy: 93.75\n",
      "Minibatch accuracy: 61.3333333333\n",
      "Predictions | Labels:\n",
      " [[ 0.26902145  0.        ]\n",
      " [ 0.28819385  0.        ]]\n",
      "Minibatch loss at step 2265: 0.279238 Learning rate: 0.00542086\n",
      "Minibatch accuracy: 93.75\n",
      "Minibatch accuracy: 63.3333333333\n",
      "Predictions | Labels:\n",
      " [[ 0.32341176  0.        ]\n",
      " [ 0.98921484  1.        ]]\n",
      "Minibatch loss at step 2280: 0.311663 Learning rate: 0.00542086\n",
      "Minibatch accuracy: 87.5\n",
      "Minibatch accuracy: 60.6666666667\n",
      "Predictions | Labels:\n",
      " [[ 0.99716336  1.        ]\n",
      " [ 0.85328466  1.        ]]\n",
      "Minibatch loss at step 2295: 0.314015 Learning rate: 0.00542086\n",
      "Minibatch accuracy: 93.75\n",
      "Minibatch accuracy: 60.6666666667\n",
      "Predictions | Labels:\n",
      " [[ 0.29118389  0.        ]\n",
      " [ 0.98616761  1.        ]]\n",
      "Minibatch loss at step 2310: 0.166841 Learning rate: 0.00542086\n",
      "Minibatch accuracy: 100.0\n",
      "Minibatch accuracy: 62.0\n",
      "Predictions | Labels:\n",
      " [[ 0.32961506  0.        ]\n",
      " [ 0.89441156  1.        ]]\n",
      "Minibatch loss at step 2325: 0.283835 Learning rate: 0.00542086\n",
      "Minibatch accuracy: 93.75\n",
      "Minibatch accuracy: 57.3333333333\n",
      "Predictions | Labels:\n",
      " [[ 0.99347103  1.        ]\n",
      " [ 0.93770754  1.        ]]\n",
      "Minibatch loss at step 2340: 0.357487 Learning rate: 0.00542086\n",
      "Minibatch accuracy: 87.5\n",
      "Minibatch accuracy: 60.6666666667\n",
      "Predictions | Labels:\n",
      " [[ 0.27491415  0.        ]\n",
      " [ 0.32932073  0.        ]]\n",
      "Minibatch loss at step 2355: 0.247236 Learning rate: 0.00542086\n",
      "Minibatch accuracy: 100.0\n",
      "Minibatch accuracy: 61.3333333333\n",
      "Predictions | Labels:\n",
      " [[ 0.30243248  0.        ]\n",
      " [ 0.99633574  1.        ]]\n",
      "Minibatch loss at step 2370: 0.185148 Learning rate: 0.00542086\n",
      "Minibatch accuracy: 100.0\n",
      "Minibatch accuracy: 59.3333333333\n",
      "Predictions | Labels:\n",
      " [[ 0.86157686  1.        ]\n",
      " [ 0.97514874  1.        ]]\n",
      "Minibatch loss at step 2385: 0.284822 Learning rate: 0.00542086\n",
      "Minibatch accuracy: 87.5\n",
      "Minibatch accuracy: 60.6666666667\n",
      "Predictions | Labels:\n",
      " [[ 0.9969573   1.        ]\n",
      " [ 0.32965672  0.        ]]\n",
      "Minibatch loss at step 2400: 0.180970 Learning rate: 0.00520403\n",
      "Minibatch accuracy: 93.75\n",
      "Minibatch accuracy: 60.0\n",
      "Predictions | Labels:\n",
      " [[ 0.97803736  1.        ]\n",
      " [ 0.93698215  1.        ]]\n",
      "Minibatch loss at step 2415: 0.322229 Learning rate: 0.00520403\n",
      "Minibatch accuracy: 93.75\n",
      "Minibatch accuracy: 60.6666666667\n",
      "Predictions | Labels:\n",
      " [[ 0.71983802  1.        ]\n",
      " [ 0.96218675  1.        ]]\n",
      "Minibatch loss at step 2430: 0.218279 Learning rate: 0.00520403\n",
      "Minibatch accuracy: 100.0\n",
      "Minibatch accuracy: 59.3333333333\n",
      "Predictions | Labels:\n",
      " [[ 0.2872557   0.        ]\n",
      " [ 0.27077636  0.        ]]\n",
      "Minibatch loss at step 2445: 0.230545 Learning rate: 0.00520403\n",
      "Minibatch accuracy: 93.75\n",
      "Minibatch accuracy: 57.3333333333\n",
      "Predictions | Labels:\n",
      " [[ 0.99628276  1.        ]\n",
      " [ 0.35614488  0.        ]]\n",
      "Minibatch loss at step 2460: 0.238282 Learning rate: 0.00520403\n",
      "Minibatch accuracy: 93.75\n",
      "Minibatch accuracy: 60.0\n",
      "Predictions | Labels:\n",
      " [[ 0.98263276  1.        ]\n",
      " [ 0.9678123   1.        ]]\n",
      "Minibatch loss at step 2475: 0.347975 Learning rate: 0.00520403\n",
      "Minibatch accuracy: 87.5\n",
      "Minibatch accuracy: 60.0\n",
      "Predictions | Labels:\n",
      " [[ 0.96648538  1.        ]\n",
      " [ 0.96901679  1.        ]]\n",
      "Minibatch loss at step 2490: 0.205306 Learning rate: 0.00520403\n",
      "Minibatch accuracy: 100.0\n",
      "Minibatch accuracy: 59.3333333333\n",
      "Predictions | Labels:\n",
      " [[ 0.27434048  0.        ]\n",
      " [ 0.28549096  0.        ]]\n",
      "Minibatch loss at step 2505: 0.120624 Learning rate: 0.00520403\n",
      "Minibatch accuracy: 100.0\n",
      "Minibatch accuracy: 60.0\n",
      "Predictions | Labels:\n",
      " [[ 0.27424309  0.        ]\n",
      " [ 0.9609319   1.        ]]\n",
      "Minibatch loss at step 2520: 0.170188 Learning rate: 0.00520403\n",
      "Minibatch accuracy: 100.0\n",
      "Minibatch accuracy: 60.0\n",
      "Predictions | Labels:\n",
      " [[ 0.98779231  1.        ]\n",
      " [ 0.28750408  0.        ]]\n",
      "Minibatch loss at step 2535: 0.181600 Learning rate: 0.00520403\n",
      "Minibatch accuracy: 100.0\n",
      "Minibatch accuracy: 59.3333333333\n",
      "Predictions | Labels:\n",
      " [[ 0.99305844  1.        ]\n",
      " [ 0.29995659  0.        ]]\n",
      "Minibatch loss at step 2550: 0.301166 Learning rate: 0.00499587\n",
      "Minibatch accuracy: 87.5\n",
      "Minibatch accuracy: 58.6666666667\n",
      "Predictions | Labels:\n",
      " [[ 0.92313653  1.        ]\n",
      " [ 0.27004397  0.        ]]\n",
      "Minibatch loss at step 2565: 0.160151 Learning rate: 0.00499587\n",
      "Minibatch accuracy: 100.0\n",
      "Minibatch accuracy: 60.0\n",
      "Predictions | Labels:\n",
      " [[ 0.31142521  0.        ]\n",
      " [ 0.32791001  0.        ]]\n",
      "Minibatch loss at step 2580: 0.290756 Learning rate: 0.00499587\n",
      "Minibatch accuracy: 93.75\n",
      "Minibatch accuracy: 58.0\n",
      "Predictions | Labels:\n",
      " [[ 0.31207883  0.        ]\n",
      " [ 0.27529249  0.        ]]\n",
      "Minibatch loss at step 2595: 0.168136 Learning rate: 0.00499587\n",
      "Minibatch accuracy: 100.0\n",
      "Minibatch accuracy: 61.3333333333\n",
      "Predictions | Labels:\n",
      " [[ 0.27503136  0.        ]\n",
      " [ 0.33053407  0.        ]]\n",
      "Minibatch loss at step 2610: 0.146649 Learning rate: 0.00499587\n",
      "Minibatch accuracy: 100.0\n",
      "Minibatch accuracy: 58.6666666667\n",
      "Predictions | Labels:\n",
      " [[ 0.95244986  1.        ]\n",
      " [ 0.95920855  1.        ]]\n",
      "Minibatch loss at step 2625: 0.191329 Learning rate: 0.00499587\n",
      "Minibatch accuracy: 100.0\n",
      "Minibatch accuracy: 60.0\n",
      "Predictions | Labels:\n",
      " [[ 0.28332582  0.        ]\n",
      " [ 0.2874504   0.        ]]\n",
      "Minibatch loss at step 2640: 0.239750 Learning rate: 0.00499587\n",
      "Minibatch accuracy: 100.0\n",
      "Minibatch accuracy: 57.3333333333\n",
      "Predictions | Labels:\n",
      " [[ 0.99936527  1.        ]\n",
      " [ 0.98083192  1.        ]]\n",
      "Minibatch loss at step 2655: 0.313287 Learning rate: 0.00499587\n",
      "Minibatch accuracy: 93.75\n",
      "Minibatch accuracy: 58.0\n",
      "Predictions | Labels:\n",
      " [[ 0.26947886  0.        ]\n",
      " [ 0.28443807  0.        ]]\n",
      "Minibatch loss at step 2670: 0.292994 Learning rate: 0.00499587\n",
      "Minibatch accuracy: 87.5\n",
      "Minibatch accuracy: 59.3333333333\n",
      "Predictions | Labels:\n",
      " [[ 0.99851757  1.        ]\n",
      " [ 0.35196066  1.        ]]\n",
      "Minibatch loss at step 2685: 0.289188 Learning rate: 0.00499587\n",
      "Minibatch accuracy: 93.75\n",
      "Minibatch accuracy: 60.6666666667\n",
      "Predictions | Labels:\n",
      " [[ 0.27484539  0.        ]\n",
      " [ 0.9722926   1.        ]]\n",
      "Minibatch loss at step 2700: 0.161257 Learning rate: 0.00479603\n",
      "Minibatch accuracy: 100.0\n",
      "Minibatch accuracy: 60.0\n",
      "Predictions | Labels:\n",
      " [[ 0.31275764  0.        ]\n",
      " [ 0.89723337  1.        ]]\n",
      "Minibatch loss at step 2715: 0.366780 Learning rate: 0.00479603\n",
      "Minibatch accuracy: 87.5\n",
      "Minibatch accuracy: 60.6666666667\n",
      "Predictions | Labels:\n",
      " [[ 0.27562064  1.        ]\n",
      " [ 0.27425003  0.        ]]\n",
      "Minibatch loss at step 2730: 0.168553 Learning rate: 0.00479603\n",
      "Minibatch accuracy: 100.0\n",
      "Minibatch accuracy: 62.0\n",
      "Predictions | Labels:\n",
      " [[ 0.91170698  1.        ]\n",
      " [ 0.95042902  1.        ]]\n",
      "Minibatch loss at step 2745: 0.189582 Learning rate: 0.00479603\n",
      "Minibatch accuracy: 100.0\n",
      "Minibatch accuracy: 60.6666666667\n",
      "Predictions | Labels:\n",
      " [[ 0.95387274  1.        ]\n",
      " [ 0.30360779  0.        ]]\n",
      "Minibatch loss at step 2760: 0.157664 Learning rate: 0.00479603\n",
      "Minibatch accuracy: 100.0\n",
      "Minibatch accuracy: 60.0\n",
      "Predictions | Labels:\n",
      " [[ 0.2981323  0.       ]\n",
      " [ 0.3038615  0.       ]]\n",
      "Minibatch loss at step 2775: 0.192406 Learning rate: 0.00479603\n",
      "Minibatch accuracy: 100.0\n",
      "Minibatch accuracy: 59.3333333333\n",
      "Predictions | Labels:\n",
      " [[ 0.99969101  1.        ]\n",
      " [ 0.99616385  1.        ]]\n",
      "Minibatch loss at step 2790: 0.176134 Learning rate: 0.00479603\n",
      "Minibatch accuracy: 100.0\n",
      "Minibatch accuracy: 60.0\n",
      "Predictions | Labels:\n",
      " [[ 0.99900889  1.        ]\n",
      " [ 0.29122356  0.        ]]\n",
      "Minibatch loss at step 2805: 0.148643 Learning rate: 0.00479603\n",
      "Minibatch accuracy: 100.0\n",
      "Minibatch accuracy: 62.0\n",
      "Predictions | Labels:\n",
      " [[ 0.98967057  1.        ]\n",
      " [ 0.27149454  0.        ]]\n",
      "Minibatch loss at step 2820: 0.158842 Learning rate: 0.00479603\n",
      "Minibatch accuracy: 100.0\n",
      "Minibatch accuracy: 61.3333333333\n",
      "Predictions | Labels:\n",
      " [[ 0.9991498   1.        ]\n",
      " [ 0.98931164  1.        ]]\n",
      "Minibatch loss at step 2835: 0.214729 Learning rate: 0.00479603\n",
      "Minibatch accuracy: 93.75\n",
      "Minibatch accuracy: 60.6666666667\n",
      "Predictions | Labels:\n",
      " [[ 0.97536534  1.        ]\n",
      " [ 0.99525708  1.        ]]\n",
      "Minibatch loss at step 2850: 0.143735 Learning rate: 0.00460419\n",
      "Minibatch accuracy: 100.0\n",
      "Minibatch accuracy: 62.6666666667\n",
      "Predictions | Labels:\n",
      " [[ 0.27405024  0.        ]\n",
      " [ 0.89093494  1.        ]]\n",
      "Minibatch loss at step 2865: 0.193532 Learning rate: 0.00460419\n",
      "Minibatch accuracy: 100.0\n",
      "Minibatch accuracy: 60.0\n",
      "Predictions | Labels:\n",
      " [[ 0.27636904  0.        ]\n",
      " [ 0.29442683  0.        ]]\n",
      "Minibatch loss at step 2880: 0.329177 Learning rate: 0.00460419\n",
      "Minibatch accuracy: 93.75\n",
      "Minibatch accuracy: 58.6666666667\n",
      "Predictions | Labels:\n",
      " [[ 0.27169111  0.        ]\n",
      " [ 0.30031043  0.        ]]\n",
      "Minibatch loss at step 2895: 0.221284 Learning rate: 0.00460419\n",
      "Minibatch accuracy: 93.75\n",
      "Minibatch accuracy: 59.3333333333\n",
      "Predictions | Labels:\n",
      " [[ 0.28010765  0.        ]\n",
      " [ 0.99962366  1.        ]]\n",
      "Minibatch loss at step 2910: 0.179974 Learning rate: 0.00460419\n",
      "Minibatch accuracy: 100.0\n",
      "Minibatch accuracy: 59.3333333333\n",
      "Predictions | Labels:\n",
      " [[ 0.28875756  0.        ]\n",
      " [ 0.99146336  1.        ]]\n",
      "Minibatch loss at step 2925: 0.294723 Learning rate: 0.00460419\n",
      "Minibatch accuracy: 93.75\n",
      "Minibatch accuracy: 60.6666666667\n",
      "Predictions | Labels:\n",
      " [[ 0.99516088  1.        ]\n",
      " [ 0.27529261  0.        ]]\n",
      "Minibatch loss at step 2940: 0.321567 Learning rate: 0.00460419\n",
      "Minibatch accuracy: 87.5\n",
      "Minibatch accuracy: 59.3333333333\n",
      "Predictions | Labels:\n",
      " [[ 0.97497171  1.        ]\n",
      " [ 0.280444    0.        ]]\n",
      "Minibatch loss at step 2955: 0.140058 Learning rate: 0.00460419\n",
      "Minibatch accuracy: 93.75\n",
      "Minibatch accuracy: 60.0\n",
      "Predictions | Labels:\n",
      " [[ 0.99991453  1.        ]\n",
      " [ 0.99983156  1.        ]]\n",
      "Minibatch loss at step 2970: 0.220653 Learning rate: 0.00460419\n",
      "Minibatch accuracy: 93.75\n",
      "Minibatch accuracy: 61.3333333333\n",
      "Predictions | Labels:\n",
      " [[ 0.97683936  1.        ]\n",
      " [ 0.29009208  0.        ]]\n",
      "Minibatch loss at step 2985: 0.240583 Learning rate: 0.00460419\n",
      "Minibatch accuracy: 93.75\n",
      "Minibatch accuracy: 59.3333333333\n",
      "Predictions | Labels:\n",
      " [[ 0.98004925  1.        ]\n",
      " [ 0.28580368  0.        ]]\n",
      "Minibatch loss at step 3000: 0.166275 Learning rate: 0.00442002\n",
      "Minibatch accuracy: 100.0\n",
      "Minibatch accuracy: 60.6666666667\n",
      "Predictions | Labels:\n",
      " [[ 0.90751535  1.        ]\n",
      " [ 0.98581243  1.        ]]\n",
      "Minibatch loss at step 3015: 0.143026 Learning rate: 0.00442002\n",
      "Minibatch accuracy: 100.0\n",
      "Minibatch accuracy: 60.6666666667\n",
      "Predictions | Labels:\n",
      " [[ 0.93132323  1.        ]\n",
      " [ 0.95836717  1.        ]]\n",
      "Minibatch loss at step 3030: 0.227467 Learning rate: 0.00442002\n",
      "Minibatch accuracy: 93.75\n",
      "Minibatch accuracy: 59.3333333333\n",
      "Predictions | Labels:\n",
      " [[ 0.27043825  0.        ]\n",
      " [ 0.99982554  1.        ]]\n",
      "Minibatch loss at step 3045: 0.112412 Learning rate: 0.00442002\n",
      "Minibatch accuracy: 100.0\n",
      "Minibatch accuracy: 60.6666666667\n",
      "Predictions | Labels:\n",
      " [[ 0.28499684  0.        ]\n",
      " [ 0.94860125  1.        ]]\n",
      "Minibatch loss at step 3060: 0.245067 Learning rate: 0.00442002\n",
      "Minibatch accuracy: 100.0\n",
      "Minibatch accuracy: 58.0\n",
      "Predictions | Labels:\n",
      " [[ 0.29188877  0.        ]\n",
      " [ 0.31500405  0.        ]]\n",
      "Minibatch loss at step 3075: 0.169313 Learning rate: 0.00442002\n",
      "Minibatch accuracy: 100.0\n",
      "Minibatch accuracy: 59.3333333333\n",
      "Predictions | Labels:\n",
      " [[ 0.99914205  1.        ]\n",
      " [ 0.27072522  0.        ]]\n",
      "Minibatch loss at step 3090: 0.193759 Learning rate: 0.00442002\n",
      "Minibatch accuracy: 93.75\n",
      "Minibatch accuracy: 59.3333333333\n",
      "Predictions | Labels:\n",
      " [[ 0.26916096  0.        ]\n",
      " [ 0.27580187  0.        ]]\n",
      "Minibatch loss at step 3105: 0.228383 Learning rate: 0.00442002\n",
      "Minibatch accuracy: 93.75\n",
      "Minibatch accuracy: 60.0\n",
      "Predictions | Labels:\n",
      " [[ 0.26936585  0.        ]\n",
      " [ 0.26904759  0.        ]]\n",
      "Minibatch loss at step 3120: 0.387422 Learning rate: 0.00442002\n",
      "Minibatch accuracy: 87.5\n",
      "Minibatch accuracy: 61.3333333333\n",
      "Predictions | Labels:\n",
      " [[ 0.26894444  1.        ]\n",
      " [ 0.95192808  1.        ]]\n",
      "Minibatch loss at step 3135: 0.168638 Learning rate: 0.00442002\n",
      "Minibatch accuracy: 100.0\n",
      "Minibatch accuracy: 58.6666666667\n",
      "Predictions | Labels:\n",
      " [[ 0.27294329  0.        ]\n",
      " [ 0.99553949  1.        ]]\n",
      "Minibatch loss at step 3150: 0.068423 Learning rate: 0.00424322\n",
      "Minibatch accuracy: 100.0\n",
      "Minibatch accuracy: 62.0\n",
      "Predictions | Labels:\n",
      " [[ 0.98514116  1.        ]\n",
      " [ 0.99944431  1.        ]]\n",
      "Minibatch loss at step 3165: 0.191906 Learning rate: 0.00424322\n",
      "Minibatch accuracy: 100.0\n",
      "Minibatch accuracy: 59.3333333333\n",
      "Predictions | Labels:\n",
      " [[ 0.27400896  0.        ]\n",
      " [ 0.27046129  0.        ]]\n",
      "Minibatch loss at step 3180: 0.166973 Learning rate: 0.00424322\n",
      "Minibatch accuracy: 100.0\n",
      "Minibatch accuracy: 58.6666666667\n",
      "Predictions | Labels:\n",
      " [[ 0.70056707  1.        ]\n",
      " [ 0.99738282  1.        ]]\n",
      "Minibatch loss at step 3195: 0.384238 Learning rate: 0.00424322\n",
      "Minibatch accuracy: 81.25\n",
      "Minibatch accuracy: 59.3333333333\n",
      "Predictions | Labels:\n",
      " [[ 0.26894173  0.        ]\n",
      " [ 0.97779751  1.        ]]\n",
      "Minibatch loss at step 3210: 0.114416 Learning rate: 0.00424322\n",
      "Minibatch accuracy: 100.0\n",
      "Minibatch accuracy: 60.0\n",
      "Predictions | Labels:\n",
      " [[ 0.99974877  1.        ]\n",
      " [ 0.99744499  1.        ]]\n",
      "Minibatch loss at step 3225: 0.189981 Learning rate: 0.00424322\n",
      "Minibatch accuracy: 93.75\n",
      "Minibatch accuracy: 61.3333333333\n",
      "Predictions | Labels:\n",
      " [[ 0.2698946   0.        ]\n",
      " [ 0.98213965  1.        ]]\n",
      "Minibatch loss at step 3240: 0.169673 Learning rate: 0.00424322\n",
      "Minibatch accuracy: 100.0\n",
      "Minibatch accuracy: 60.0\n",
      "Predictions | Labels:\n",
      " [[ 0.29118168  0.        ]\n",
      " [ 0.99982709  1.        ]]\n",
      "Minibatch loss at step 3255: 0.149747 Learning rate: 0.00424322\n",
      "Minibatch accuracy: 100.0\n",
      "Minibatch accuracy: 59.3333333333\n",
      "Predictions | Labels:\n",
      " [[ 0.981354    1.        ]\n",
      " [ 0.96170825  1.        ]]\n",
      "Minibatch loss at step 3270: 0.214526 Learning rate: 0.00424322\n",
      "Minibatch accuracy: 93.75\n",
      "Minibatch accuracy: 60.6666666667\n",
      "Predictions | Labels:\n",
      " [[ 0.95245552  1.        ]\n",
      " [ 0.27310061  0.        ]]\n",
      "Minibatch loss at step 3285: 0.212237 Learning rate: 0.00424322\n",
      "Minibatch accuracy: 100.0\n",
      "Minibatch accuracy: 59.3333333333\n",
      "Predictions | Labels:\n",
      " [[ 0.28727141  0.        ]\n",
      " [ 0.27397642  0.        ]]\n",
      "Minibatch loss at step 3300: 0.271904 Learning rate: 0.00407349\n",
      "Minibatch accuracy: 93.75\n",
      "Minibatch accuracy: 58.6666666667\n",
      "Predictions | Labels:\n",
      " [[ 0.99641317  1.        ]\n",
      " [ 0.28316954  0.        ]]\n",
      "Minibatch loss at step 3315: 0.259724 Learning rate: 0.00407349\n",
      "Minibatch accuracy: 93.75\n",
      "Minibatch accuracy: 60.6666666667\n",
      "Predictions | Labels:\n",
      " [[ 0.96982175  1.        ]\n",
      " [ 0.27636787  0.        ]]\n",
      "Minibatch loss at step 3330: 0.215061 Learning rate: 0.00407349\n",
      "Minibatch accuracy: 100.0\n",
      "Minibatch accuracy: 61.3333333333\n",
      "Predictions | Labels:\n",
      " [[ 0.27628505  0.        ]\n",
      " [ 0.27233237  0.        ]]\n",
      "Minibatch loss at step 3345: 0.126906 Learning rate: 0.00407349\n",
      "Minibatch accuracy: 100.0\n",
      "Minibatch accuracy: 61.3333333333\n",
      "Predictions | Labels:\n",
      " [[ 0.99967182  1.        ]\n",
      " [ 0.27490878  0.        ]]\n",
      "Minibatch loss at step 3360: 0.246002 Learning rate: 0.00407349\n",
      "Minibatch accuracy: 93.75\n",
      "Minibatch accuracy: 61.3333333333\n",
      "Predictions | Labels:\n",
      " [[ 0.28229299  0.        ]\n",
      " [ 0.27033651  0.        ]]\n",
      "Minibatch loss at step 3375: 0.148870 Learning rate: 0.00407349\n",
      "Minibatch accuracy: 100.0\n",
      "Minibatch accuracy: 59.3333333333\n",
      "Predictions | Labels:\n",
      " [[ 0.96864289  1.        ]\n",
      " [ 0.26973525  0.        ]]\n",
      "Minibatch loss at step 3390: 0.171285 Learning rate: 0.00407349\n",
      "Minibatch accuracy: 100.0\n",
      "Minibatch accuracy: 59.3333333333\n",
      "Predictions | Labels:\n",
      " [[ 0.27605543  0.        ]\n",
      " [ 0.28078148  0.        ]]\n",
      "Minibatch loss at step 3405: 0.143807 Learning rate: 0.00407349\n",
      "Minibatch accuracy: 100.0\n",
      "Minibatch accuracy: 61.3333333333\n",
      "Predictions | Labels:\n",
      " [[ 0.99524176  1.        ]\n",
      " [ 0.29192021  0.        ]]\n",
      "Minibatch loss at step 3420: 0.222372 Learning rate: 0.00407349\n",
      "Minibatch accuracy: 100.0\n",
      "Minibatch accuracy: 59.3333333333\n",
      "Predictions | Labels:\n",
      " [[ 0.27174798  0.        ]\n",
      " [ 0.26911911  0.        ]]\n",
      "Minibatch loss at step 3435: 0.146551 Learning rate: 0.00407349\n",
      "Minibatch accuracy: 100.0\n",
      "Minibatch accuracy: 59.3333333333\n",
      "Predictions | Labels:\n",
      " [[ 0.27324152  0.        ]\n",
      " [ 0.28425843  0.        ]]\n",
      "Minibatch loss at step 3450: 0.112252 Learning rate: 0.00391055\n",
      "Minibatch accuracy: 100.0\n",
      "Minibatch accuracy: 59.3333333333\n",
      "Predictions | Labels:\n",
      " [[ 0.95594811  1.        ]\n",
      " [ 0.99721271  1.        ]]\n",
      "Minibatch loss at step 3465: 0.127180 Learning rate: 0.00391055\n",
      "Minibatch accuracy: 100.0\n",
      "Minibatch accuracy: 59.3333333333\n",
      "Predictions | Labels:\n",
      " [[ 0.99992967  1.        ]\n",
      " [ 0.99686521  1.        ]]\n",
      "Minibatch loss at step 3480: 0.143818 Learning rate: 0.00391055\n",
      "Minibatch accuracy: 100.0\n",
      "Minibatch accuracy: 61.3333333333\n",
      "Predictions | Labels:\n",
      " [[ 0.27052125  0.        ]\n",
      " [ 0.98873293  1.        ]]\n",
      "Minibatch loss at step 3495: 0.205760 Learning rate: 0.00391055\n",
      "Minibatch accuracy: 100.0\n",
      "Minibatch accuracy: 59.3333333333\n",
      "Predictions | Labels:\n",
      " [[ 0.99742365  1.        ]\n",
      " [ 0.9849866   1.        ]]\n",
      "Minibatch loss at step 3510: 0.109461 Learning rate: 0.00391055\n",
      "Minibatch accuracy: 100.0\n",
      "Minibatch accuracy: 59.3333333333\n",
      "Predictions | Labels:\n",
      " [[ 0.9949621   1.        ]\n",
      " [ 0.99117082  1.        ]]\n",
      "Minibatch loss at step 3525: 0.355587 Learning rate: 0.00391055\n",
      "Minibatch accuracy: 87.5\n",
      "Minibatch accuracy: 59.3333333333\n",
      "Predictions | Labels:\n",
      " [[ 0.26912284  0.        ]\n",
      " [ 0.2696822   1.        ]]\n",
      "Minibatch loss at step 3540: 0.190946 Learning rate: 0.00391055\n",
      "Minibatch accuracy: 93.75\n",
      "Minibatch accuracy: 59.3333333333\n",
      "Predictions | Labels:\n",
      " [[ 0.26918504  1.        ]\n",
      " [ 0.27916816  0.        ]]\n",
      "Minibatch loss at step 3555: 0.126976 Learning rate: 0.00391055\n",
      "Minibatch accuracy: 100.0\n",
      "Minibatch accuracy: 59.3333333333\n",
      "Predictions | Labels:\n",
      " [[ 0.27591011  0.        ]\n",
      " [ 0.99872655  1.        ]]\n",
      "Minibatch loss at step 3570: 0.265319 Learning rate: 0.00391055\n",
      "Minibatch accuracy: 100.0\n",
      "Minibatch accuracy: 59.3333333333\n",
      "Predictions | Labels:\n",
      " [[ 0.2842043   0.        ]\n",
      " [ 0.26975974  0.        ]]\n",
      "Minibatch loss at step 3585: 0.274203 Learning rate: 0.00391055\n",
      "Minibatch accuracy: 87.5\n",
      "Minibatch accuracy: 59.3333333333\n",
      "Predictions | Labels:\n",
      " [[ 0.98587137  1.        ]\n",
      " [ 0.2697587   0.        ]]\n",
      "Minibatch loss at step 3600: 0.152493 Learning rate: 0.00375413\n",
      "Minibatch accuracy: 93.75\n",
      "Minibatch accuracy: 59.3333333333\n",
      "Predictions | Labels:\n",
      " [[ 0.99619663  1.        ]\n",
      " [ 0.99571639  1.        ]]\n",
      "Minibatch loss at step 3615: 0.149836 Learning rate: 0.00375413\n",
      "Minibatch accuracy: 100.0\n",
      "Minibatch accuracy: 61.3333333333\n",
      "Predictions | Labels:\n",
      " [[ 0.27530584  0.        ]\n",
      " [ 0.27653447  0.        ]]\n",
      "Minibatch loss at step 3630: 0.386455 Learning rate: 0.00375413\n",
      "Minibatch accuracy: 81.25\n",
      "Minibatch accuracy: 58.6666666667\n",
      "Predictions | Labels:\n",
      " [[ 0.26916295  0.        ]\n",
      " [ 0.95490485  1.        ]]\n",
      "Minibatch loss at step 3645: 0.249231 Learning rate: 0.00375413\n",
      "Minibatch accuracy: 93.75\n",
      "Minibatch accuracy: 60.0\n",
      "Predictions | Labels:\n",
      " [[ 0.98749906  1.        ]\n",
      " [ 0.27045736  0.        ]]\n",
      "Minibatch loss at step 3660: 0.211482 Learning rate: 0.00375413\n",
      "Minibatch accuracy: 100.0\n",
      "Minibatch accuracy: 59.3333333333\n",
      "Predictions | Labels:\n",
      " [[ 0.9999038   1.        ]\n",
      " [ 0.28451192  0.        ]]\n",
      "Minibatch loss at step 3675: 0.154426 Learning rate: 0.00375413\n",
      "Minibatch accuracy: 100.0\n",
      "Minibatch accuracy: 59.3333333333\n",
      "Predictions | Labels:\n",
      " [[ 0.28288516  0.        ]\n",
      " [ 0.26968431  0.        ]]\n",
      "Minibatch loss at step 3690: 0.142821 Learning rate: 0.00375413\n",
      "Minibatch accuracy: 100.0\n",
      "Minibatch accuracy: 59.3333333333\n",
      "Predictions | Labels:\n",
      " [[ 0.98732668  1.        ]\n",
      " [ 0.99724257  1.        ]]\n",
      "Minibatch loss at step 3705: 0.251533 Learning rate: 0.00375413\n",
      "Minibatch accuracy: 93.75\n",
      "Minibatch accuracy: 59.3333333333\n",
      "Predictions | Labels:\n",
      " [[ 0.95408887  1.        ]\n",
      " [ 0.99675542  1.        ]]\n",
      "Minibatch loss at step 3720: 0.167037 Learning rate: 0.00375413\n",
      "Minibatch accuracy: 100.0\n",
      "Minibatch accuracy: 60.6666666667\n",
      "Predictions | Labels:\n",
      " [[ 0.27230507  0.        ]\n",
      " [ 0.27389616  0.        ]]\n",
      "Minibatch loss at step 3735: 0.206218 Learning rate: 0.00375413\n",
      "Minibatch accuracy: 93.75\n",
      "Minibatch accuracy: 58.6666666667\n",
      "Predictions | Labels:\n",
      " [[ 0.99656624  1.        ]\n",
      " [ 0.26910594  1.        ]]\n",
      "Minibatch loss at step 3750: 0.284554 Learning rate: 0.00360397\n",
      "Minibatch accuracy: 93.75\n",
      "Minibatch accuracy: 60.0\n",
      "Predictions | Labels:\n",
      " [[ 0.2747525   0.        ]\n",
      " [ 0.27211052  1.        ]]\n",
      "Minibatch loss at step 3765: 0.266098 Learning rate: 0.00360397\n",
      "Minibatch accuracy: 93.75\n",
      "Minibatch accuracy: 60.0\n",
      "Predictions | Labels:\n",
      " [[ 0.26964644  1.        ]\n",
      " [ 0.99098927  1.        ]]\n",
      "Minibatch loss at step 3780: 0.266086 Learning rate: 0.00360397\n",
      "Minibatch accuracy: 93.75\n",
      "Minibatch accuracy: 59.3333333333\n",
      "Predictions | Labels:\n",
      " [[ 0.27077931  0.        ]\n",
      " [ 0.99953067  1.        ]]\n",
      "Minibatch loss at step 3795: 0.104148 Learning rate: 0.00360397\n",
      "Minibatch accuracy: 100.0\n",
      "Minibatch accuracy: 60.0\n",
      "Predictions | Labels:\n",
      " [[ 0.27172196  0.        ]\n",
      " [ 0.99962568  1.        ]]\n",
      "Minibatch loss at step 3810: 0.144066 Learning rate: 0.00360397\n",
      "Minibatch accuracy: 100.0\n",
      "Minibatch accuracy: 58.0\n",
      "Predictions | Labels:\n",
      " [[ 0.27057844  0.        ]\n",
      " [ 0.28155953  0.        ]]\n",
      "Minibatch loss at step 3825: 0.226927 Learning rate: 0.00360397\n",
      "Minibatch accuracy: 93.75\n",
      "Minibatch accuracy: 60.6666666667\n",
      "Predictions | Labels:\n",
      " [[ 0.28258044  0.        ]\n",
      " [ 0.99864191  1.        ]]\n",
      "Minibatch loss at step 3840: 0.309305 Learning rate: 0.00360397\n",
      "Minibatch accuracy: 87.5\n",
      "Minibatch accuracy: 58.6666666667\n",
      "Predictions | Labels:\n",
      " [[ 0.26909363  1.        ]\n",
      " [ 0.99531424  1.        ]]\n",
      "Minibatch loss at step 3855: 0.123524 Learning rate: 0.00360397\n",
      "Minibatch accuracy: 100.0\n",
      "Minibatch accuracy: 60.0\n",
      "Predictions | Labels:\n",
      " [[ 0.99808669  1.        ]\n",
      " [ 0.99640465  1.        ]]\n",
      "Minibatch loss at step 3870: 0.166156 Learning rate: 0.00360397\n",
      "Minibatch accuracy: 93.75\n",
      "Minibatch accuracy: 60.0\n",
      "Predictions | Labels:\n",
      " [[ 0.99951959  1.        ]\n",
      " [ 0.27429539  0.        ]]\n",
      "Minibatch loss at step 3885: 0.183627 Learning rate: 0.00360397\n",
      "Minibatch accuracy: 100.0\n",
      "Minibatch accuracy: 59.3333333333\n",
      "Predictions | Labels:\n",
      " [[ 0.27879125  0.        ]\n",
      " [ 0.99990451  1.        ]]\n",
      "Minibatch loss at step 3900: 0.104838 Learning rate: 0.00345981\n",
      "Minibatch accuracy: 100.0\n",
      "Minibatch accuracy: 60.0\n",
      "Predictions | Labels:\n",
      " [[ 0.996445    1.        ]\n",
      " [ 0.99240315  1.        ]]\n",
      "Minibatch loss at step 3915: 0.290363 Learning rate: 0.00345981\n",
      "Minibatch accuracy: 87.5\n",
      "Minibatch accuracy: 57.3333333333\n",
      "Predictions | Labels:\n",
      " [[ 0.99980611  1.        ]\n",
      " [ 0.28240049  0.        ]]\n",
      "Minibatch loss at step 3930: 0.187541 Learning rate: 0.00345981\n",
      "Minibatch accuracy: 100.0\n",
      "Minibatch accuracy: 61.3333333333\n",
      "Predictions | Labels:\n",
      " [[ 0.27924329  0.        ]\n",
      " [ 0.28083679  0.        ]]\n",
      "Minibatch loss at step 3945: 0.247134 Learning rate: 0.00345981\n",
      "Minibatch accuracy: 93.75\n",
      "Minibatch accuracy: 58.0\n",
      "Predictions | Labels:\n",
      " [[ 0.9999733  1.       ]\n",
      " [ 0.2766057  0.       ]]\n",
      "Minibatch loss at step 3960: 0.259942 Learning rate: 0.00345981\n",
      "Minibatch accuracy: 93.75\n",
      "Minibatch accuracy: 60.0\n",
      "Predictions | Labels:\n",
      " [[ 0.26948869  0.        ]\n",
      " [ 0.27142403  0.        ]]\n",
      "Minibatch loss at step 3975: 0.154249 Learning rate: 0.00345981\n",
      "Minibatch accuracy: 100.0\n",
      "Minibatch accuracy: 59.3333333333\n",
      "Predictions | Labels:\n",
      " [[ 0.99999118  1.        ]\n",
      " [ 0.27730471  0.        ]]\n",
      "Minibatch loss at step 3990: 0.125004 Learning rate: 0.00345981\n",
      "Minibatch accuracy: 100.0\n",
      "Minibatch accuracy: 59.3333333333\n",
      "Predictions | Labels:\n",
      " [[ 0.97155511  1.        ]\n",
      " [ 0.99988031  1.        ]]\n",
      "Model saved in file: ./checkpoints/model.ckpt\n",
      "Finished training\n"
     ]
    }
   ],
   "source": [
    "num_steps = 4001\n",
    "\n",
    "trace_file = open('./tracing/timeline.json', 'w')\n",
    "save_path = './checkpoints/model.ckpt'\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    tf.initialize_local_variables().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_dataset.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels}\n",
    "        _, l, lrate, predictions, summary = session.run(\n",
    "            [optimizer, loss, learning_rate, train_predictions, merged], \n",
    "            feed_dict=feed_dict, \n",
    "            options=run_options,\n",
    "            run_metadata=run_metadata)\n",
    "        results_writer.add_summary(summary, step)\n",
    "        if (step % 15 == 0):\n",
    "            print('Minibatch loss at step %d: %f' % (step, l), 'Learning rate:', lrate)\n",
    "            print('Minibatch accuracy:', accuracy(predictions, batch_labels.astype(np.bool_)))\n",
    "            print('Minibatch accuracy:', accuracy(valid_predictions.eval(), valid_labels.astype(np.bool_)))\n",
    "            print('Predictions | Labels:\\n', np.concatenate((predictions[:2], batch_labels[:2]), axis=1))\n",
    "            \n",
    "    # Save tracing into disl\n",
    "    trace = timeline.Timeline(step_stats=run_metadata.step_stats)\n",
    "    trace_file.write(trace.generate_chrome_trace_format(show_memory=True))\n",
    "            \n",
    "    # Save the variables to disk.\n",
    "    saver.save(session, save_path)\n",
    "    print(\"Model saved in file: %s\" % save_path)\n",
    "            \n",
    "    results_writer.flush()\n",
    "    results_writer.close()\n",
    "\n",
    "    print('Finished training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "valid_batch_size = 1\n",
    "\n",
    "def accuracy_notpercent(predictions, labels):\n",
    "  return np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    saver.restore(session, save_path)\n",
    "    print('Model Loaded')\n",
    "    data_split = np.array_split(valid_dataset, valid_dataset.shape[0]//valid_batch_size, axis=0)\n",
    "    labels_split = np.array_split(valid_labels, valid_labels.shape[0]//valid_batch_size, axis=0)\n",
    "    correct_predictions = 0\n",
    "    for idx, batch_data in enumerate(data_split):\n",
    "        correct_predictions += accuracy_notpercent(\n",
    "            train_prediction.eval(feed_dict={tf_train_dataset: batch_data}), \n",
    "            labels_split[idx])\n",
    "        print('accuracy:', (100.0*correct_predictions)/((idx+1)*valid_batch_size))\n",
    "        \n",
    "        \n",
    "    print('Finished validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
