{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import done\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from six.moves import cPickle as pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import timeline\n",
    "\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"import done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load pickled dataset into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unpickling ./data/trainsh1.pickle\n",
      "dataset shape: (29, 240000, 16)\n",
      "labels shape: (29,)\n"
     ]
    }
   ],
   "source": [
    "name_pickle = './data/trainsh1.pickle'\n",
    "\n",
    "with open(name_pickle, 'rb') as f:\n",
    "    print('Unpickling ' + name_pickle)\n",
    "    load = pickle.load(f)\n",
    "    dataset = load['data']\n",
    "    labels = load['labels']\n",
    "    del load\n",
    "    print('dataset shape:', dataset.shape)\n",
    "    print('labels shape:', labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reformat data for training\n",
    "- Divide each file with 240000 samples into smaller batch_samples ~= size of receptive field of eegnet\n",
    "- Keep valid_dataset nr of samples intact for proper validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset shape: (37262, 1, 128, 1) train_labels shape: (37262, 1) mix: 0.442488325908\n",
      "valid_dataset shape: (150, 1, 128, 1) valid_labels shape: (150, 1) mix: 0.833333333333\n"
     ]
    }
   ],
   "source": [
    "#Output size of the layer\n",
    "num_labels = 1\n",
    "\n",
    "#60% for train and 40% for validation\n",
    "split_idx = int(dataset.shape[0]*0.8)\n",
    "#nr of splits\n",
    "nrOfSplits = 1000\n",
    "\n",
    "def format_data(data, labels, nr_splits):\n",
    "    shape = data.shape\n",
    "    # stack 3D array into 2D\n",
    "    data = np.reshape(data, (shape[0]*shape[1], shape[2]))\n",
    "    # 3D array from 2D array by splitting 2D array into the desired smaller chuncks\n",
    "    data = np.asarray(np.split(data, shape[0]*nr_splits, axis=0))\n",
    "    # labels are obtained by repeating original labels nr_splits times\n",
    "    labels = np.repeat((np.arange(num_labels) == labels[:,None]).astype(np.float32), nr_splits, axis=0)\n",
    "    # eliminate batches that only contain drop-outs\n",
    "    data_tmp = list()\n",
    "    labels_tmp = list()\n",
    "    for idx, d in enumerate(data):\n",
    "        if (np.count_nonzero(d) < 10) or (np.std(d) < 0.01):\n",
    "            continue\n",
    "        data_tmp.append(d)\n",
    "        labels_tmp.append(labels[idx])\n",
    "    data = np.asarray(data_tmp)\n",
    "    labels = np.asarray(labels_tmp)\n",
    "    # data has to be 4D for tensorflow (insert an empty dimension)\n",
    "    data = data[:,None,:,:]\n",
    "    # shuffle data and labels mantaining relation between them\n",
    "    shuffle_idx = np.random.permutation(data.shape[0])\n",
    "    data = data[shuffle_idx,:,:,:]\n",
    "    labels = labels[shuffle_idx]\n",
    "    return data, labels\n",
    "\n",
    "# shuffle file data\n",
    "shuffle_idx = np.random.permutation(dataset.shape[0])\n",
    "dataset = dataset[shuffle_idx,:,:]\n",
    "labels = labels[shuffle_idx]\n",
    "# format and split data into smaller chunks\n",
    "train_dataset, train_labels = format_data(dataset[:split_idx], labels[:split_idx], nrOfSplits)\n",
    "valid_dataset, valid_labels = format_data(dataset[split_idx:-1], labels[split_idx:-1], nrOfSplits)\n",
    "del dataset, labels\n",
    "\n",
    "train_dataset = train_dataset[:,:,:,0]\n",
    "train_dataset = train_dataset[:,:,:,None]\n",
    "\n",
    "valid_dataset = valid_dataset[:150,:,:,0]\n",
    "valid_dataset = valid_dataset[:,:,:,None]\n",
    "valid_labels = valid_labels[:150]\n",
    "\n",
    "print('train_dataset shape:', train_dataset.shape, 'train_labels shape:', train_labels.shape, \n",
    "      'mix:', float(np.count_nonzero(train_labels, axis=0))/train_labels.shape[0])\n",
    "print('valid_dataset shape:', valid_dataset.shape, 'valid_labels shape:', valid_labels.shape, \n",
    "      'mix:', float(np.count_nonzero(valid_labels, axis=0))/valid_labels.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot some data to have an idea of how data looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f12f4da6fd0>]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAACGCAYAAAAo/HbcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f12fa2dd090>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplot(211)\n",
    "plt.plot(train_dataset[1,0,:,0])\n",
    "plt.plot(train_dataset[2,0,:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quantize signal\n",
    "\n",
    "Based on wavenet paper, raw signal is first mu-law transformed and then quantized to 256 possible values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset shape: (3999, 1, 1200, 1)\n",
      "valid_dataset shape: (150, 1, 1200, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f12f4e5aa50>]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAACGCAYAAADO1b2YAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f12efbab910>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAACGCAYAAAA7KzYoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f12f4f42a10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "quantization_levels = 256\n",
    "\n",
    "def quantize_data(data, levels):\n",
    "    levels -= 1\n",
    "    # mu law companding transformation\n",
    "    data = np.sign(data) * (np.log(1.0 + levels * np.absolute(data)) / np.log(1.0 + levels))\n",
    "    data = data/np.ptp(data, axis=0)\n",
    "    data = data + np.absolute(np.amin(data, axis=0))\n",
    "    # quantize signal to the amount of levels: data+1 makes only positive values\n",
    "    return (data*levels).astype(np.uint8)\n",
    "\n",
    "train_dataset_quant = quantize_data(train_dataset, quantization_levels)\n",
    "valid_dataset_quant = quantize_data(valid_dataset, quantization_levels)\n",
    "\n",
    "print('train_dataset shape:', train_dataset.shape)\n",
    "print('valid_dataset shape:', valid_dataset.shape)\n",
    "\n",
    "plt.subplot(211)\n",
    "plt.plot(train_dataset[0,0,:,0])\n",
    "plt.figure(2)\n",
    "plt.subplot(211)\n",
    "plt.plot(train_dataset_quant[0,0,:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot data after quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f12fa1f4790>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAACGCAYAAADZ9y1RAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMCiKoigBqGBQFEVRAlDBoCiKogTw/7YnIXkOpqpEAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f12fa341590>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAACGCAYAAADTh0FGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f12fa264ad0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplot(211)\n",
    "plt.plot(train_dataset[2,0,0:50,0])\n",
    "plt.figure(2)\n",
    "plt.subplot(211)\n",
    "plt.plot(train_dataset_quant[2,0,:50,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EEGNET implementation\n",
    "\n",
    "Ideas\n",
    "  - Condition classification based on sensor?\n",
    "  \n",
    "Part of https://arxiv.org/pdf/1609.03499.pdf that most concerns classification:\n",
    "\"As a last experiment we looked at speech recognition with WaveNets on the TIMIT (Garofolo et al., 1993) dataset. For this task we added a mean-pooling layer after the dilation convolutions that aggregated the activations to coarser frames spanning 10 milliseconds (160 x downsampling). The pooling layer was followed by a few non-causal convolutions. We trained WaveNet with two loss terms, one to predict the next sample and one to classify the frame, the model generalized better than with a single loss and achieved 18.8 PER on the test set, which is to our knowledge the best score obtained from a model trained directly on raw audio on TIMIT.\"\n",
    "\n",
    "Look into: http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43290.pdf\n",
    "\"Input: This layer extracts 275 ms waveform segments from each of M input microphones. Successive inputs are hopped by 10ms. At the 16kHz sampling rate used in our experiments each segment contains M X 4401 dimensions.\"\n",
    "...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum((predictions > 0.5) == labels)\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computational graph created\n"
     ]
    }
   ],
   "source": [
    "#How many files are supplied per batch.\n",
    "batch_size=16\n",
    "#Number of samples in each batch entry\n",
    "batch_samples=train_dataset.shape[2]\n",
    "#How many filters to learn for the input.\n",
    "input_channels=1\n",
    "#How many filters to learn for the residual.\n",
    "residual_channels=16\n",
    "#Hidden layer size for fully connected layer\n",
    "hidden_size=64\n",
    "# size after pooling layer\n",
    "pool_size = 600\n",
    "#number of steps after which learning rate is decayed\n",
    "decay_steps=40\n",
    "\n",
    "filter_width=3\n",
    "\n",
    "slim = tf.contrib.slim\n",
    "\n",
    "#Construct computation graph\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    \n",
    "    # Input data\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, 1, batch_samples, input_channels))\n",
    "    tf_train_labels = tf.placeholder(tf.uint8, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset, dtype=tf.float32)\n",
    "            \n",
    "    def network(batch_data, reuse=False, is_training=True):\n",
    "        with tf.variable_scope('eegnet_network', reuse=reuse):\n",
    "            with slim.arg_scope([slim.batch_norm], \n",
    "                                is_training=is_training):\n",
    "                with slim.arg_scope([slim.conv2d, slim.fully_connected], \n",
    "                                    activation_fn=tf.nn.relu, \n",
    "                                    weights_initializer=slim.xavier_initializer(), \n",
    "                                    normalizer_fn=slim.batch_norm):\n",
    "                    with tf.variable_scope('input_layer'):\n",
    "                        hidden = slim.conv2d(batch_data, residual_channels, [1, filter_width], stride=1, rate=1, scope='conv1')\n",
    "                        skip = hidden\n",
    "\n",
    "                    with tf.variable_scope('hidden_layers'):\n",
    "                        hidden = slim.conv2d(hidden, residual_channels, [1, filter_width], stride=1, rate=2, scope='conv1')\n",
    "                        skip = tf.add(skip, hidden)\n",
    "                        hidden = slim.conv2d(hidden, residual_channels, [1, filter_width], stride=1, rate=4, scope='conv2')\n",
    "                        skip = tf.add(skip, hidden)\n",
    "                        hidden = slim.conv2d(hidden, residual_channels, [1, filter_width], stride=1, rate=8, scope='conv3')\n",
    "                        skip = tf.add(skip, hidden)\n",
    "                        hidden = slim.conv2d(hidden, residual_channels, [1, filter_width], stride=1, rate=16, scope='conv4')\n",
    "                        skip = tf.add(skip, hidden)\n",
    "                        hidden = slim.conv2d(hidden, residual_channels, [1, filter_width], stride=1, rate=32, scope='conv5')\n",
    "                        skip = tf.add(skip, hidden)\n",
    "\n",
    "                    with tf.variable_scope('prediction'):\n",
    "                        predc = slim.conv2d(skip, quantization_levels, 1, scope='1x1_skip_out1')\n",
    "                        predc = slim.conv2d(predc, quantization_levels, 1, scope='1x1_skip_out2')\n",
    "\n",
    "                    with tf.variable_scope('classification'):\n",
    "                        hidden = slim.avg_pool2d(hidden, [1, batch_samples*2//pool_size], [1, batch_samples//pool_size])\n",
    "                        shape = hidden.get_shape().as_list()\n",
    "                        hidden = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "                        hidden = slim.fully_connected(hidden, hidden_size, scope='fc1')\n",
    "                        classf = slim.fully_connected(hidden, num_labels, scope='fc2')                    \n",
    "        return predc, classf \n",
    "\n",
    "    with tf.name_scope('eegnet_handling'):\n",
    "        with tf.name_scope('network'):\n",
    "            predictions, classification = network(tf_train_dataset)\n",
    "        with tf.name_scope('classification_loss'):\n",
    "            loss_class = slim.losses.sigmoid_cross_entropy(classification, tf_train_labels, scope='classification_loss')\n",
    "            tf.scalar_summary('classification_loss', loss_class)\n",
    "        with tf.name_scope('prediction_loss'):\n",
    "            # remove last predicted point\n",
    "            predictions = tf.slice(predictions, [0, 0, 0, 0], [-1, -1, batch_samples - 1, -1])\n",
    "            predictions = tf.reshape(predictions, [batch_size, (batch_samples - 1) * quantization_levels])\n",
    "            # remove first training sample\n",
    "            shift_data = tf.slice(tf_train_dataset, [0, 0, 1, 0], [-1, -1, -1, -1])\n",
    "            shift_data = tf.reshape(shift_data, [batch_size, (batch_samples - 1) * input_channels])\n",
    "            # one hot encoding training samples\n",
    "            shift_data = tf.cast(shift_data, tf.uint8) #one hot requires integer indices\n",
    "            shift_data = tf.one_hot(shift_data, quantization_levels)\n",
    "            shift_data = tf.reshape(shift_data, [batch_size, (batch_samples - 1) * quantization_levels])\n",
    "            # loss\n",
    "            loss_pred = slim.losses.softmax_cross_entropy(predictions, shift_data, scope='prediction_loss')\n",
    "            tf.scalar_summary('prediction_loss', loss_pred)\n",
    "        with tf.name_scope('total_loss'):\n",
    "            total_loss = slim.losses.get_total_loss(add_regularization_losses=False)\n",
    "            tf.scalar_summary('total_loss', total_loss)\n",
    "        with tf.name_scope('optimizer'):\n",
    "            global_step = tf.Variable(0)\n",
    "            learning_rate = tf.train.exponential_decay(1e-3, global_step, decay_steps, 0.96, staircase=True)\n",
    "            #optimizer = tf.train.RMSPropOptimizer(learning_rate, 0.9).minimize(loss, global_step=global_step)\n",
    "            optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(total_loss, global_step=global_step)\n",
    "            tf.scalar_summary('learning_rate', learning_rate)\n",
    "        with tf.name_scope('accuracy'):\n",
    "            #train_accuracy, _ = slim.metrics.streaming_auc(logits, tf_train_labels)\n",
    "            train_predictions = tf.nn.sigmoid(classification)\n",
    "            valid_predictions = tf.nn.sigmoid(network(tf_valid_dataset, True, True)[1])\n",
    "\n",
    "    # Add histograms for trainable variables.\n",
    "    for var in tf.trainable_variables():\n",
    "        tf.histogram_summary(var.op.name, var)\n",
    "    \n",
    "    #Merge all summaries and write to a folder\n",
    "    merged = tf.merge_all_summaries()\n",
    "    results_writer = tf.train.SummaryWriter('./results', graph)\n",
    "    \n",
    "    # Add ops to save and restore all the variables.\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    #tracing for timeline\n",
    "    run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "    run_metadata = tf.RunMetadata()    \n",
    "    \n",
    "print('computational graph created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch total | class | pred loss at step 0: 15473.956055 | 0.748838 | 15473.207031 Learning rate: 0.001\n",
      "Minibatch accuracy: 43.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Validation accuracy: 43.3333333333\n",
      "Minibatch total | class | pred loss at step 2: 15069.749023 | 0.947236 | 15068.801758 Learning rate: 0.001\n",
      "Minibatch accuracy: 43.75\n",
      "Predictions | Labels:\n",
      " [[ 0.73330057  0.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 4: 14907.757812 | 0.776386 | 14906.981445 Learning rate: 0.001\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.89969534  0.        ]\n",
      " [ 0.66496044  1.        ]]\n",
      "Minibatch total | class | pred loss at step 6: 14603.030273 | 0.669244 | 14602.361328 Learning rate: 0.001\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 8: 14267.717773 | 0.692851 | 14267.025391 Learning rate: 0.001\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5  1. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 10: 14133.799805 | 0.656621 | 14133.143555 Learning rate: 0.001\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.85526174  1.        ]]\n",
      "Minibatch total | class | pred loss at step 12: 14123.649414 | 0.748595 | 14122.900391 Learning rate: 0.001\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.65582997  0.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 14: 14199.666016 | 0.683356 | 14198.982422 Learning rate: 0.001\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5         1.        ]\n",
      " [ 0.62170607  0.        ]]\n",
      "Minibatch total | class | pred loss at step 16: 13894.009766 | 0.791797 | 13893.217773 Learning rate: 0.001\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.51914364  0.        ]]\n",
      "Minibatch total | class | pred loss at step 18: 13872.271484 | 0.635270 | 13871.635742 Learning rate: 0.001\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.65707976  1.        ]\n",
      " [ 0.5         1.        ]]\n",
      "Minibatch total | class | pred loss at step 20: 14063.931641 | 0.759680 | 14063.171875 Learning rate: 0.001\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5  1. ]\n",
      " [ 0.5  0. ]]\n",
      "Validation accuracy: 41.3333333333\n",
      "Minibatch total | class | pred loss at step 22: 13863.140625 | 0.604761 | 13862.536133 Learning rate: 0.001\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.51993209  0.        ]\n",
      " [ 0.5         1.        ]]\n",
      "Minibatch total | class | pred loss at step 24: 13849.897461 | 0.796286 | 13849.101562 Learning rate: 0.001\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.85950083  1.        ]\n",
      " [ 0.5         1.        ]]\n",
      "Minibatch total | class | pred loss at step 26: 14084.833984 | 0.898627 | 14083.935547 Learning rate: 0.001\n",
      "Minibatch accuracy: 37.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5         1.        ]\n",
      " [ 0.90714025  0.        ]]\n",
      "Minibatch total | class | pred loss at step 28: 13839.994141 | 0.923718 | 13839.070312 Learning rate: 0.001\n",
      "Minibatch accuracy: 31.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5        0.       ]\n",
      " [ 0.8219769  0.       ]]\n",
      "Minibatch total | class | pred loss at step 30: 13707.814453 | 0.804519 | 13707.009766 Learning rate: 0.001\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5         1.        ]\n",
      " [ 0.52751237  1.        ]]\n",
      "Minibatch total | class | pred loss at step 32: 13556.479492 | 0.662026 | 13555.817383 Learning rate: 0.001\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.84627545  0.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 34: 13858.559570 | 0.835551 | 13857.723633 Learning rate: 0.001\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.60016328  1.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 36: 13800.166992 | 0.798193 | 13799.369141 Learning rate: 0.001\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5        0.       ]\n",
      " [ 0.5735935  0.       ]]\n",
      "Minibatch total | class | pred loss at step 38: 13545.980469 | 0.816099 | 13545.164062 Learning rate: 0.001\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.72437406  0.        ]\n",
      " [ 0.89148837  1.        ]]\n",
      "Minibatch total | class | pred loss at step 40: 13724.979492 | 0.762632 | 13724.216797 Learning rate: 0.00096\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.71203071  1.        ]\n",
      " [ 0.61020273  0.        ]]\n",
      "Validation accuracy: 44.0\n",
      "Minibatch total | class | pred loss at step 42: 13310.964844 | 0.631159 | 13310.333984 Learning rate: 0.00096\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  1. ]]\n",
      "Minibatch total | class | pred loss at step 44: 13522.586914 | 0.702459 | 13521.884766 Learning rate: 0.00096\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 46: 13482.742188 | 0.814057 | 13481.927734 Learning rate: 0.00096\n",
      "Minibatch accuracy: 43.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5  1. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 48: 13598.481445 | 0.780042 | 13597.701172 Learning rate: 0.00096\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.58238345  0.        ]\n",
      " [ 0.58500189  1.        ]]\n",
      "Minibatch total | class | pred loss at step 50: 13645.037109 | 0.649793 | 13644.387695 Learning rate: 0.00096\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5  1. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 52: 13320.888672 | 0.710586 | 13320.177734 Learning rate: 0.00096\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 54: 13330.796875 | 0.714506 | 13330.082031 Learning rate: 0.00096\n",
      "Minibatch accuracy: 25.0\n",
      "Predictions | Labels:\n",
      " [[ 0.76349735  1.        ]\n",
      " [ 0.5         1.        ]]\n",
      "Minibatch total | class | pred loss at step 56: 14140.385742 | 0.854703 | 14139.531250 Learning rate: 0.00096\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.74503803  0.        ]\n",
      " [ 0.5         1.        ]]\n",
      "Minibatch total | class | pred loss at step 58: 13452.578125 | 0.848754 | 13451.729492 Learning rate: 0.00096\n",
      "Minibatch accuracy: 37.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.67659634  0.        ]]\n",
      "Minibatch total | class | pred loss at step 60: 13172.321289 | 0.702355 | 13171.619141 Learning rate: 0.00096\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.80913526  1.        ]]\n",
      "Validation accuracy: 50.0\n",
      "Minibatch total | class | pred loss at step 62: 13278.983398 | 0.688179 | 13278.294922 Learning rate: 0.00096\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5         1.        ]\n",
      " [ 0.64071339  0.        ]]\n",
      "Minibatch total | class | pred loss at step 64: 13304.166016 | 0.872150 | 13303.293945 Learning rate: 0.00096\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5  1. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 66: 13453.031250 | 0.666210 | 13452.365234 Learning rate: 0.00096\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.75405443  0.        ]]\n",
      "Minibatch total | class | pred loss at step 68: 13268.031250 | 0.878872 | 13267.152344 Learning rate: 0.00096\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 70: 13980.625977 | 0.924692 | 13979.701172 Learning rate: 0.00096\n",
      "Minibatch accuracy: 37.5\n",
      "Predictions | Labels:\n",
      " [[ 0.6961481  1.       ]\n",
      " [ 0.5        0.       ]]\n",
      "Minibatch total | class | pred loss at step 72: 13192.483398 | 0.671381 | 13191.812500 Learning rate: 0.00096\n",
      "Minibatch accuracy: 43.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.57811362  0.        ]]\n",
      "Minibatch total | class | pred loss at step 74: 13282.308594 | 0.824138 | 13281.484375 Learning rate: 0.00096\n",
      "Minibatch accuracy: 31.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5  1. ]\n",
      " [ 0.5  1. ]]\n",
      "Minibatch total | class | pred loss at step 76: 13428.165039 | 0.910935 | 13427.253906 Learning rate: 0.00096\n",
      "Minibatch accuracy: 43.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5       1.      ]\n",
      " [ 0.668612  0.      ]]\n",
      "Minibatch total | class | pred loss at step 78: 13361.377930 | 0.790614 | 13360.586914 Learning rate: 0.00096\n",
      "Minibatch accuracy: 43.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.81092709  1.        ]]\n",
      "Minibatch total | class | pred loss at step 80: 13017.609375 | 0.852160 | 13016.756836 Learning rate: 0.0009216\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.83661914  1.        ]]\n",
      "Validation accuracy: 44.6666666667\n",
      "Minibatch total | class | pred loss at step 82: 13485.594727 | 0.655478 | 13484.939453 Learning rate: 0.0009216\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.62843502  1.        ]]\n",
      "Minibatch total | class | pred loss at step 84: 13849.747070 | 0.952396 | 13848.794922 Learning rate: 0.0009216\n",
      "Minibatch accuracy: 25.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5        0.       ]\n",
      " [ 0.7859655  0.       ]]\n",
      "Minibatch total | class | pred loss at step 86: 13197.036133 | 0.732100 | 13196.303711 Learning rate: 0.0009216\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.59897345  0.        ]\n",
      " [ 0.59783471  1.        ]]\n",
      "Minibatch total | class | pred loss at step 88: 13200.975586 | 0.864272 | 13200.111328 Learning rate: 0.0009216\n",
      "Minibatch accuracy: 37.5\n",
      "Predictions | Labels:\n",
      " [[ 0.6352787  0.       ]\n",
      " [ 0.5        0.       ]]\n",
      "Minibatch total | class | pred loss at step 90: 12882.031250 | 0.906382 | 12881.125000 Learning rate: 0.0009216\n",
      "Minibatch accuracy: 37.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5  1. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 92: 13716.160156 | 0.806381 | 13715.353516 Learning rate: 0.0009216\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.571356    1.        ]\n",
      " [ 0.68665957  0.        ]]\n",
      "Minibatch total | class | pred loss at step 94: 13002.418945 | 0.788318 | 13001.630859 Learning rate: 0.0009216\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.57605994  1.        ]\n",
      " [ 0.85789436  0.        ]]\n",
      "Minibatch total | class | pred loss at step 96: 12968.693359 | 0.714381 | 12967.978516 Learning rate: 0.0009216\n",
      "Minibatch accuracy: 37.5\n",
      "Predictions | Labels:\n",
      " [[ 0.82317692  1.        ]\n",
      " [ 0.5         1.        ]]\n",
      "Minibatch total | class | pred loss at step 98: 13222.107422 | 0.896237 | 13221.210938 Learning rate: 0.0009216\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.72660309  0.        ]\n",
      " [ 0.52179301  1.        ]]\n",
      "Minibatch total | class | pred loss at step 100: 12707.833984 | 0.658144 | 12707.175781 Learning rate: 0.0009216\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Validation accuracy: 43.3333333333\n",
      "Minibatch total | class | pred loss at step 102: 13533.543945 | 0.839436 | 13532.704102 Learning rate: 0.0009216\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5        0.       ]\n",
      " [ 0.8205874  0.       ]]\n",
      "Minibatch total | class | pred loss at step 104: 13531.817383 | 0.682474 | 13531.134766 Learning rate: 0.0009216\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.52604276  1.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 106: 12622.048828 | 0.741710 | 12621.306641 Learning rate: 0.0009216\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  1. ]]\n",
      "Minibatch total | class | pred loss at step 108: 13576.214844 | 0.756287 | 13575.458984 Learning rate: 0.0009216\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5         1.        ]\n",
      " [ 0.64395523  1.        ]]\n",
      "Minibatch total | class | pred loss at step 110: 13007.872070 | 0.864235 | 13007.007812 Learning rate: 0.0009216\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5  1. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 112: 12845.094727 | 0.796992 | 12844.297852 Learning rate: 0.0009216\n",
      "Minibatch accuracy: 37.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5         1.        ]\n",
      " [ 0.62195104  0.        ]]\n",
      "Minibatch total | class | pred loss at step 114: 12869.256836 | 0.831205 | 12868.425781 Learning rate: 0.0009216\n",
      "Minibatch accuracy: 37.5\n",
      "Predictions | Labels:\n",
      " [[ 0.62668198  0.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 116: 13029.149414 | 0.593892 | 13028.555664 Learning rate: 0.0009216\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5  1. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 118: 12930.628906 | 0.818746 | 12929.810547 Learning rate: 0.0009216\n",
      "Minibatch accuracy: 37.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5         1.        ]\n",
      " [ 0.70218313  1.        ]]\n",
      "Minibatch total | class | pred loss at step 120: 13011.625977 | 0.790006 | 13010.835938 Learning rate: 0.000884736\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  1. ]]\n",
      "Validation accuracy: 48.0\n",
      "Minibatch total | class | pred loss at step 122: 13174.018555 | 0.719258 | 13173.298828 Learning rate: 0.000884736\n",
      "Minibatch accuracy: 37.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5         1.        ]\n",
      " [ 0.94488436  1.        ]]\n",
      "Minibatch total | class | pred loss at step 124: 13080.632812 | 0.636883 | 13079.996094 Learning rate: 0.000884736\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.60575604  1.        ]\n",
      " [ 0.59692466  1.        ]]\n",
      "Minibatch total | class | pred loss at step 126: 13986.233398 | 0.655207 | 13985.578125 Learning rate: 0.000884736\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5  1. ]\n",
      " [ 0.5  1. ]]\n",
      "Minibatch total | class | pred loss at step 128: 12716.008789 | 0.660920 | 12715.347656 Learning rate: 0.000884736\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.55910277  1.        ]]\n",
      "Minibatch total | class | pred loss at step 130: 13207.148438 | 0.635149 | 13206.513672 Learning rate: 0.000884736\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.51754731  1.        ]\n",
      " [ 0.5         1.        ]]\n",
      "Minibatch total | class | pred loss at step 132: 13183.694336 | 0.739121 | 13182.955078 Learning rate: 0.000884736\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 134: 13350.581055 | 0.855998 | 13349.724609 Learning rate: 0.000884736\n",
      "Minibatch accuracy: 37.5\n",
      "Predictions | Labels:\n",
      " [[ 0.6178385   0.        ]\n",
      " [ 0.56143707  1.        ]]\n",
      "Minibatch total | class | pred loss at step 136: 13022.923828 | 0.921716 | 13022.001953 Learning rate: 0.000884736\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 138: 12693.429688 | 0.773255 | 12692.656250 Learning rate: 0.000884736\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.66122276  0.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 140: 13166.323242 | 0.736882 | 13165.585938 Learning rate: 0.000884736\n",
      "Minibatch accuracy: 43.75\n",
      "Predictions | Labels:\n",
      " [[ 0.7932927   1.        ]\n",
      " [ 0.81648082  0.        ]]\n",
      "Validation accuracy: 49.3333333333\n",
      "Minibatch total | class | pred loss at step 142: 12935.546875 | 0.806327 | 12934.740234 Learning rate: 0.000884736\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.79098016  0.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 144: 12892.359375 | 0.636568 | 12891.722656 Learning rate: 0.000884736\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5541507  0.       ]\n",
      " [ 0.5        0.       ]]\n",
      "Minibatch total | class | pred loss at step 146: 13515.170898 | 0.755144 | 13514.416016 Learning rate: 0.000884736\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.77146488  1.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 148: 12527.010742 | 0.822836 | 12526.187500 Learning rate: 0.000884736\n",
      "Minibatch accuracy: 31.25\n",
      "Predictions | Labels:\n",
      " [[ 0.56461638  1.        ]\n",
      " [ 0.63324308  1.        ]]\n",
      "Minibatch total | class | pred loss at step 150: 12716.768555 | 0.654955 | 12716.113281 Learning rate: 0.000884736\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.61530697  1.        ]\n",
      " [ 0.57955927  1.        ]]\n",
      "Minibatch total | class | pred loss at step 152: 12802.262695 | 0.877594 | 12801.384766 Learning rate: 0.000884736\n",
      "Minibatch accuracy: 43.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5         1.        ]\n",
      " [ 0.58142847  1.        ]]\n",
      "Minibatch total | class | pred loss at step 154: 12963.207031 | 0.644248 | 12962.562500 Learning rate: 0.000884736\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.75286645  1.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 156: 12757.095703 | 0.605405 | 12756.490234 Learning rate: 0.000884736\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.85969043  1.        ]\n",
      " [ 0.56477445  0.        ]]\n",
      "Minibatch total | class | pred loss at step 158: 13166.452148 | 0.740015 | 13165.711914 Learning rate: 0.000884736\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.75095236  1.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 160: 12742.384766 | 0.849866 | 12741.535156 Learning rate: 0.000849347\n",
      "Minibatch accuracy: 43.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5         1.        ]\n",
      " [ 0.69829822  0.        ]]\n",
      "Validation accuracy: 51.3333333333\n",
      "Minibatch total | class | pred loss at step 162: 13168.856445 | 0.686731 | 13168.169922 Learning rate: 0.000849347\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.61894137  0.        ]]\n",
      "Minibatch total | class | pred loss at step 164: 12732.851562 | 0.648835 | 12732.203125 Learning rate: 0.000849347\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5  1. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 166: 13016.921875 | 0.851095 | 13016.070312 Learning rate: 0.000849347\n",
      "Minibatch accuracy: 43.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 168: 12984.138672 | 0.572066 | 12983.566406 Learning rate: 0.000849347\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  1. ]]\n",
      "Minibatch total | class | pred loss at step 170: 12702.462891 | 0.697322 | 12701.765625 Learning rate: 0.000849347\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5        1.       ]\n",
      " [ 0.5814302  1.       ]]\n",
      "Minibatch total | class | pred loss at step 172: 13127.722656 | 0.609314 | 13127.113281 Learning rate: 0.000849347\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.65327668  1.        ]]\n",
      "Minibatch total | class | pred loss at step 174: 13350.708984 | 0.601551 | 13350.107422 Learning rate: 0.000849347\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5         1.        ]\n",
      " [ 0.54358757  1.        ]]\n",
      "Minibatch total | class | pred loss at step 176: 13409.987305 | 0.761971 | 13409.225586 Learning rate: 0.000849347\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.64290679  0.        ]]\n",
      "Minibatch total | class | pred loss at step 178: 13035.457031 | 0.595656 | 13034.861328 Learning rate: 0.000849347\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.78394192  1.        ]\n",
      " [ 0.52482313  0.        ]]\n",
      "Minibatch total | class | pred loss at step 180: 12518.398438 | 0.596786 | 12517.801758 Learning rate: 0.000849347\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.68341851  1.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Validation accuracy: 48.6666666667\n",
      "Minibatch total | class | pred loss at step 182: 13103.631836 | 0.834582 | 13102.796875 Learning rate: 0.000849347\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5         1.        ]\n",
      " [ 0.54703635  0.        ]]\n",
      "Minibatch total | class | pred loss at step 184: 12970.454102 | 0.751501 | 12969.702148 Learning rate: 0.000849347\n",
      "Minibatch accuracy: 43.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 186: 13589.197266 | 0.763451 | 13588.433594 Learning rate: 0.000849347\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.90415341  0.        ]\n",
      " [ 0.57521826  0.        ]]\n",
      "Minibatch total | class | pred loss at step 188: 13474.918945 | 0.817220 | 13474.101562 Learning rate: 0.000849347\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.74199575  1.        ]\n",
      " [ 0.76473576  0.        ]]\n",
      "Minibatch total | class | pred loss at step 190: 12625.187500 | 0.752441 | 12624.435547 Learning rate: 0.000849347\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 192: 12910.839844 | 0.637442 | 12910.202148 Learning rate: 0.000849347\n",
      "Minibatch accuracy: 43.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.61087447  0.        ]]\n",
      "Minibatch total | class | pred loss at step 194: 12911.686523 | 0.754739 | 12910.931641 Learning rate: 0.000849347\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.83674097  0.        ]\n",
      " [ 0.63995373  1.        ]]\n",
      "Minibatch total | class | pred loss at step 196: 13099.387695 | 0.765960 | 13098.622070 Learning rate: 0.000849347\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5        1.       ]\n",
      " [ 0.7002176  0.       ]]\n",
      "Minibatch total | class | pred loss at step 198: 13245.479492 | 0.764781 | 13244.714844 Learning rate: 0.000849347\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.69929266  1.        ]]\n",
      "Minibatch total | class | pred loss at step 200: 13319.317383 | 0.624845 | 13318.692383 Learning rate: 0.000815373\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.58582777  0.        ]\n",
      " [ 0.5         1.        ]]\n",
      "Validation accuracy: 45.3333333333\n",
      "Minibatch total | class | pred loss at step 202: 12750.333984 | 0.618617 | 12749.715820 Learning rate: 0.000815373\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.50581974  1.        ]\n",
      " [ 0.50447071  1.        ]]\n",
      "Minibatch total | class | pred loss at step 204: 12829.331055 | 0.642077 | 12828.689453 Learning rate: 0.000815373\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.74846774  1.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 206: 12620.427734 | 0.727669 | 12619.700195 Learning rate: 0.000815373\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.83064884  1.        ]]\n",
      "Minibatch total | class | pred loss at step 208: 13269.063477 | 0.653359 | 13268.410156 Learning rate: 0.000815373\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.80229837  1.        ]\n",
      " [ 0.5         1.        ]]\n",
      "Minibatch total | class | pred loss at step 210: 13512.172852 | 0.775222 | 13511.397461 Learning rate: 0.000815373\n",
      "Minibatch accuracy: 43.75\n",
      "Predictions | Labels:\n",
      " [[ 0.85179663  0.        ]\n",
      " [ 0.86794555  1.        ]]\n",
      "Minibatch total | class | pred loss at step 212: 12609.164062 | 0.851807 | 12608.312500 Learning rate: 0.000815373\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.86125302  0.        ]\n",
      " [ 0.55991787  0.        ]]\n",
      "Minibatch total | class | pred loss at step 214: 13063.788086 | 0.750790 | 13063.037109 Learning rate: 0.000815373\n",
      "Minibatch accuracy: 37.5\n",
      "Predictions | Labels:\n",
      " [[ 0.6338824   0.        ]\n",
      " [ 0.65671396  1.        ]]\n",
      "Minibatch total | class | pred loss at step 216: 12802.945312 | 0.736434 | 12802.208984 Learning rate: 0.000815373\n",
      "Minibatch accuracy: 43.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5287323  0.       ]\n",
      " [ 0.5        1.       ]]\n",
      "Minibatch total | class | pred loss at step 218: 13360.773438 | 0.650758 | 13360.123047 Learning rate: 0.000815373\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  1. ]]\n",
      "Minibatch total | class | pred loss at step 220: 13660.256836 | 0.621338 | 13659.635742 Learning rate: 0.000815373\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.77703851  1.        ]]\n",
      "Validation accuracy: 52.0\n",
      "Minibatch total | class | pred loss at step 222: 13342.937500 | 0.852712 | 13342.084961 Learning rate: 0.000815373\n",
      "Minibatch accuracy: 37.5\n",
      "Predictions | Labels:\n",
      " [[ 0.76694131  1.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 224: 13319.061523 | 0.860489 | 13318.201172 Learning rate: 0.000815373\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.68592739  0.        ]]\n",
      "Minibatch total | class | pred loss at step 226: 13113.520508 | 0.632243 | 13112.888672 Learning rate: 0.000815373\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.55837291  0.        ]\n",
      " [ 0.81433088  1.        ]]\n",
      "Minibatch total | class | pred loss at step 228: 13350.445312 | 0.724438 | 13349.720703 Learning rate: 0.000815373\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5  1. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 230: 13064.429688 | 0.803220 | 13063.626953 Learning rate: 0.000815373\n",
      "Minibatch accuracy: 43.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.73200274  0.        ]]\n",
      "Minibatch total | class | pred loss at step 232: 13045.772461 | 0.711507 | 13045.060547 Learning rate: 0.000815373\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.88846034  1.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 234: 12620.322266 | 0.811607 | 12619.510742 Learning rate: 0.000815373\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5  1. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 236: 12211.632812 | 0.752755 | 12210.879883 Learning rate: 0.000815373\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5  1. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 238: 13107.009766 | 0.615642 | 13106.394531 Learning rate: 0.000815373\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.65692985  1.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 240: 13389.445312 | 0.730119 | 13388.714844 Learning rate: 0.000782758\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  1. ]]\n",
      "Validation accuracy: 46.0\n",
      "Minibatch total | class | pred loss at step 242: 12819.599609 | 0.890731 | 12818.708984 Learning rate: 0.000782758\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  1. ]]\n",
      "Minibatch total | class | pred loss at step 244: 12687.395508 | 0.873782 | 12686.521484 Learning rate: 0.000782758\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.64831054  0.        ]\n",
      " [ 0.52086163  1.        ]]\n",
      "Minibatch total | class | pred loss at step 246: 12681.614258 | 0.729603 | 12680.884766 Learning rate: 0.000782758\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.6341415   0.        ]\n",
      " [ 0.57110822  0.        ]]\n",
      "Minibatch total | class | pred loss at step 248: 13646.716797 | 0.747452 | 13645.969727 Learning rate: 0.000782758\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.64687365  1.        ]]\n",
      "Minibatch total | class | pred loss at step 250: 12982.664062 | 0.803051 | 12981.861328 Learning rate: 0.000782758\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 252: 12568.298828 | 0.941595 | 12567.357422 Learning rate: 0.000782758\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 254: 13690.049805 | 0.836717 | 13689.212891 Learning rate: 0.000782758\n",
      "Minibatch accuracy: 43.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5  1. ]\n",
      " [ 0.5  1. ]]\n",
      "Minibatch total | class | pred loss at step 256: 13237.132812 | 0.704479 | 13236.428711 Learning rate: 0.000782758\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 258: 12888.510742 | 0.714219 | 12887.796875 Learning rate: 0.000782758\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.77600414  0.        ]]\n",
      "Minibatch total | class | pred loss at step 260: 12833.907227 | 0.816488 | 12833.090820 Learning rate: 0.000782758\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5         1.        ]\n",
      " [ 0.76687884  1.        ]]\n",
      "Validation accuracy: 45.3333333333\n",
      "Minibatch total | class | pred loss at step 262: 13033.194336 | 0.752337 | 13032.442383 Learning rate: 0.000782758\n",
      "Minibatch accuracy: 37.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.85925084  1.        ]]\n",
      "Minibatch total | class | pred loss at step 264: 12347.729492 | 0.752043 | 12346.977539 Learning rate: 0.000782758\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.89388639  0.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 266: 12590.835938 | 0.726309 | 12590.109375 Learning rate: 0.000782758\n",
      "Minibatch accuracy: 25.0\n",
      "Predictions | Labels:\n",
      " [[ 0.64141732  0.        ]\n",
      " [ 0.9083758   1.        ]]\n",
      "Minibatch total | class | pred loss at step 268: 13742.213867 | 0.778101 | 13741.435547 Learning rate: 0.000782758\n",
      "Minibatch accuracy: 43.75\n",
      "Predictions | Labels:\n",
      " [[ 0.81564683  1.        ]\n",
      " [ 0.5         1.        ]]\n",
      "Minibatch total | class | pred loss at step 270: 12832.503906 | 0.773271 | 12831.730469 Learning rate: 0.000782758\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 272: 13223.651367 | 0.654298 | 13222.997070 Learning rate: 0.000782758\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5         1.        ]\n",
      " [ 0.73049951  0.        ]]\n",
      "Minibatch total | class | pred loss at step 274: 13533.321289 | 0.839542 | 13532.481445 Learning rate: 0.000782758\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5         1.        ]\n",
      " [ 0.51887351  1.        ]]\n",
      "Minibatch total | class | pred loss at step 276: 14249.048828 | 0.767612 | 14248.281250 Learning rate: 0.000782758\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  1. ]]\n",
      "Minibatch total | class | pred loss at step 278: 12906.530273 | 0.775174 | 12905.754883 Learning rate: 0.000782758\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.75973099  0.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 280: 13116.408203 | 0.821654 | 13115.586914 Learning rate: 0.000751447\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5  1. ]\n",
      " [ 0.5  1. ]]\n",
      "Validation accuracy: 45.3333333333\n",
      "Minibatch total | class | pred loss at step 282: 12962.492188 | 0.566256 | 12961.925781 Learning rate: 0.000751447\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  1. ]]\n",
      "Minibatch total | class | pred loss at step 284: 14250.680664 | 0.827694 | 14249.852539 Learning rate: 0.000751447\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.91172057  0.        ]]\n",
      "Minibatch total | class | pred loss at step 286: 13634.201172 | 0.768634 | 13633.432617 Learning rate: 0.000751447\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.54238647  0.        ]\n",
      " [ 0.92791319  0.        ]]\n",
      "Minibatch total | class | pred loss at step 288: 13282.344727 | 0.911387 | 13281.433594 Learning rate: 0.000751447\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.64934278  1.        ]\n",
      " [ 0.83347279  0.        ]]\n",
      "Minibatch total | class | pred loss at step 290: 13122.242188 | 0.802781 | 13121.439453 Learning rate: 0.000751447\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 292: 13187.272461 | 0.633789 | 13186.638672 Learning rate: 0.000751447\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.54637241  1.        ]\n",
      " [ 0.58785725  1.        ]]\n",
      "Minibatch total | class | pred loss at step 294: 12887.939453 | 0.748049 | 12887.191406 Learning rate: 0.000751447\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.56198394  1.        ]]\n",
      "Minibatch total | class | pred loss at step 296: 13001.618164 | 0.721701 | 13000.896484 Learning rate: 0.000751447\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.54772794  0.        ]\n",
      " [ 0.5         1.        ]]\n",
      "Minibatch total | class | pred loss at step 298: 13382.806641 | 0.648189 | 13382.158203 Learning rate: 0.000751447\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.66947269  1.        ]\n",
      " [ 0.5         1.        ]]\n",
      "Minibatch total | class | pred loss at step 300: 14411.628906 | 0.740075 | 14410.888672 Learning rate: 0.000751447\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  1. ]]\n",
      "Validation accuracy: 44.6666666667\n",
      "Minibatch total | class | pred loss at step 302: 13260.453125 | 0.695735 | 13259.757812 Learning rate: 0.000751447\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 304: 13162.714844 | 0.717920 | 13161.997070 Learning rate: 0.000751447\n",
      "Minibatch accuracy: 43.75\n",
      "Predictions | Labels:\n",
      " [[ 0.61315906  1.        ]\n",
      " [ 0.5         1.        ]]\n",
      "Minibatch total | class | pred loss at step 306: 12951.204102 | 0.762604 | 12950.441406 Learning rate: 0.000751447\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5  1. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 308: 13537.267578 | 0.750471 | 13536.517578 Learning rate: 0.000751447\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.59007221  0.        ]\n",
      " [ 0.55126143  0.        ]]\n",
      "Minibatch total | class | pred loss at step 310: 12791.481445 | 0.671142 | 12790.810547 Learning rate: 0.000751447\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.81492591  1.        ]\n",
      " [ 0.5         1.        ]]\n",
      "Minibatch total | class | pred loss at step 312: 12811.008789 | 0.690006 | 12810.318359 Learning rate: 0.000751447\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.6404196   0.        ]\n",
      " [ 0.75496125  0.        ]]\n",
      "Minibatch total | class | pred loss at step 314: 13111.453125 | 0.815732 | 13110.637695 Learning rate: 0.000751447\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.66700929  0.        ]]\n",
      "Minibatch total | class | pred loss at step 316: 13306.916992 | 0.578634 | 13306.337891 Learning rate: 0.000751447\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 318: 13409.360352 | 0.809684 | 13408.550781 Learning rate: 0.000751447\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 320: 14805.248047 | 0.935822 | 14804.312500 Learning rate: 0.000721389\n",
      "Minibatch accuracy: 43.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.63505715  0.        ]]\n",
      "Validation accuracy: 46.6666666667\n",
      "Minibatch total | class | pred loss at step 322: 12835.893555 | 0.655607 | 12835.238281 Learning rate: 0.000721389\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.85612124  1.        ]]\n",
      "Minibatch total | class | pred loss at step 324: 13174.759766 | 0.814002 | 13173.945312 Learning rate: 0.000721389\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5         1.        ]\n",
      " [ 0.60102689  1.        ]]\n",
      "Minibatch total | class | pred loss at step 326: 13338.003906 | 0.914136 | 13337.089844 Learning rate: 0.000721389\n",
      "Minibatch accuracy: 43.75\n",
      "Predictions | Labels:\n",
      " [[ 0.88705468  0.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 328: 13742.735352 | 0.716545 | 13742.018555 Learning rate: 0.000721389\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.74198133  1.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 330: 12598.462891 | 0.760151 | 12597.703125 Learning rate: 0.000721389\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.82023442  1.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 332: 13491.373047 | 0.642832 | 13490.730469 Learning rate: 0.000721389\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5  1. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 334: 14016.316406 | 0.914397 | 14015.402344 Learning rate: 0.000721389\n",
      "Minibatch accuracy: 37.5\n",
      "Predictions | Labels:\n",
      " [[ 0.66842389  0.        ]\n",
      " [ 0.61524945  0.        ]]\n",
      "Minibatch total | class | pred loss at step 336: 13857.696289 | 0.757034 | 13856.939453 Learning rate: 0.000721389\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.53740227  1.        ]\n",
      " [ 0.59619343  0.        ]]\n",
      "Minibatch total | class | pred loss at step 338: 12701.295898 | 0.807677 | 12700.488281 Learning rate: 0.000721389\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.52836418  0.        ]\n",
      " [ 0.71141505  0.        ]]\n",
      "Minibatch total | class | pred loss at step 340: 12919.748047 | 0.949503 | 12918.798828 Learning rate: 0.000721389\n",
      "Minibatch accuracy: 37.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Validation accuracy: 47.3333333333\n",
      "Minibatch total | class | pred loss at step 342: 13064.718750 | 0.818664 | 13063.900391 Learning rate: 0.000721389\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.56403446  0.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 344: 13227.825195 | 0.737516 | 13227.087891 Learning rate: 0.000721389\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.85000813  0.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 346: 12910.992188 | 0.658429 | 12910.333984 Learning rate: 0.000721389\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.71347523  1.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 348: 13458.129883 | 0.819635 | 13457.310547 Learning rate: 0.000721389\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.72128969  1.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 350: 12881.717773 | 0.589170 | 12881.128906 Learning rate: 0.000721389\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 352: 14120.462891 | 0.788450 | 14119.674805 Learning rate: 0.000721389\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.88994026  0.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 354: 13704.565430 | 0.625713 | 13703.939453 Learning rate: 0.000721389\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  1. ]]\n",
      "Minibatch total | class | pred loss at step 356: 12336.359375 | 0.617407 | 12335.742188 Learning rate: 0.000721389\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5  1. ]\n",
      " [ 0.5  1. ]]\n",
      "Minibatch total | class | pred loss at step 358: 14057.760742 | 0.653044 | 14057.107422 Learning rate: 0.000721389\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.55253506  1.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 360: 13448.855469 | 0.794773 | 13448.060547 Learning rate: 0.000692534\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.70933121  0.        ]]\n",
      "Validation accuracy: 50.0\n",
      "Minibatch total | class | pred loss at step 362: 12698.559570 | 0.723425 | 12697.835938 Learning rate: 0.000692534\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.50702149  0.        ]\n",
      " [ 0.80210006  0.        ]]\n",
      "Minibatch total | class | pred loss at step 364: 12758.532227 | 0.754576 | 12757.777344 Learning rate: 0.000692534\n",
      "Minibatch accuracy: 37.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.73686099  1.        ]]\n",
      "Minibatch total | class | pred loss at step 366: 12607.085938 | 0.636781 | 12606.449219 Learning rate: 0.000692534\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.80348265  0.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 368: 12436.408203 | 0.767357 | 12435.640625 Learning rate: 0.000692534\n",
      "Minibatch accuracy: 43.75\n",
      "Predictions | Labels:\n",
      " [[ 0.8038168  1.       ]\n",
      " [ 0.5        1.       ]]\n",
      "Minibatch total | class | pred loss at step 370: 13417.219727 | 0.711476 | 13416.507812 Learning rate: 0.000692534\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5         1.        ]\n",
      " [ 0.82466221  1.        ]]\n",
      "Minibatch total | class | pred loss at step 372: 13246.253906 | 0.644633 | 13245.609375 Learning rate: 0.000692534\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.89685905  1.        ]\n",
      " [ 0.53365487  0.        ]]\n",
      "Minibatch total | class | pred loss at step 374: 13977.851562 | 0.623661 | 13977.227539 Learning rate: 0.000692534\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.65874094  1.        ]\n",
      " [ 0.86603761  1.        ]]\n",
      "Minibatch total | class | pred loss at step 376: 15204.363281 | 0.575168 | 15203.788086 Learning rate: 0.000692534\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5         1.        ]\n",
      " [ 0.69877195  1.        ]]\n",
      "Minibatch total | class | pred loss at step 378: 13123.924805 | 0.596858 | 13123.328125 Learning rate: 0.000692534\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.56202012  1.        ]\n",
      " [ 0.5         1.        ]]\n",
      "Minibatch total | class | pred loss at step 380: 12706.718750 | 0.572770 | 12706.145508 Learning rate: 0.000692534\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5         1.        ]\n",
      " [ 0.50301433  1.        ]]\n",
      "Validation accuracy: 40.6666666667\n",
      "Minibatch total | class | pred loss at step 382: 12845.602539 | 0.739723 | 12844.863281 Learning rate: 0.000692534\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 384: 13931.248047 | 0.851316 | 13930.396484 Learning rate: 0.000692534\n",
      "Minibatch accuracy: 31.25\n",
      "Predictions | Labels:\n",
      " [[ 0.50245833  1.        ]\n",
      " [ 0.64530694  0.        ]]\n",
      "Minibatch total | class | pred loss at step 386: 13625.803711 | 0.688816 | 13625.115234 Learning rate: 0.000692534\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.53763664  1.        ]]\n",
      "Minibatch total | class | pred loss at step 388: 13710.439453 | 0.709304 | 13709.730469 Learning rate: 0.000692534\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 390: 13789.227539 | 0.767867 | 13788.459961 Learning rate: 0.000692534\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.75090253  0.        ]\n",
      " [ 0.63666242  1.        ]]\n",
      "Minibatch total | class | pred loss at step 392: 13435.823242 | 0.735545 | 13435.087891 Learning rate: 0.000692534\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 394: 12445.768555 | 0.573255 | 12445.195312 Learning rate: 0.000692534\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  1. ]]\n",
      "Minibatch total | class | pred loss at step 396: 13753.740234 | 0.737796 | 13753.001953 Learning rate: 0.000692534\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  1. ]]\n",
      "Minibatch total | class | pred loss at step 398: 12610.652344 | 0.734559 | 12609.917969 Learning rate: 0.000692534\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.63900584  1.        ]\n",
      " [ 0.62659967  1.        ]]\n",
      "Minibatch total | class | pred loss at step 400: 12770.432617 | 0.630821 | 12769.801758 Learning rate: 0.000664832\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.58725077  1.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Validation accuracy: 48.0\n",
      "Minibatch total | class | pred loss at step 402: 12930.761719 | 0.799133 | 12929.962891 Learning rate: 0.000664832\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.62231213  1.        ]\n",
      " [ 0.81395525  0.        ]]\n",
      "Minibatch total | class | pred loss at step 404: 12625.332031 | 0.641723 | 12624.690430 Learning rate: 0.000664832\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.51502049  0.        ]\n",
      " [ 0.6000272   1.        ]]\n",
      "Minibatch total | class | pred loss at step 406: 13637.115234 | 0.587299 | 13636.528320 Learning rate: 0.000664832\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.6094439  0.       ]\n",
      " [ 0.5        0.       ]]\n",
      "Minibatch total | class | pred loss at step 408: 13554.830078 | 0.753739 | 13554.076172 Learning rate: 0.000664832\n",
      "Minibatch accuracy: 43.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.65714449  0.        ]]\n",
      "Minibatch total | class | pred loss at step 410: 13276.461914 | 0.714925 | 13275.747070 Learning rate: 0.000664832\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.8011772  0.       ]\n",
      " [ 0.5        1.       ]]\n",
      "Minibatch total | class | pred loss at step 412: 12988.964844 | 0.708529 | 12988.255859 Learning rate: 0.000664832\n",
      "Minibatch accuracy: 43.75\n",
      "Predictions | Labels:\n",
      " [[ 0.78064382  0.        ]\n",
      " [ 0.5         1.        ]]\n",
      "Minibatch total | class | pred loss at step 414: 13757.656250 | 0.765713 | 13756.890625 Learning rate: 0.000664832\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.69464469  1.        ]]\n",
      "Minibatch total | class | pred loss at step 416: 12900.037109 | 0.809123 | 12899.227539 Learning rate: 0.000664832\n",
      "Minibatch accuracy: 43.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  1. ]]\n",
      "Minibatch total | class | pred loss at step 418: 13541.426758 | 0.590052 | 13540.836914 Learning rate: 0.000664832\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5  1. ]\n",
      " [ 0.5  1. ]]\n",
      "Minibatch total | class | pred loss at step 420: 14159.193359 | 0.714641 | 14158.478516 Learning rate: 0.000664832\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.62602246  1.        ]\n",
      " [ 0.5         1.        ]]\n",
      "Validation accuracy: 48.0\n",
      "Minibatch total | class | pred loss at step 422: 13942.557617 | 0.567053 | 13941.990234 Learning rate: 0.000664832\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.61433232  1.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 424: 13766.848633 | 0.544806 | 13766.303711 Learning rate: 0.000664832\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.7940383   1.        ]\n",
      " [ 0.52271312  0.        ]]\n",
      "Minibatch total | class | pred loss at step 426: 14059.644531 | 0.692268 | 14058.952148 Learning rate: 0.000664832\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  1. ]]\n",
      "Minibatch total | class | pred loss at step 428: 13851.814453 | 0.622813 | 13851.191406 Learning rate: 0.000664832\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5121547  0.       ]\n",
      " [ 0.5383957  0.       ]]\n",
      "Minibatch total | class | pred loss at step 430: 12570.625000 | 0.604102 | 12570.020508 Learning rate: 0.000664832\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  1. ]]\n",
      "Minibatch total | class | pred loss at step 432: 13297.679688 | 0.830678 | 13296.848633 Learning rate: 0.000664832\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  1. ]]\n",
      "Minibatch total | class | pred loss at step 434: 14041.182617 | 0.716003 | 14040.466797 Learning rate: 0.000664832\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.61566806  0.        ]]\n",
      "Minibatch total | class | pred loss at step 436: 13367.373047 | 0.644098 | 13366.728516 Learning rate: 0.000664832\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5091117  0.       ]\n",
      " [ 0.5        1.       ]]\n",
      "Minibatch total | class | pred loss at step 438: 13666.860352 | 0.786557 | 13666.074219 Learning rate: 0.000664832\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.72704077  0.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 440: 12840.316406 | 0.755359 | 12839.561523 Learning rate: 0.000638239\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.77524614  1.        ]]\n",
      "Validation accuracy: 48.6666666667\n",
      "Minibatch total | class | pred loss at step 442: 14001.870117 | 0.674825 | 14001.195312 Learning rate: 0.000638239\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  1. ]]\n",
      "Minibatch total | class | pred loss at step 444: 13129.972656 | 0.667310 | 13129.305664 Learning rate: 0.000638239\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.55752265  1.        ]\n",
      " [ 0.5         1.        ]]\n",
      "Minibatch total | class | pred loss at step 446: 13787.093750 | 0.710774 | 13786.382812 Learning rate: 0.000638239\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.69738227  0.        ]\n",
      " [ 0.65137994  0.        ]]\n",
      "Minibatch total | class | pred loss at step 448: 14135.408203 | 0.723118 | 14134.685547 Learning rate: 0.000638239\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.77836221  1.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 450: 13118.271484 | 0.621184 | 13117.650391 Learning rate: 0.000638239\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5  1. ]\n",
      " [ 0.5  1. ]]\n",
      "Minibatch total | class | pred loss at step 452: 14390.192383 | 0.565654 | 14389.626953 Learning rate: 0.000638239\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.73186177  1.        ]\n",
      " [ 0.5         1.        ]]\n",
      "Minibatch total | class | pred loss at step 454: 13005.346680 | 0.680892 | 13004.666016 Learning rate: 0.000638239\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.64040613  1.        ]]\n",
      "Minibatch total | class | pred loss at step 456: 13507.223633 | 0.698961 | 13506.524414 Learning rate: 0.000638239\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.8759253  1.       ]\n",
      " [ 0.5        0.       ]]\n",
      "Minibatch total | class | pred loss at step 458: 13177.516602 | 0.717338 | 13176.798828 Learning rate: 0.000638239\n",
      "Minibatch accuracy: 43.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5  1. ]\n",
      " [ 0.5  1. ]]\n",
      "Minibatch total | class | pred loss at step 460: 13252.042969 | 0.685998 | 13251.357422 Learning rate: 0.000638239\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.91661745  1.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Validation accuracy: 49.3333333333\n",
      "Minibatch total | class | pred loss at step 462: 12946.210938 | 0.731574 | 12945.479492 Learning rate: 0.000638239\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.59391737  0.        ]]\n",
      "Minibatch total | class | pred loss at step 464: 13328.937500 | 0.729051 | 13328.208008 Learning rate: 0.000638239\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.66989964  1.        ]\n",
      " [ 0.54707265  1.        ]]\n",
      "Minibatch total | class | pred loss at step 466: 12503.605469 | 0.703087 | 12502.902344 Learning rate: 0.000638239\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5         1.        ]\n",
      " [ 0.56048894  0.        ]]\n",
      "Minibatch total | class | pred loss at step 468: 13446.347656 | 0.625944 | 13445.721680 Learning rate: 0.000638239\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5  1. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 470: 14614.093750 | 0.709591 | 14613.383789 Learning rate: 0.000638239\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.71078295  1.        ]\n",
      " [ 0.59331143  1.        ]]\n",
      "Minibatch total | class | pred loss at step 472: 13755.462891 | 0.899911 | 13754.562500 Learning rate: 0.000638239\n",
      "Minibatch accuracy: 31.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.61579245  0.        ]]\n",
      "Minibatch total | class | pred loss at step 474: 13758.711914 | 0.856648 | 13757.855469 Learning rate: 0.000638239\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.63859957  0.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 476: 12908.434570 | 0.577541 | 12907.857422 Learning rate: 0.000638239\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.74382716  1.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 478: 14616.201172 | 0.722165 | 14615.479492 Learning rate: 0.000638239\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  1. ]]\n",
      "Minibatch total | class | pred loss at step 480: 14497.573242 | 0.769056 | 14496.803711 Learning rate: 0.00061271\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.69481635  0.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Validation accuracy: 48.6666666667\n",
      "Minibatch total | class | pred loss at step 482: 13889.349609 | 0.844563 | 13888.504883 Learning rate: 0.00061271\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 484: 12745.665039 | 0.821470 | 12744.843750 Learning rate: 0.00061271\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 486: 12039.424805 | 0.675096 | 12038.750000 Learning rate: 0.00061271\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 488: 13855.593750 | 0.627443 | 13854.965820 Learning rate: 0.00061271\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.60684359  0.        ]]\n",
      "Minibatch total | class | pred loss at step 490: 14557.319336 | 0.698169 | 14556.621094 Learning rate: 0.00061271\n",
      "Minibatch accuracy: 37.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5  1. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 492: 13163.072266 | 0.832804 | 13162.239258 Learning rate: 0.00061271\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5         1.        ]\n",
      " [ 0.93762904  0.        ]]\n",
      "Minibatch total | class | pred loss at step 494: 12715.841797 | 0.817465 | 12715.024414 Learning rate: 0.00061271\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.64459068  1.        ]\n",
      " [ 0.5         1.        ]]\n",
      "Minibatch total | class | pred loss at step 496: 12574.862305 | 0.713824 | 12574.148438 Learning rate: 0.00061271\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.62044859  0.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 498: 14619.206055 | 0.657430 | 14618.548828 Learning rate: 0.00061271\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.76864964  1.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 500: 13616.581055 | 0.760531 | 13615.820312 Learning rate: 0.00061271\n",
      "Minibatch accuracy: 37.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.69079608  0.        ]]\n",
      "Validation accuracy: 50.0\n",
      "Minibatch total | class | pred loss at step 502: 13245.082031 | 0.923576 | 13244.158203 Learning rate: 0.00061271\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 504: 14153.977539 | 0.817804 | 14153.160156 Learning rate: 0.00061271\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5  1. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 506: 14778.149414 | 0.670711 | 14777.478516 Learning rate: 0.00061271\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.63834113  0.        ]]\n",
      "Minibatch total | class | pred loss at step 508: 12842.976562 | 0.658118 | 12842.318359 Learning rate: 0.00061271\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.74533135  0.        ]\n",
      " [ 0.5         1.        ]]\n",
      "Minibatch total | class | pred loss at step 510: 12463.530273 | 0.733106 | 12462.796875 Learning rate: 0.00061271\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.79715097  1.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 512: 13239.241211 | 0.674501 | 13238.566406 Learning rate: 0.00061271\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.87858301  1.        ]\n",
      " [ 0.5         1.        ]]\n",
      "Minibatch total | class | pred loss at step 514: 12719.306641 | 0.683628 | 12718.623047 Learning rate: 0.00061271\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 516: 13321.654297 | 0.676464 | 13320.977539 Learning rate: 0.00061271\n",
      "Minibatch accuracy: 37.5\n",
      "Predictions | Labels:\n",
      " [[ 0.9370935  1.       ]\n",
      " [ 0.5        0.       ]]\n",
      "Minibatch total | class | pred loss at step 518: 15729.798828 | 0.763283 | 15729.035156 Learning rate: 0.00061271\n",
      "Minibatch accuracy: 37.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5  1. ]\n",
      " [ 0.5  1. ]]\n",
      "Minibatch total | class | pred loss at step 520: 14549.450195 | 0.709816 | 14548.740234 Learning rate: 0.000588201\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Validation accuracy: 50.6666666667\n",
      "Minibatch total | class | pred loss at step 522: 13191.601562 | 0.621051 | 13190.980469 Learning rate: 0.000588201\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.67410618  0.        ]\n",
      " [ 0.55221188  1.        ]]\n",
      "Minibatch total | class | pred loss at step 524: 15132.136719 | 0.748005 | 15131.388672 Learning rate: 0.000588201\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.60578811  1.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 526: 15263.375977 | 0.727319 | 15262.648438 Learning rate: 0.000588201\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.54495329  1.        ]\n",
      " [ 0.51285076  1.        ]]\n",
      "Minibatch total | class | pred loss at step 528: 14550.908203 | 0.746554 | 14550.162109 Learning rate: 0.000588201\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.77994007  1.        ]]\n",
      "Minibatch total | class | pred loss at step 530: 14482.548828 | 0.777957 | 14481.770508 Learning rate: 0.000588201\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5        1.       ]\n",
      " [ 0.7223413  0.       ]]\n",
      "Minibatch total | class | pred loss at step 532: 13213.037109 | 0.570994 | 13212.465820 Learning rate: 0.000588201\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5         1.        ]\n",
      " [ 0.77490938  1.        ]]\n",
      "Minibatch total | class | pred loss at step 534: 14397.192383 | 0.791730 | 14396.400391 Learning rate: 0.000588201\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.93280989  0.        ]\n",
      " [ 0.82318842  1.        ]]\n",
      "Minibatch total | class | pred loss at step 536: 13131.054688 | 0.804689 | 13130.250000 Learning rate: 0.000588201\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.93457866  0.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 538: 13911.364258 | 0.923043 | 13910.441406 Learning rate: 0.000588201\n",
      "Minibatch accuracy: 43.75\n",
      "Predictions | Labels:\n",
      " [[ 0.75957525  0.        ]\n",
      " [ 0.5         1.        ]]\n",
      "Minibatch total | class | pred loss at step 540: 14509.056641 | 0.691376 | 14508.365234 Learning rate: 0.000588201\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Validation accuracy: 50.0\n",
      "Minibatch total | class | pred loss at step 542: 12937.291992 | 0.578283 | 12936.713867 Learning rate: 0.000588201\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.60812265  1.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 544: 13363.004883 | 0.717314 | 13362.287109 Learning rate: 0.000588201\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.51746464  1.        ]\n",
      " [ 0.65402681  0.        ]]\n",
      "Minibatch total | class | pred loss at step 546: 13088.551758 | 0.608417 | 13087.943359 Learning rate: 0.000588201\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5  1. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 548: 14133.327148 | 0.645402 | 14132.681641 Learning rate: 0.000588201\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.55277789  1.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 550: 14757.380859 | 0.733594 | 14756.647461 Learning rate: 0.000588201\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5         1.        ]\n",
      " [ 0.76813871  1.        ]]\n",
      "Minibatch total | class | pred loss at step 552: 14558.447266 | 0.691656 | 14557.755859 Learning rate: 0.000588201\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 554: 12780.511719 | 0.727959 | 12779.784180 Learning rate: 0.000588201\n",
      "Minibatch accuracy: 37.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5  1. ]\n",
      " [ 0.5  1. ]]\n",
      "Minibatch total | class | pred loss at step 556: 13285.243164 | 0.743208 | 13284.500000 Learning rate: 0.000588201\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  1. ]]\n",
      "Minibatch total | class | pred loss at step 558: 13951.224609 | 0.730936 | 13950.494141 Learning rate: 0.000588201\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 560: 13306.287109 | 0.712303 | 13305.575195 Learning rate: 0.000564673\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5         1.        ]\n",
      " [ 0.51311034  0.        ]]\n",
      "Validation accuracy: 48.6666666667\n",
      "Minibatch total | class | pred loss at step 562: 13593.638672 | 0.676736 | 13592.961914 Learning rate: 0.000564673\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.66371787  0.        ]\n",
      " [ 0.93288827  1.        ]]\n",
      "Minibatch total | class | pred loss at step 564: 14112.291016 | 0.759755 | 14111.531250 Learning rate: 0.000564673\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.58108693  0.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 566: 15141.936523 | 0.560083 | 15141.375977 Learning rate: 0.000564673\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  1. ]]\n",
      "Minibatch total | class | pred loss at step 568: 13841.652344 | 0.810656 | 13840.841797 Learning rate: 0.000564673\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5        0.       ]\n",
      " [ 0.8826741  0.       ]]\n",
      "Minibatch total | class | pred loss at step 570: 16156.632812 | 0.942281 | 16155.690430 Learning rate: 0.000564673\n",
      "Minibatch accuracy: 31.25\n",
      "Predictions | Labels:\n",
      " [[ 0.61579806  0.        ]\n",
      " [ 0.75553292  0.        ]]\n",
      "Minibatch total | class | pred loss at step 572: 14050.514648 | 0.681230 | 14049.833008 Learning rate: 0.000564673\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.84707665  1.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 574: 15426.894531 | 0.801182 | 15426.093750 Learning rate: 0.000564673\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.55674785  1.        ]\n",
      " [ 0.60285455  1.        ]]\n",
      "Minibatch total | class | pred loss at step 576: 15066.497070 | 0.889298 | 15065.607422 Learning rate: 0.000564673\n",
      "Minibatch accuracy: 43.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  1. ]]\n",
      "Minibatch total | class | pred loss at step 578: 13557.117188 | 0.705220 | 13556.412109 Learning rate: 0.000564673\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.73466289  0.        ]]\n",
      "Minibatch total | class | pred loss at step 580: 13076.077148 | 0.800183 | 13075.277344 Learning rate: 0.000564673\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.79543662  0.        ]]\n",
      "Validation accuracy: 50.0\n",
      "Minibatch total | class | pred loss at step 582: 13876.916016 | 0.637977 | 13876.278320 Learning rate: 0.000564673\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.51385176  1.        ]]\n",
      "Minibatch total | class | pred loss at step 584: 13918.588867 | 0.880330 | 13917.708984 Learning rate: 0.000564673\n",
      "Minibatch accuracy: 43.75\n",
      "Predictions | Labels:\n",
      " [[ 0.57602757  0.        ]\n",
      " [ 0.5         1.        ]]\n",
      "Minibatch total | class | pred loss at step 586: 14396.864258 | 0.755235 | 14396.109375 Learning rate: 0.000564673\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.57267141  0.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 588: 13711.728516 | 0.784722 | 13710.943359 Learning rate: 0.000564673\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.67130828  0.        ]\n",
      " [ 0.8953706   1.        ]]\n",
      "Minibatch total | class | pred loss at step 590: 12750.956055 | 0.898870 | 12750.057617 Learning rate: 0.000564673\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 592: 13157.593750 | 0.787002 | 13156.806641 Learning rate: 0.000564673\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.91307628  0.        ]]\n",
      "Minibatch total | class | pred loss at step 594: 14773.344727 | 0.632017 | 14772.712891 Learning rate: 0.000564673\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  1. ]]\n",
      "Minibatch total | class | pred loss at step 596: 14320.120117 | 0.659529 | 14319.460938 Learning rate: 0.000564673\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  1. ]]\n",
      "Minibatch total | class | pred loss at step 598: 13819.233398 | 0.783259 | 13818.450195 Learning rate: 0.000564673\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.55402625  0.        ]]\n",
      "Minibatch total | class | pred loss at step 600: 14011.870117 | 0.604761 | 14011.265625 Learning rate: 0.000542086\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.78414297  1.        ]]\n",
      "Validation accuracy: 49.3333333333\n",
      "Minibatch total | class | pred loss at step 602: 15049.489258 | 0.638376 | 15048.850586 Learning rate: 0.000542086\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 604: 14522.573242 | 0.654833 | 14521.917969 Learning rate: 0.000542086\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5         1.        ]\n",
      " [ 0.71939296  1.        ]]\n",
      "Minibatch total | class | pred loss at step 606: 13015.601562 | 0.599003 | 13015.002930 Learning rate: 0.000542086\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5         1.        ]\n",
      " [ 0.68535477  1.        ]]\n",
      "Minibatch total | class | pred loss at step 608: 14103.131836 | 0.622579 | 14102.508789 Learning rate: 0.000542086\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  1. ]]\n",
      "Minibatch total | class | pred loss at step 610: 15799.922852 | 0.787032 | 15799.135742 Learning rate: 0.000542086\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.68624139  0.        ]\n",
      " [ 0.88238972  0.        ]]\n",
      "Minibatch total | class | pred loss at step 612: 12561.135742 | 0.682932 | 12560.453125 Learning rate: 0.000542086\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.86413878  0.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 614: 13051.065430 | 0.688564 | 13050.376953 Learning rate: 0.000542086\n",
      "Minibatch accuracy: 43.75\n",
      "Predictions | Labels:\n",
      " [[ 0.83659607  1.        ]\n",
      " [ 0.5         1.        ]]\n",
      "Minibatch total | class | pred loss at step 616: 12459.033203 | 0.548876 | 12458.484375 Learning rate: 0.000542086\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5        0.       ]\n",
      " [ 0.8574726  1.       ]]\n",
      "Minibatch total | class | pred loss at step 618: 12789.530273 | 0.811538 | 12788.718750 Learning rate: 0.000542086\n",
      "Minibatch accuracy: 43.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5         1.        ]\n",
      " [ 0.58971661  1.        ]]\n",
      "Minibatch total | class | pred loss at step 620: 16162.177734 | 0.682689 | 16161.495117 Learning rate: 0.000542086\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.72322941  1.        ]\n",
      " [ 0.87563193  1.        ]]\n",
      "Validation accuracy: 54.0\n",
      "Minibatch total | class | pred loss at step 622: 15521.300781 | 0.687402 | 15520.613281 Learning rate: 0.000542086\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.52314156  0.        ]]\n",
      "Minibatch total | class | pred loss at step 624: 15655.441406 | 0.625220 | 15654.816406 Learning rate: 0.000542086\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.87788057  1.        ]\n",
      " [ 0.85807937  1.        ]]\n",
      "Minibatch total | class | pred loss at step 626: 16457.916016 | 0.567547 | 16457.347656 Learning rate: 0.000542086\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.66529548  1.        ]\n",
      " [ 0.5         1.        ]]\n",
      "Minibatch total | class | pred loss at step 628: 13241.943359 | 0.567239 | 13241.375977 Learning rate: 0.000542086\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5  1. ]\n",
      " [ 0.5  1. ]]\n",
      "Minibatch total | class | pred loss at step 630: 13677.814453 | 0.566289 | 13677.248047 Learning rate: 0.000542086\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.52689695  1.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 632: 13653.875977 | 0.696211 | 13653.179688 Learning rate: 0.000542086\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.60732126  1.        ]]\n",
      "Minibatch total | class | pred loss at step 634: 14696.016602 | 0.759091 | 14695.257812 Learning rate: 0.000542086\n",
      "Minibatch accuracy: 31.25\n",
      "Predictions | Labels:\n",
      " [[ 0.52320403  0.        ]\n",
      " [ 0.791816    1.        ]]\n",
      "Minibatch total | class | pred loss at step 636: 15616.703125 | 0.615701 | 15616.087891 Learning rate: 0.000542086\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.51283169  1.        ]\n",
      " [ 0.50435024  0.        ]]\n",
      "Minibatch total | class | pred loss at step 638: 15017.067383 | 0.672909 | 15016.394531 Learning rate: 0.000542086\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.84244204  1.        ]]\n",
      "Minibatch total | class | pred loss at step 640: 13668.249023 | 0.720220 | 13667.528320 Learning rate: 0.000520403\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.60797453  1.        ]\n",
      " [ 0.50770485  0.        ]]\n",
      "Validation accuracy: 52.0\n",
      "Minibatch total | class | pred loss at step 642: 13573.243164 | 0.681945 | 13572.561523 Learning rate: 0.000520403\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.75722587  1.        ]]\n",
      "Minibatch total | class | pred loss at step 644: 13132.043945 | 0.576045 | 13131.467773 Learning rate: 0.000520403\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5  1. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 646: 15498.799805 | 0.738870 | 15498.060547 Learning rate: 0.000520403\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5  1. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 648: 12801.149414 | 0.721804 | 12800.427734 Learning rate: 0.000520403\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.65518379  1.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 650: 14226.264648 | 0.649765 | 14225.615234 Learning rate: 0.000520403\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 652: 13407.200195 | 0.727448 | 13406.472656 Learning rate: 0.000520403\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.75569177  0.        ]\n",
      " [ 0.58024496  1.        ]]\n",
      "Minibatch total | class | pred loss at step 654: 13480.830078 | 0.667167 | 13480.163086 Learning rate: 0.000520403\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.59260744  1.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 656: 15590.457031 | 0.580570 | 15589.875977 Learning rate: 0.000520403\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.65727842  1.        ]]\n",
      "Minibatch total | class | pred loss at step 658: 15941.233398 | 0.717765 | 15940.515625 Learning rate: 0.000520403\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.572034  0.      ]\n",
      " [ 0.5       0.      ]]\n",
      "Minibatch total | class | pred loss at step 660: 14308.891602 | 0.666096 | 14308.225586 Learning rate: 0.000520403\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5  1. ]\n",
      " [ 0.5  0. ]]\n",
      "Validation accuracy: 51.3333333333\n",
      "Minibatch total | class | pred loss at step 662: 13094.421875 | 0.614494 | 13093.807617 Learning rate: 0.000520403\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5  1. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 664: 15490.593750 | 0.693609 | 15489.900391 Learning rate: 0.000520403\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.69157833  1.        ]\n",
      " [ 0.69516683  0.        ]]\n",
      "Minibatch total | class | pred loss at step 666: 14207.980469 | 0.798012 | 14207.182617 Learning rate: 0.000520403\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5  1. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 668: 14801.285156 | 0.631126 | 14800.654297 Learning rate: 0.000520403\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5        1.       ]\n",
      " [ 0.5312016  0.       ]]\n",
      "Minibatch total | class | pred loss at step 670: 15694.670898 | 0.693120 | 15693.977539 Learning rate: 0.000520403\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5  1. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 672: 15388.244141 | 0.555427 | 15387.688477 Learning rate: 0.000520403\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 674: 14836.906250 | 0.640786 | 14836.265625 Learning rate: 0.000520403\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.51172352  0.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 676: 14333.592773 | 0.666166 | 14332.926758 Learning rate: 0.000520403\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.58201724  1.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 678: 16098.012695 | 0.631393 | 16097.380859 Learning rate: 0.000520403\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.50020206  0.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 680: 13141.173828 | 0.590320 | 13140.583984 Learning rate: 0.000499587\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5         1.        ]\n",
      " [ 0.87778568  1.        ]]\n",
      "Validation accuracy: 48.6666666667\n",
      "Minibatch total | class | pred loss at step 682: 13867.651367 | 0.836058 | 13866.815430 Learning rate: 0.000499587\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5  1. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 684: 15777.742188 | 0.709926 | 15777.032227 Learning rate: 0.000499587\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.54929709  0.        ]\n",
      " [ 0.59347534  0.        ]]\n",
      "Minibatch total | class | pred loss at step 686: 13003.753906 | 0.669313 | 13003.084961 Learning rate: 0.000499587\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5         1.        ]\n",
      " [ 0.92940152  1.        ]]\n",
      "Minibatch total | class | pred loss at step 688: 15396.272461 | 0.745191 | 15395.527344 Learning rate: 0.000499587\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.67710263  0.        ]]\n",
      "Minibatch total | class | pred loss at step 690: 13420.381836 | 0.796218 | 13419.585938 Learning rate: 0.000499587\n",
      "Minibatch accuracy: 43.75\n",
      "Predictions | Labels:\n",
      " [[ 0.83256429  1.        ]\n",
      " [ 0.5         1.        ]]\n",
      "Minibatch total | class | pred loss at step 692: 14791.492188 | 0.652742 | 14790.839844 Learning rate: 0.000499587\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5  1. ]\n",
      " [ 0.5  1. ]]\n",
      "Minibatch total | class | pred loss at step 694: 14233.449219 | 0.643639 | 14232.805664 Learning rate: 0.000499587\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.53849369  1.        ]\n",
      " [ 0.655487    0.        ]]\n",
      "Minibatch total | class | pred loss at step 696: 14374.915039 | 0.626614 | 14374.288086 Learning rate: 0.000499587\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.54869854  0.        ]\n",
      " [ 0.65821874  1.        ]]\n",
      "Minibatch total | class | pred loss at step 698: 16188.838867 | 0.693511 | 16188.145508 Learning rate: 0.000499587\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.85549271  0.        ]]\n",
      "Minibatch total | class | pred loss at step 700: 13631.196289 | 0.622435 | 13630.574219 Learning rate: 0.000499587\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5  1. ]\n",
      " [ 0.5  0. ]]\n",
      "Validation accuracy: 50.0\n",
      "Minibatch total | class | pred loss at step 702: 16683.416016 | 0.563756 | 16682.851562 Learning rate: 0.000499587\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5  1. ]\n",
      " [ 0.5  1. ]]\n",
      "Minibatch total | class | pred loss at step 704: 13610.316406 | 0.620843 | 13609.695312 Learning rate: 0.000499587\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.64919025  1.        ]\n",
      " [ 0.5         1.        ]]\n",
      "Minibatch total | class | pred loss at step 706: 15861.925781 | 0.803559 | 15861.122070 Learning rate: 0.000499587\n",
      "Minibatch accuracy: 43.75\n",
      "Predictions | Labels:\n",
      " [[ 0.63132155  0.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 708: 15108.212891 | 0.713866 | 15107.499023 Learning rate: 0.000499587\n",
      "Minibatch accuracy: 37.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5  1. ]\n",
      " [ 0.5  1. ]]\n",
      "Minibatch total | class | pred loss at step 710: 14159.765625 | 0.769603 | 14158.996094 Learning rate: 0.000499587\n",
      "Minibatch accuracy: 43.75\n",
      "Predictions | Labels:\n",
      " [[ 0.58446395  0.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 712: 16196.767578 | 0.697266 | 16196.070312 Learning rate: 0.000499587\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.6431874  0.       ]\n",
      " [ 0.6835779  0.       ]]\n",
      "Minibatch total | class | pred loss at step 714: 16224.687500 | 0.728212 | 16223.958984 Learning rate: 0.000499587\n",
      "Minibatch accuracy: 43.75\n",
      "Predictions | Labels:\n",
      " [[ 0.54621184  1.        ]\n",
      " [ 0.81444889  0.        ]]\n",
      "Minibatch total | class | pred loss at step 716: 12614.343750 | 0.679414 | 12613.664062 Learning rate: 0.000499587\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.58014822  0.        ]\n",
      " [ 0.5         1.        ]]\n",
      "Minibatch total | class | pred loss at step 718: 13348.399414 | 0.640140 | 13347.758789 Learning rate: 0.000499587\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.53178787  0.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 720: 14725.201172 | 0.731941 | 14724.468750 Learning rate: 0.000479603\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.63706905  1.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Validation accuracy: 50.6666666667\n",
      "Minibatch total | class | pred loss at step 722: 15227.364258 | 0.884935 | 15226.479492 Learning rate: 0.000479603\n",
      "Minibatch accuracy: 25.0\n",
      "Predictions | Labels:\n",
      " [[ 0.59698832  0.        ]\n",
      " [ 0.5         1.        ]]\n",
      "Minibatch total | class | pred loss at step 724: 15424.390625 | 0.844356 | 15423.545898 Learning rate: 0.000479603\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 726: 13788.854492 | 0.584209 | 13788.270508 Learning rate: 0.000479603\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  1. ]]\n",
      "Minibatch total | class | pred loss at step 728: 17047.787109 | 0.704824 | 17047.082031 Learning rate: 0.000479603\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5         1.        ]\n",
      " [ 0.68893653  1.        ]]\n",
      "Minibatch total | class | pred loss at step 730: 15632.990234 | 0.757273 | 15632.233398 Learning rate: 0.000479603\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  1. ]]\n",
      "Minibatch total | class | pred loss at step 732: 14682.859375 | 0.752020 | 14682.107422 Learning rate: 0.000479603\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.61228168  0.        ]]\n",
      "Minibatch total | class | pred loss at step 734: 12943.916016 | 0.787457 | 12943.128906 Learning rate: 0.000479603\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 736: 12746.403320 | 0.658748 | 12745.744141 Learning rate: 0.000479603\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.53486449  0.        ]]\n",
      "Minibatch total | class | pred loss at step 738: 16271.907227 | 0.596870 | 16271.310547 Learning rate: 0.000479603\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.63128066  0.        ]\n",
      " [ 0.87495732  1.        ]]\n",
      "Minibatch total | class | pred loss at step 740: 16356.444336 | 0.695188 | 16355.749023 Learning rate: 0.000479603\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Validation accuracy: 48.6666666667\n",
      "Minibatch total | class | pred loss at step 742: 13695.940430 | 0.807469 | 13695.132812 Learning rate: 0.000479603\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.92775482  0.        ]\n",
      " [ 0.50954252  1.        ]]\n",
      "Minibatch total | class | pred loss at step 744: 12769.290039 | 0.806277 | 12768.483398 Learning rate: 0.000479603\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5         1.        ]\n",
      " [ 0.78923064  0.        ]]\n",
      "Minibatch total | class | pred loss at step 746: 12922.951172 | 0.711186 | 12922.240234 Learning rate: 0.000479603\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.78506601  1.        ]]\n",
      "Minibatch total | class | pred loss at step 748: 16662.277344 | 0.715569 | 16661.562500 Learning rate: 0.000479603\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5283016  0.       ]\n",
      " [ 0.5        1.       ]]\n",
      "Minibatch total | class | pred loss at step 750: 15067.027344 | 0.738673 | 15066.289062 Learning rate: 0.000479603\n",
      "Minibatch accuracy: 37.5\n",
      "Predictions | Labels:\n",
      " [[ 0.65938026  0.        ]\n",
      " [ 0.5         1.        ]]\n",
      "Minibatch total | class | pred loss at step 752: 13820.925781 | 0.886991 | 13820.039062 Learning rate: 0.000479603\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 754: 15914.019531 | 0.778510 | 15913.241211 Learning rate: 0.000479603\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.58738935  1.        ]]\n",
      "Minibatch total | class | pred loss at step 756: 16998.941406 | 0.644772 | 16998.296875 Learning rate: 0.000479603\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.64868021  0.        ]\n",
      " [ 0.5         1.        ]]\n",
      "Minibatch total | class | pred loss at step 758: 13085.340820 | 0.602609 | 13084.738281 Learning rate: 0.000479603\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.50610638  1.        ]\n",
      " [ 0.5         1.        ]]\n",
      "Minibatch total | class | pred loss at step 760: 12805.955078 | 0.759086 | 12805.196289 Learning rate: 0.000460419\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5        0.       ]\n",
      " [ 0.8148194  1.       ]]\n",
      "Validation accuracy: 49.3333333333\n",
      "Minibatch total | class | pred loss at step 762: 13113.958984 | 0.682241 | 13113.276367 Learning rate: 0.000460419\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.51858515  1.        ]\n",
      " [ 0.5         1.        ]]\n",
      "Minibatch total | class | pred loss at step 764: 13962.553711 | 0.672821 | 13961.880859 Learning rate: 0.000460419\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 766: 12973.736328 | 0.649966 | 12973.085938 Learning rate: 0.000460419\n",
      "Minibatch accuracy: 37.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5580011   0.        ]\n",
      " [ 0.56881768  0.        ]]\n",
      "Minibatch total | class | pred loss at step 768: 18133.417969 | 0.744206 | 18132.673828 Learning rate: 0.000460419\n",
      "Minibatch accuracy: 43.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5  1. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 770: 17588.193359 | 0.692082 | 17587.501953 Learning rate: 0.000460419\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.62248141  1.        ]]\n",
      "Minibatch total | class | pred loss at step 772: 12975.099609 | 0.571463 | 12974.528320 Learning rate: 0.000460419\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.51789194  1.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 774: 16944.373047 | 0.752682 | 16943.621094 Learning rate: 0.000460419\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  1. ]]\n",
      "Minibatch total | class | pred loss at step 776: 16587.841797 | 0.766231 | 16587.076172 Learning rate: 0.000460419\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.57012951  1.        ]\n",
      " [ 0.5         1.        ]]\n",
      "Minibatch total | class | pred loss at step 778: 16967.785156 | 0.689398 | 16967.095703 Learning rate: 0.000460419\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.73866963  1.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 780: 16734.964844 | 0.733756 | 16734.230469 Learning rate: 0.000460419\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.67663038  0.        ]\n",
      " [ 0.5         1.        ]]\n",
      "Validation accuracy: 50.6666666667\n",
      "Minibatch total | class | pred loss at step 782: 12905.501953 | 0.785399 | 12904.716797 Learning rate: 0.000460419\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.6600588  1.       ]\n",
      " [ 0.5        0.       ]]\n",
      "Minibatch total | class | pred loss at step 784: 14883.147461 | 0.764475 | 14882.382812 Learning rate: 0.000460419\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.89148855  1.        ]\n",
      " [ 0.57077384  0.        ]]\n",
      "Minibatch total | class | pred loss at step 786: 13750.882812 | 0.656738 | 13750.226562 Learning rate: 0.000460419\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 788: 14297.331055 | 0.898536 | 14296.432617 Learning rate: 0.000460419\n",
      "Minibatch accuracy: 37.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5  1. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 790: 14764.940430 | 0.692582 | 14764.248047 Learning rate: 0.000460419\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.54190707  0.        ]]\n",
      "Minibatch total | class | pred loss at step 792: 13358.795898 | 0.562768 | 13358.233398 Learning rate: 0.000460419\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.64287418  1.        ]]\n",
      "Minibatch total | class | pred loss at step 794: 13050.400391 | 0.687288 | 13049.712891 Learning rate: 0.000460419\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.6273284  0.       ]\n",
      " [ 0.5        1.       ]]\n",
      "Minibatch total | class | pred loss at step 796: 13006.552734 | 0.583720 | 13005.968750 Learning rate: 0.000460419\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.79298711  1.        ]]\n",
      "Minibatch total | class | pred loss at step 798: 13967.120117 | 0.638995 | 13966.481445 Learning rate: 0.000460419\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.68192822  1.        ]]\n",
      "Minibatch total | class | pred loss at step 800: 14903.256836 | 0.725972 | 14902.531250 Learning rate: 0.000442002\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.74431747  1.        ]\n",
      " [ 0.69365883  1.        ]]\n",
      "Validation accuracy: 48.0\n",
      "Minibatch total | class | pred loss at step 802: 14972.203125 | 0.687607 | 14971.515625 Learning rate: 0.000442002\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  1. ]]\n",
      "Minibatch total | class | pred loss at step 804: 13306.034180 | 0.696556 | 13305.337891 Learning rate: 0.000442002\n",
      "Minibatch accuracy: 43.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5         1.        ]\n",
      " [ 0.81653875  1.        ]]\n",
      "Minibatch total | class | pred loss at step 806: 13110.407227 | 0.718184 | 13109.689453 Learning rate: 0.000442002\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5         1.        ]\n",
      " [ 0.50529778  0.        ]]\n",
      "Minibatch total | class | pred loss at step 808: 14471.226562 | 0.690961 | 14470.535156 Learning rate: 0.000442002\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 810: 13985.662109 | 0.656800 | 13985.004883 Learning rate: 0.000442002\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 812: 14584.246094 | 0.635999 | 14583.610352 Learning rate: 0.000442002\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.93645173  1.        ]\n",
      " [ 0.63842905  1.        ]]\n",
      "Minibatch total | class | pred loss at step 814: 15474.607422 | 0.678391 | 15473.928711 Learning rate: 0.000442002\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.63976908  0.        ]]\n",
      "Minibatch total | class | pred loss at step 816: 13800.690430 | 0.556575 | 13800.133789 Learning rate: 0.000442002\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5  1. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 818: 14910.078125 | 0.727033 | 14909.351562 Learning rate: 0.000442002\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.81838393  0.        ]\n",
      " [ 0.62050283  0.        ]]\n",
      "Minibatch total | class | pred loss at step 820: 15972.583008 | 0.908750 | 15971.673828 Learning rate: 0.000442002\n",
      "Minibatch accuracy: 25.0\n",
      "Predictions | Labels:\n",
      " [[ 0.75761563  0.        ]\n",
      " [ 0.5         1.        ]]\n",
      "Validation accuracy: 51.3333333333\n",
      "Minibatch total | class | pred loss at step 822: 13994.244141 | 0.711226 | 13993.533203 Learning rate: 0.000442002\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.51168513  0.        ]\n",
      " [ 0.6215505   0.        ]]\n",
      "Minibatch total | class | pred loss at step 824: 15750.225586 | 0.725623 | 15749.500000 Learning rate: 0.000442002\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.58982033  1.        ]\n",
      " [ 0.518094    0.        ]]\n",
      "Minibatch total | class | pred loss at step 826: 14957.867188 | 0.902282 | 14956.964844 Learning rate: 0.000442002\n",
      "Minibatch accuracy: 43.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5         1.        ]\n",
      " [ 0.85673738  0.        ]]\n",
      "Minibatch total | class | pred loss at step 828: 13614.959961 | 0.729257 | 13614.230469 Learning rate: 0.000442002\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.59539795  0.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 830: 13848.184570 | 0.770941 | 13847.414062 Learning rate: 0.000442002\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.77484149  0.        ]\n",
      " [ 0.5         1.        ]]\n",
      "Minibatch total | class | pred loss at step 832: 14227.525391 | 0.629376 | 14226.896484 Learning rate: 0.000442002\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.52014703  1.        ]\n",
      " [ 0.55732018  1.        ]]\n",
      "Minibatch total | class | pred loss at step 834: 14259.027344 | 0.854852 | 14258.172852 Learning rate: 0.000442002\n",
      "Minibatch accuracy: 37.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5         1.        ]\n",
      " [ 0.88577592  0.        ]]\n",
      "Minibatch total | class | pred loss at step 836: 15985.227539 | 0.705385 | 15984.522461 Learning rate: 0.000442002\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.61944956  1.        ]]\n",
      "Minibatch total | class | pred loss at step 838: 13422.855469 | 0.698826 | 13422.156250 Learning rate: 0.000442002\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.85099816  1.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 840: 15128.295898 | 0.893775 | 15127.402344 Learning rate: 0.000424322\n",
      "Minibatch accuracy: 43.75\n",
      "Predictions | Labels:\n",
      " [[ 0.50745696  0.        ]\n",
      " [ 0.5         1.        ]]\n",
      "Validation accuracy: 50.6666666667\n",
      "Minibatch total | class | pred loss at step 842: 13711.092773 | 0.743923 | 13710.348633 Learning rate: 0.000424322\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.87576491  0.        ]\n",
      " [ 0.74751127  0.        ]]\n",
      "Minibatch total | class | pred loss at step 844: 15669.721680 | 0.697039 | 15669.024414 Learning rate: 0.000424322\n",
      "Minibatch accuracy: 43.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5  1. ]\n",
      " [ 0.5  1. ]]\n",
      "Minibatch total | class | pred loss at step 846: 14363.684570 | 0.602199 | 14363.082031 Learning rate: 0.000424322\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5         1.        ]\n",
      " [ 0.51764137  1.        ]]\n",
      "Minibatch total | class | pred loss at step 848: 13954.459961 | 0.780634 | 13953.679688 Learning rate: 0.000424322\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.54604656  0.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 850: 15400.495117 | 0.607772 | 15399.887695 Learning rate: 0.000424322\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.69873041  1.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 852: 16202.653320 | 0.651258 | 16202.001953 Learning rate: 0.000424322\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 854: 15854.143555 | 0.634037 | 15853.509766 Learning rate: 0.000424322\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.745278    1.        ]\n",
      " [ 0.55887163  0.        ]]\n",
      "Minibatch total | class | pred loss at step 856: 14008.660156 | 0.581660 | 14008.078125 Learning rate: 0.000424322\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.71387863  1.        ]\n",
      " [ 0.54063284  0.        ]]\n",
      "Minibatch total | class | pred loss at step 858: 14554.109375 | 0.620988 | 14553.488281 Learning rate: 0.000424322\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5  1. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 860: 18304.335938 | 0.765665 | 18303.570312 Learning rate: 0.000424322\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.87711221  0.        ]\n",
      " [ 0.5         1.        ]]\n",
      "Validation accuracy: 50.0\n",
      "Minibatch total | class | pred loss at step 862: 17651.921875 | 0.659883 | 17651.261719 Learning rate: 0.000424322\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 864: 12765.121094 | 0.744976 | 12764.375977 Learning rate: 0.000424322\n",
      "Minibatch accuracy: 43.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5  1. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 866: 12734.678711 | 0.557421 | 12734.121094 Learning rate: 0.000424322\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.72278535  1.        ]\n",
      " [ 0.57685703  1.        ]]\n",
      "Minibatch total | class | pred loss at step 868: 13157.371094 | 0.813033 | 13156.557617 Learning rate: 0.000424322\n",
      "Minibatch accuracy: 43.75\n",
      "Predictions | Labels:\n",
      " [[ 0.60661632  1.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 870: 17843.582031 | 0.688730 | 17842.892578 Learning rate: 0.000424322\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.91133249  1.        ]\n",
      " [ 0.80639523  1.        ]]\n",
      "Minibatch total | class | pred loss at step 872: 13336.292969 | 0.628924 | 13335.664062 Learning rate: 0.000424322\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.59296018  1.        ]]\n",
      "Minibatch total | class | pred loss at step 874: 18496.318359 | 0.640628 | 18495.677734 Learning rate: 0.000424322\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.89208347  1.        ]\n",
      " [ 0.53953367  0.        ]]\n",
      "Minibatch total | class | pred loss at step 876: 17753.859375 | 0.570401 | 17753.289062 Learning rate: 0.000424322\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5         1.        ]\n",
      " [ 0.50192064  1.        ]]\n",
      "Minibatch total | class | pred loss at step 878: 14121.965820 | 0.576488 | 14121.389648 Learning rate: 0.000424322\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5  1. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 880: 16452.701172 | 0.557440 | 16452.144531 Learning rate: 0.000407349\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  1. ]]\n",
      "Validation accuracy: 50.0\n",
      "Minibatch total | class | pred loss at step 882: 15082.056641 | 0.692498 | 15081.364258 Learning rate: 0.000407349\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.56452447  1.        ]\n",
      " [ 0.7803371   0.        ]]\n",
      "Minibatch total | class | pred loss at step 884: 16277.555664 | 0.741345 | 16276.814453 Learning rate: 0.000407349\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.82375729  1.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 886: 18047.089844 | 0.624716 | 18046.464844 Learning rate: 0.000407349\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 888: 17600.363281 | 0.661597 | 17599.701172 Learning rate: 0.000407349\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.80833048  1.        ]\n",
      " [ 0.72073168  1.        ]]\n",
      "Minibatch total | class | pred loss at step 890: 13919.172852 | 0.737144 | 13918.435547 Learning rate: 0.000407349\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.51566428  0.        ]\n",
      " [ 0.66430187  1.        ]]\n",
      "Minibatch total | class | pred loss at step 892: 13374.548828 | 0.667902 | 13373.880859 Learning rate: 0.000407349\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.78881282  1.        ]\n",
      " [ 0.62024146  1.        ]]\n",
      "Minibatch total | class | pred loss at step 894: 14194.431641 | 0.577146 | 14193.854492 Learning rate: 0.000407349\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.68858123  1.        ]]\n",
      "Minibatch total | class | pred loss at step 896: 16934.207031 | 0.795180 | 16933.412109 Learning rate: 0.000407349\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 898: 13015.538086 | 0.707873 | 13014.830078 Learning rate: 0.000407349\n",
      "Minibatch accuracy: 43.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.56868386  0.        ]]\n",
      "Minibatch total | class | pred loss at step 900: 15848.736328 | 0.617566 | 15848.119141 Learning rate: 0.000407349\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5        0.       ]\n",
      " [ 0.8392114  1.       ]]\n",
      "Validation accuracy: 51.3333333333\n",
      "Minibatch total | class | pred loss at step 902: 13588.529297 | 0.665994 | 13587.863281 Learning rate: 0.000407349\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.54193896  1.        ]\n",
      " [ 0.68081045  1.        ]]\n",
      "Minibatch total | class | pred loss at step 904: 14128.692383 | 0.675223 | 14128.017578 Learning rate: 0.000407349\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 906: 17364.835938 | 0.649368 | 17364.187500 Learning rate: 0.000407349\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.59693611  1.        ]\n",
      " [ 0.71618992  1.        ]]\n",
      "Minibatch total | class | pred loss at step 908: 17558.244141 | 0.708457 | 17557.535156 Learning rate: 0.000407349\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 910: 16591.220703 | 0.667926 | 16590.552734 Learning rate: 0.000407349\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.56630689  0.        ]]\n",
      "Minibatch total | class | pred loss at step 912: 14283.128906 | 0.607235 | 14282.521484 Learning rate: 0.000407349\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.84693283  1.        ]]\n",
      "Minibatch total | class | pred loss at step 914: 16494.300781 | 0.710368 | 16493.589844 Learning rate: 0.000407349\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.70433789  0.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 916: 16368.352539 | 0.741994 | 16367.610352 Learning rate: 0.000407349\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.76579869  1.        ]]\n",
      "Minibatch total | class | pred loss at step 918: 18180.759766 | 0.704804 | 18180.054688 Learning rate: 0.000407349\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.50248259  1.        ]]\n",
      "Minibatch total | class | pred loss at step 920: 18145.412109 | 0.682984 | 18144.728516 Learning rate: 0.000391055\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.74040705  0.        ]]\n",
      "Validation accuracy: 50.0\n",
      "Minibatch total | class | pred loss at step 922: 15865.121094 | 0.548461 | 15864.572266 Learning rate: 0.000391055\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  1. ]]\n",
      "Minibatch total | class | pred loss at step 924: 15892.417969 | 0.687326 | 15891.730469 Learning rate: 0.000391055\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 926: 14530.910156 | 0.671603 | 14530.238281 Learning rate: 0.000391055\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 928: 17850.845703 | 0.656231 | 17850.189453 Learning rate: 0.000391055\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 930: 13435.468750 | 0.628013 | 13434.840820 Learning rate: 0.000391055\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.86954069  1.        ]\n",
      " [ 0.5         1.        ]]\n",
      "Minibatch total | class | pred loss at step 932: 14611.137695 | 0.837534 | 14610.299805 Learning rate: 0.000391055\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 934: 17828.857422 | 0.677974 | 17828.179688 Learning rate: 0.000391055\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.59933549  0.        ]\n",
      " [ 0.57615983  0.        ]]\n",
      "Minibatch total | class | pred loss at step 936: 13244.133789 | 0.637627 | 13243.496094 Learning rate: 0.000391055\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.93555403  1.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 938: 17729.648438 | 0.708409 | 17728.939453 Learning rate: 0.000391055\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.63227266  0.        ]\n",
      " [ 0.5         1.        ]]\n",
      "Minibatch total | class | pred loss at step 940: 13381.686523 | 0.834754 | 13380.851562 Learning rate: 0.000391055\n",
      "Minibatch accuracy: 43.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5  1. ]\n",
      " [ 0.5  1. ]]\n",
      "Validation accuracy: 48.6666666667\n",
      "Minibatch total | class | pred loss at step 942: 13285.893555 | 0.626842 | 13285.266602 Learning rate: 0.000391055\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5         1.        ]\n",
      " [ 0.58518535  1.        ]]\n",
      "Minibatch total | class | pred loss at step 944: 14425.932617 | 0.656776 | 14425.275391 Learning rate: 0.000391055\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.68614471  0.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 946: 16267.710938 | 0.598273 | 16267.112305 Learning rate: 0.000391055\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.66296786  1.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 948: 17368.078125 | 0.675268 | 17367.402344 Learning rate: 0.000391055\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.81837964  0.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 950: 14155.991211 | 0.617769 | 14155.373047 Learning rate: 0.000391055\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.57780039  0.        ]]\n",
      "Minibatch total | class | pred loss at step 952: 18999.900391 | 0.582574 | 18999.318359 Learning rate: 0.000391055\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5         1.        ]\n",
      " [ 0.83129156  1.        ]]\n",
      "Minibatch total | class | pred loss at step 954: 14144.110352 | 0.603339 | 14143.506836 Learning rate: 0.000391055\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5  1. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 956: 18443.712891 | 0.796091 | 18442.916016 Learning rate: 0.000391055\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.72653431  1.        ]]\n",
      "Minibatch total | class | pred loss at step 958: 13963.359375 | 0.684538 | 13962.674805 Learning rate: 0.000391055\n",
      "Minibatch accuracy: 43.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5         1.        ]\n",
      " [ 0.59659249  0.        ]]\n",
      "Minibatch total | class | pred loss at step 960: 14630.114258 | 0.770387 | 14629.343750 Learning rate: 0.000375413\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Validation accuracy: 50.6666666667\n",
      "Minibatch total | class | pred loss at step 962: 17771.726562 | 0.631429 | 17771.095703 Learning rate: 0.000375413\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.56880093  0.        ]\n",
      " [ 0.53011209  0.        ]]\n",
      "Minibatch total | class | pred loss at step 964: 17799.537109 | 0.716071 | 17798.820312 Learning rate: 0.000375413\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.7974304  0.       ]\n",
      " [ 0.5        1.       ]]\n",
      "Minibatch total | class | pred loss at step 966: 13519.949219 | 0.656647 | 13519.292969 Learning rate: 0.000375413\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5         1.        ]\n",
      " [ 0.67974156  1.        ]]\n",
      "Minibatch total | class | pred loss at step 968: 13569.375977 | 0.643309 | 13568.732422 Learning rate: 0.000375413\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  1. ]]\n",
      "Minibatch total | class | pred loss at step 970: 14729.588867 | 0.742729 | 14728.845703 Learning rate: 0.000375413\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 972: 17306.488281 | 0.888731 | 17305.599609 Learning rate: 0.000375413\n",
      "Minibatch accuracy: 25.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5         1.        ]\n",
      " [ 0.90776044  0.        ]]\n",
      "Minibatch total | class | pred loss at step 974: 17213.277344 | 0.861923 | 17212.416016 Learning rate: 0.000375413\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 976: 14470.544922 | 0.580384 | 14469.964844 Learning rate: 0.000375413\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5  1. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 978: 18950.667969 | 0.702260 | 18949.964844 Learning rate: 0.000375413\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.67665023  1.        ]\n",
      " [ 0.87513983  1.        ]]\n",
      "Minibatch total | class | pred loss at step 980: 17098.179688 | 0.738450 | 17097.441406 Learning rate: 0.000375413\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5         1.        ]\n",
      " [ 0.62421066  1.        ]]\n",
      "Validation accuracy: 50.0\n",
      "Minibatch total | class | pred loss at step 982: 17081.816406 | 0.680239 | 17081.136719 Learning rate: 0.000375413\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.56524938  0.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 984: 13910.736328 | 0.790687 | 13909.945312 Learning rate: 0.000375413\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 986: 13685.682617 | 0.658650 | 13685.024414 Learning rate: 0.000375413\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.54750615  0.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 988: 17849.640625 | 0.562426 | 17849.078125 Learning rate: 0.000375413\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.86032486  1.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 990: 17835.261719 | 0.687824 | 17834.574219 Learning rate: 0.000375413\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.96255189  1.        ]]\n",
      "Minibatch total | class | pred loss at step 992: 13830.618164 | 0.672784 | 13829.945312 Learning rate: 0.000375413\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.59018385  1.        ]\n",
      " [ 0.50347191  0.        ]]\n",
      "Minibatch total | class | pred loss at step 994: 12871.291016 | 0.752463 | 12870.538086 Learning rate: 0.000375413\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.76995879  0.        ]\n",
      " [ 0.84309256  1.        ]]\n",
      "Minibatch total | class | pred loss at step 996: 13464.397461 | 0.690179 | 13463.707031 Learning rate: 0.000375413\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.80648345  1.        ]\n",
      " [ 0.65115607  0.        ]]\n",
      "Minibatch total | class | pred loss at step 998: 18399.318359 | 0.712495 | 18398.605469 Learning rate: 0.000375413\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5        1.       ]\n",
      " [ 0.7191909  1.       ]]\n",
      "Minibatch total | class | pred loss at step 1000: 17449.482422 | 0.720775 | 17448.761719 Learning rate: 0.000360397\n",
      "Minibatch accuracy: 31.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5         1.        ]\n",
      " [ 0.82487261  1.        ]]\n",
      "Validation accuracy: 50.0\n",
      "Minibatch total | class | pred loss at step 1002: 14579.781250 | 0.810467 | 14578.970703 Learning rate: 0.000360397\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  1. ]]\n",
      "Minibatch total | class | pred loss at step 1004: 16238.486328 | 0.762527 | 16237.723633 Learning rate: 0.000360397\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5772109  1.       ]\n",
      " [ 0.5        0.       ]]\n",
      "Minibatch total | class | pred loss at step 1006: 18024.398438 | 0.625711 | 18023.773438 Learning rate: 0.000360397\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5         1.        ]\n",
      " [ 0.77644193  1.        ]]\n",
      "Minibatch total | class | pred loss at step 1008: 13723.407227 | 0.592336 | 13722.814453 Learning rate: 0.000360397\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.52808082  1.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 1010: 13342.534180 | 0.736929 | 13341.796875 Learning rate: 0.000360397\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.85810745  1.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 1012: 14004.191406 | 0.685851 | 14003.505859 Learning rate: 0.000360397\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5  1. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 1014: 13930.539062 | 0.682751 | 13929.856445 Learning rate: 0.000360397\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 1016: 13183.799805 | 0.635972 | 13183.164062 Learning rate: 0.000360397\n",
      "Minibatch accuracy: 37.5\n",
      "Predictions | Labels:\n",
      " [[ 0.58202142  0.        ]\n",
      " [ 0.5         1.        ]]\n",
      "Minibatch total | class | pred loss at step 1018: 19395.250000 | 0.723026 | 19394.527344 Learning rate: 0.000360397\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.81205654  0.        ]]\n",
      "Minibatch total | class | pred loss at step 1020: 19551.708984 | 0.678045 | 19551.031250 Learning rate: 0.000360397\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.59266049  1.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Validation accuracy: 50.6666666667\n",
      "Minibatch total | class | pred loss at step 1022: 13175.305664 | 0.578964 | 13174.726562 Learning rate: 0.000360397\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.68824142  1.        ]]\n",
      "Minibatch total | class | pred loss at step 1024: 18848.474609 | 0.736774 | 18847.738281 Learning rate: 0.000360397\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5  1. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 1026: 17520.992188 | 0.718306 | 17520.273438 Learning rate: 0.000360397\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5         1.        ]\n",
      " [ 0.70694965  1.        ]]\n",
      "Minibatch total | class | pred loss at step 1028: 18137.847656 | 0.737391 | 18137.109375 Learning rate: 0.000360397\n",
      "Minibatch accuracy: 43.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  1. ]]\n",
      "Minibatch total | class | pred loss at step 1030: 18194.968750 | 0.682920 | 18194.285156 Learning rate: 0.000360397\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5  1. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 1032: 12954.207031 | 0.771902 | 12953.435547 Learning rate: 0.000360397\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  1. ]]\n",
      "Minibatch total | class | pred loss at step 1034: 15611.105469 | 0.840913 | 15610.264648 Learning rate: 0.000360397\n",
      "Minibatch accuracy: 43.75\n",
      "Predictions | Labels:\n",
      " [[ 0.61614537  0.        ]\n",
      " [ 0.5         1.        ]]\n",
      "Minibatch total | class | pred loss at step 1036: 14836.616211 | 0.663293 | 14835.953125 Learning rate: 0.000360397\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.66057855  1.        ]]\n",
      "Minibatch total | class | pred loss at step 1038: 14429.544922 | 0.888312 | 14428.656250 Learning rate: 0.000360397\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5        0.       ]\n",
      " [ 0.5922488  1.       ]]\n",
      "Minibatch total | class | pred loss at step 1040: 17189.560547 | 0.659746 | 17188.900391 Learning rate: 0.000345981\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.57314086  0.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Validation accuracy: 50.6666666667\n",
      "Minibatch total | class | pred loss at step 1042: 13786.949219 | 0.622481 | 13786.327148 Learning rate: 0.000345981\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.61461008  1.        ]\n",
      " [ 0.81011349  1.        ]]\n",
      "Minibatch total | class | pred loss at step 1044: 17458.191406 | 0.655483 | 17457.535156 Learning rate: 0.000345981\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5  1. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 1046: 16875.527344 | 0.553963 | 16874.972656 Learning rate: 0.000345981\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.7056334  1.       ]\n",
      " [ 0.5        1.       ]]\n",
      "Minibatch total | class | pred loss at step 1048: 18085.296875 | 0.679644 | 18084.617188 Learning rate: 0.000345981\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.70735484  1.        ]\n",
      " [ 0.56935483  0.        ]]\n",
      "Minibatch total | class | pred loss at step 1050: 18869.234375 | 0.742729 | 18868.492188 Learning rate: 0.000345981\n",
      "Minibatch accuracy: 43.75\n",
      "Predictions | Labels:\n",
      " [[ 0.71326363  1.        ]\n",
      " [ 0.5         1.        ]]\n",
      "Minibatch total | class | pred loss at step 1052: 18236.867188 | 0.692213 | 18236.175781 Learning rate: 0.000345981\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5         1.        ]\n",
      " [ 0.79915875  1.        ]]\n",
      "Minibatch total | class | pred loss at step 1054: 14134.873047 | 0.683320 | 14134.189453 Learning rate: 0.000345981\n",
      "Minibatch accuracy: 43.75\n",
      "Predictions | Labels:\n",
      " [[ 0.83113468  1.        ]\n",
      " [ 0.54603124  0.        ]]\n",
      "Minibatch total | class | pred loss at step 1056: 13465.517578 | 0.687224 | 13464.830078 Learning rate: 0.000345981\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.82406878  0.        ]]\n",
      "Minibatch total | class | pred loss at step 1058: 15228.617188 | 0.664853 | 15227.952148 Learning rate: 0.000345981\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  1. ]]\n",
      "Minibatch total | class | pred loss at step 1060: 17623.125000 | 0.627745 | 17622.498047 Learning rate: 0.000345981\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.79907328  1.        ]]\n",
      "Validation accuracy: 50.6666666667\n",
      "Minibatch total | class | pred loss at step 1062: 18803.041016 | 0.697526 | 18802.343750 Learning rate: 0.000345981\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.71227527  1.        ]\n",
      " [ 0.61420178  1.        ]]\n",
      "Minibatch total | class | pred loss at step 1064: 17943.605469 | 0.659972 | 17942.945312 Learning rate: 0.000345981\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.62796104  0.        ]\n",
      " [ 0.91393101  1.        ]]\n",
      "Minibatch total | class | pred loss at step 1066: 14108.448242 | 0.559392 | 14107.888672 Learning rate: 0.000345981\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.69057351  1.        ]]\n",
      "Minibatch total | class | pred loss at step 1068: 17382.705078 | 0.666331 | 17382.039062 Learning rate: 0.000345981\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.59077251  0.        ]\n",
      " [ 0.5         1.        ]]\n",
      "Minibatch total | class | pred loss at step 1070: 16039.621094 | 0.867523 | 16038.753906 Learning rate: 0.000345981\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5         1.        ]\n",
      " [ 0.71414179  0.        ]]\n",
      "Minibatch total | class | pred loss at step 1072: 14949.035156 | 0.754011 | 14948.281250 Learning rate: 0.000345981\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.61924881  0.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 1074: 19469.103516 | 0.657513 | 19468.445312 Learning rate: 0.000345981\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  1. ]]\n",
      "Minibatch total | class | pred loss at step 1076: 18789.927734 | 0.778993 | 18789.148438 Learning rate: 0.000345981\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.82389939  0.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 1078: 14059.527344 | 0.709949 | 14058.817383 Learning rate: 0.000345981\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.50552398  0.        ]]\n",
      "Minibatch total | class | pred loss at step 1080: 14376.791016 | 0.720782 | 14376.070312 Learning rate: 0.000332141\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5         1.        ]\n",
      " [ 0.59605128  0.        ]]\n",
      "Validation accuracy: 51.3333333333\n",
      "Minibatch total | class | pred loss at step 1082: 14150.548828 | 0.663947 | 14149.884766 Learning rate: 0.000332141\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5         1.        ]\n",
      " [ 0.58151692  0.        ]]\n",
      "Minibatch total | class | pred loss at step 1084: 18992.679688 | 0.829098 | 18991.851562 Learning rate: 0.000332141\n",
      "Minibatch accuracy: 43.75\n",
      "Predictions | Labels:\n",
      " [[ 0.87217402  0.        ]\n",
      " [ 0.80659729  1.        ]]\n",
      "Minibatch total | class | pred loss at step 1086: 18516.957031 | 0.703409 | 18516.253906 Learning rate: 0.000332141\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.64146298  1.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 1088: 14310.231445 | 0.754377 | 14309.477539 Learning rate: 0.000332141\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5        0.       ]\n",
      " [ 0.5743593  0.       ]]\n",
      "Minibatch total | class | pred loss at step 1090: 15038.424805 | 0.842293 | 15037.582031 Learning rate: 0.000332141\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5         1.        ]\n",
      " [ 0.83094388  0.        ]]\n",
      "Minibatch total | class | pred loss at step 1092: 14535.006836 | 0.676345 | 14534.330078 Learning rate: 0.000332141\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.79977894  0.        ]\n",
      " [ 0.64623958  1.        ]]\n",
      "Minibatch total | class | pred loss at step 1094: 18585.783203 | 0.661561 | 18585.121094 Learning rate: 0.000332141\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5  1. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 1096: 14569.425781 | 0.603610 | 14568.822266 Learning rate: 0.000332141\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5  1. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 1098: 14605.437500 | 0.756445 | 14604.680664 Learning rate: 0.000332141\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 1100: 15305.795898 | 0.632011 | 15305.164062 Learning rate: 0.000332141\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Validation accuracy: 50.6666666667\n",
      "Minibatch total | class | pred loss at step 1102: 18520.966797 | 0.685975 | 18520.281250 Learning rate: 0.000332141\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.75948352  0.        ]]\n",
      "Minibatch total | class | pred loss at step 1104: 17735.134766 | 0.677205 | 17734.457031 Learning rate: 0.000332141\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.65354639  0.        ]\n",
      " [ 0.5         1.        ]]\n",
      "Minibatch total | class | pred loss at step 1106: 14513.207031 | 0.601297 | 14512.605469 Learning rate: 0.000332141\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5        0.       ]\n",
      " [ 0.5376581  0.       ]]\n",
      "Minibatch total | class | pred loss at step 1108: 14637.418945 | 0.581842 | 14636.836914 Learning rate: 0.000332141\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.82080072  1.        ]]\n",
      "Minibatch total | class | pred loss at step 1110: 20572.378906 | 0.647727 | 20571.730469 Learning rate: 0.000332141\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5  1. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 1112: 19355.572266 | 0.651203 | 19354.921875 Learning rate: 0.000332141\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.80848801  1.        ]]\n",
      "Minibatch total | class | pred loss at step 1114: 13038.200195 | 0.790787 | 13037.409180 Learning rate: 0.000332141\n",
      "Minibatch accuracy: 37.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  1. ]]\n",
      "Minibatch total | class | pred loss at step 1116: 17711.273438 | 0.585393 | 17710.687500 Learning rate: 0.000332141\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.56902015  1.        ]\n",
      " [ 0.5         1.        ]]\n",
      "Minibatch total | class | pred loss at step 1118: 13590.301758 | 0.822823 | 13589.478516 Learning rate: 0.000332141\n",
      "Minibatch accuracy: 37.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.56723291  0.        ]]\n",
      "Minibatch total | class | pred loss at step 1120: 19446.298828 | 0.669987 | 19445.628906 Learning rate: 0.000318856\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.92149919  1.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Validation accuracy: 49.3333333333\n",
      "Minibatch total | class | pred loss at step 1122: 13822.666992 | 0.622043 | 13822.044922 Learning rate: 0.000318856\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.61751127  1.        ]\n",
      " [ 0.5504753   1.        ]]\n",
      "Minibatch total | class | pred loss at step 1124: 20094.121094 | 0.740707 | 20093.380859 Learning rate: 0.000318856\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.55074823  0.        ]\n",
      " [ 0.88120228  1.        ]]\n",
      "Minibatch total | class | pred loss at step 1126: 19936.044922 | 0.568111 | 19935.476562 Learning rate: 0.000318856\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.51278102  1.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 1128: 13998.911133 | 0.571735 | 13998.339844 Learning rate: 0.000318856\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  1. ]]\n",
      "Minibatch total | class | pred loss at step 1130: 17449.677734 | 0.562847 | 17449.115234 Learning rate: 0.000318856\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5  1. ]\n",
      " [ 0.5  1. ]]\n",
      "Minibatch total | class | pred loss at step 1132: 16090.813477 | 0.676773 | 16090.136719 Learning rate: 0.000318856\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.70787925  0.        ]\n",
      " [ 0.67245871  0.        ]]\n",
      "Minibatch total | class | pred loss at step 1134: 16423.953125 | 0.771667 | 16423.181641 Learning rate: 0.000318856\n",
      "Minibatch accuracy: 43.75\n",
      "Predictions | Labels:\n",
      " [[ 0.55769718  0.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 1136: 19773.228516 | 0.604459 | 19772.625000 Learning rate: 0.000318856\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  1. ]]\n",
      "Minibatch total | class | pred loss at step 1138: 19569.722656 | 0.694809 | 19569.027344 Learning rate: 0.000318856\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.75372881  1.        ]\n",
      " [ 0.63357574  0.        ]]\n",
      "Minibatch total | class | pred loss at step 1140: 14596.097656 | 0.709190 | 14595.388672 Learning rate: 0.000318856\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.69641334  1.        ]\n",
      " [ 0.50870812  0.        ]]\n",
      "Validation accuracy: 51.3333333333\n",
      "Minibatch total | class | pred loss at step 1142: 16168.375977 | 0.755499 | 16167.620117 Learning rate: 0.000318856\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.62000781  1.        ]\n",
      " [ 0.68478912  1.        ]]\n",
      "Minibatch total | class | pred loss at step 1144: 13935.840820 | 0.604553 | 13935.236328 Learning rate: 0.000318856\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.69773066  1.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 1146: 18512.583984 | 0.804753 | 18511.779297 Learning rate: 0.000318856\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  1. ]]\n",
      "Minibatch total | class | pred loss at step 1148: 13529.848633 | 0.656754 | 13529.191406 Learning rate: 0.000318856\n",
      "Minibatch accuracy: 37.5\n",
      "Predictions | Labels:\n",
      " [[ 0.51510924  0.        ]\n",
      " [ 0.5052982   0.        ]]\n",
      "Minibatch total | class | pred loss at step 1150: 17299.330078 | 0.625213 | 17298.705078 Learning rate: 0.000318856\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.86526889  1.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 1152: 14016.892578 | 0.646030 | 14016.246094 Learning rate: 0.000318856\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.77174193  1.        ]\n",
      " [ 0.89228761  1.        ]]\n",
      "Minibatch total | class | pred loss at step 1154: 15063.461914 | 0.681863 | 15062.780273 Learning rate: 0.000318856\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.62478936  0.        ]]\n",
      "Minibatch total | class | pred loss at step 1156: 19727.140625 | 0.639324 | 19726.501953 Learning rate: 0.000318856\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.78682601  1.        ]\n",
      " [ 0.60609621  1.        ]]\n",
      "Minibatch total | class | pred loss at step 1158: 19260.908203 | 0.694984 | 19260.212891 Learning rate: 0.000318856\n",
      "Minibatch accuracy: 43.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.78501022  1.        ]]\n",
      "Minibatch total | class | pred loss at step 1160: 18462.251953 | 0.635002 | 18461.617188 Learning rate: 0.000306102\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5078041   0.        ]\n",
      " [ 0.58336443  0.        ]]\n",
      "Validation accuracy: 50.0\n",
      "Minibatch total | class | pred loss at step 1162: 14765.856445 | 0.616453 | 14765.240234 Learning rate: 0.000306102\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.88670182  1.        ]\n",
      " [ 0.61979204  0.        ]]\n",
      "Minibatch total | class | pred loss at step 1164: 17494.406250 | 0.653099 | 17493.753906 Learning rate: 0.000306102\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.68570918  1.        ]]\n",
      "Minibatch total | class | pred loss at step 1166: 18769.013672 | 0.727680 | 18768.285156 Learning rate: 0.000306102\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.76334304  1.        ]\n",
      " [ 0.85764116  0.        ]]\n",
      "Minibatch total | class | pred loss at step 1168: 20200.671875 | 0.682675 | 20199.988281 Learning rate: 0.000306102\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5  1. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 1170: 19460.949219 | 0.629676 | 19460.320312 Learning rate: 0.000306102\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.67612606  0.        ]\n",
      " [ 0.5         1.        ]]\n",
      "Minibatch total | class | pred loss at step 1172: 17856.830078 | 0.560502 | 17856.269531 Learning rate: 0.000306102\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5         1.        ]\n",
      " [ 0.71438897  1.        ]]\n",
      "Minibatch total | class | pred loss at step 1174: 19150.343750 | 0.617054 | 19149.726562 Learning rate: 0.000306102\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 1176: 14729.732422 | 0.666179 | 14729.066406 Learning rate: 0.000306102\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.50231272  0.        ]]\n",
      "Minibatch total | class | pred loss at step 1178: 19708.404297 | 0.648546 | 19707.755859 Learning rate: 0.000306102\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 1180: 14080.624023 | 0.644115 | 14079.979492 Learning rate: 0.000306102\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5  1. ]\n",
      " [ 0.5  0. ]]\n",
      "Validation accuracy: 50.0\n",
      "Minibatch total | class | pred loss at step 1182: 14430.604492 | 0.839618 | 14429.764648 Learning rate: 0.000306102\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.87577736  0.        ]]\n",
      "Minibatch total | class | pred loss at step 1184: 19077.410156 | 0.648517 | 19076.761719 Learning rate: 0.000306102\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.57315952  0.        ]\n",
      " [ 0.5         1.        ]]\n",
      "Minibatch total | class | pred loss at step 1186: 13441.127930 | 0.698751 | 13440.428711 Learning rate: 0.000306102\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  1. ]]\n",
      "Minibatch total | class | pred loss at step 1188: 19734.412109 | 0.655250 | 19733.757812 Learning rate: 0.000306102\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5  1. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 1190: 13872.821289 | 0.819651 | 13872.001953 Learning rate: 0.000306102\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5  1. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 1192: 13862.939453 | 0.613847 | 13862.325195 Learning rate: 0.000306102\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.59238982  1.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 1194: 14382.029297 | 0.689715 | 14381.339844 Learning rate: 0.000306102\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 1196: 19097.912109 | 0.587917 | 19097.324219 Learning rate: 0.000306102\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.53984946  1.        ]]\n",
      "Minibatch total | class | pred loss at step 1198: 17788.228516 | 0.604424 | 17787.625000 Learning rate: 0.000306102\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.68905467  1.        ]]\n",
      "Minibatch total | class | pred loss at step 1200: 14911.224609 | 0.616925 | 14910.607422 Learning rate: 0.000293857\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.54940677  0.        ]\n",
      " [ 0.62074119  1.        ]]\n",
      "Validation accuracy: 52.0\n",
      "Minibatch total | class | pred loss at step 1202: 20304.103516 | 0.588758 | 20303.515625 Learning rate: 0.000293857\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.7843923   1.        ]\n",
      " [ 0.68180239  1.        ]]\n",
      "Minibatch total | class | pred loss at step 1204: 14231.588867 | 0.589389 | 14230.999023 Learning rate: 0.000293857\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 1206: 19529.158203 | 0.676877 | 19528.480469 Learning rate: 0.000293857\n",
      "Minibatch accuracy: 43.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5748021   1.        ]\n",
      " [ 0.61577588  1.        ]]\n",
      "Minibatch total | class | pred loss at step 1208: 13894.365234 | 0.690496 | 13893.674805 Learning rate: 0.000293857\n",
      "Minibatch accuracy: 43.75\n",
      "Predictions | Labels:\n",
      " [[ 0.6343047  0.       ]\n",
      " [ 0.5        0.       ]]\n",
      "Minibatch total | class | pred loss at step 1210: 17711.126953 | 0.747660 | 17710.378906 Learning rate: 0.000293857\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 1212: 19832.398438 | 0.628654 | 19831.769531 Learning rate: 0.000293857\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5374437   0.        ]\n",
      " [ 0.74579817  1.        ]]\n",
      "Minibatch total | class | pred loss at step 1214: 19087.228516 | 0.662147 | 19086.566406 Learning rate: 0.000293857\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5  1. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 1216: 13875.553711 | 0.645070 | 13874.908203 Learning rate: 0.000293857\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.68487412  1.        ]\n",
      " [ 0.61412513  0.        ]]\n",
      "Minibatch total | class | pred loss at step 1218: 13590.926758 | 0.602592 | 13590.324219 Learning rate: 0.000293857\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5  1. ]\n",
      " [ 0.5  1. ]]\n",
      "Minibatch total | class | pred loss at step 1220: 15681.047852 | 0.738144 | 15680.309570 Learning rate: 0.000293857\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.59185535  1.        ]]\n",
      "Validation accuracy: 50.0\n",
      "Minibatch total | class | pred loss at step 1222: 18319.406250 | 0.831605 | 18318.574219 Learning rate: 0.000293857\n",
      "Minibatch accuracy: 37.5\n",
      "Predictions | Labels:\n",
      " [[ 0.87724632  0.        ]\n",
      " [ 0.5         1.        ]]\n",
      "Minibatch total | class | pred loss at step 1224: 19579.314453 | 0.879263 | 19578.435547 Learning rate: 0.000293857\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 1226: 14884.809570 | 0.580317 | 14884.229492 Learning rate: 0.000293857\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  1. ]]\n",
      "Minibatch total | class | pred loss at step 1228: 20820.466797 | 0.723812 | 20819.742188 Learning rate: 0.000293857\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.89211071  1.        ]\n",
      " [ 0.84196126  1.        ]]\n",
      "Minibatch total | class | pred loss at step 1230: 19310.515625 | 0.725955 | 19309.789062 Learning rate: 0.000293857\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.69066548  1.        ]\n",
      " [ 0.91555262  1.        ]]\n",
      "Minibatch total | class | pred loss at step 1232: 18721.300781 | 0.586668 | 18720.714844 Learning rate: 0.000293857\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.74001813  1.        ]]\n",
      "Minibatch total | class | pred loss at step 1234: 13913.910156 | 0.799027 | 13913.111328 Learning rate: 0.000293857\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.74427891  0.        ]]\n",
      "Minibatch total | class | pred loss at step 1236: 13709.333008 | 0.618643 | 13708.714844 Learning rate: 0.000293857\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  1. ]]\n",
      "Minibatch total | class | pred loss at step 1238: 18821.820312 | 0.582447 | 18821.238281 Learning rate: 0.000293857\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.52682781  0.        ]]\n",
      "Minibatch total | class | pred loss at step 1240: 18225.167969 | 0.677860 | 18224.490234 Learning rate: 0.000282103\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.96073294  1.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Validation accuracy: 50.6666666667\n",
      "Minibatch total | class | pred loss at step 1242: 14576.953125 | 0.661814 | 14576.291016 Learning rate: 0.000282103\n",
      "Minibatch accuracy: 43.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5512917   0.        ]\n",
      " [ 0.86825269  1.        ]]\n",
      "Minibatch total | class | pred loss at step 1244: 12602.455078 | 0.670388 | 12601.785156 Learning rate: 0.000282103\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.81899053  1.        ]\n",
      " [ 0.75557947  1.        ]]\n",
      "Minibatch total | class | pred loss at step 1246: 13369.526367 | 0.802095 | 13368.724609 Learning rate: 0.000282103\n",
      "Minibatch accuracy: 31.25\n",
      "Predictions | Labels:\n",
      " [[ 0.67092448  0.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 1248: 19575.998047 | 0.685367 | 19575.312500 Learning rate: 0.000282103\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.67857093  1.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 1250: 19715.548828 | 0.703880 | 19714.845703 Learning rate: 0.000282103\n",
      "Minibatch accuracy: 43.75\n",
      "Predictions | Labels:\n",
      " [[ 0.83917272  1.        ]\n",
      " [ 0.60298908  0.        ]]\n",
      "Minibatch total | class | pred loss at step 1252: 12674.614258 | 0.693246 | 12673.920898 Learning rate: 0.000282103\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5  1. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 1254: 18155.988281 | 0.746744 | 18155.242188 Learning rate: 0.000282103\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5        0.       ]\n",
      " [ 0.9114337  0.       ]]\n",
      "Minibatch total | class | pred loss at step 1256: 19357.574219 | 0.581244 | 19356.992188 Learning rate: 0.000282103\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.71272838  1.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 1258: 14055.796875 | 0.599485 | 14055.197266 Learning rate: 0.000282103\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  1. ]]\n",
      "Minibatch total | class | pred loss at step 1260: 13966.724609 | 0.773507 | 13965.951172 Learning rate: 0.000282103\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Validation accuracy: 50.0\n",
      "Minibatch total | class | pred loss at step 1262: 13605.730469 | 0.640399 | 13605.089844 Learning rate: 0.000282103\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 1264: 13870.081055 | 0.676286 | 13869.404297 Learning rate: 0.000282103\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.51276284  0.        ]\n",
      " [ 0.57340121  1.        ]]\n",
      "Minibatch total | class | pred loss at step 1266: 13782.891602 | 0.616164 | 13782.275391 Learning rate: 0.000282103\n",
      "Minibatch accuracy: 43.75\n",
      "Predictions | Labels:\n",
      " [[ 0.53410506  1.        ]\n",
      " [ 0.5         1.        ]]\n",
      "Minibatch total | class | pred loss at step 1268: 20443.099609 | 0.708790 | 20442.390625 Learning rate: 0.000282103\n",
      "Minibatch accuracy: 43.75\n",
      "Predictions | Labels:\n",
      " [[ 0.80191004  0.        ]\n",
      " [ 0.5         1.        ]]\n",
      "Minibatch total | class | pred loss at step 1270: 20920.888672 | 0.689719 | 20920.199219 Learning rate: 0.000282103\n",
      "Minibatch accuracy: 43.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.62048441  0.        ]]\n",
      "Minibatch total | class | pred loss at step 1272: 13376.954102 | 0.570920 | 13376.382812 Learning rate: 0.000282103\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.68199056  1.        ]\n",
      " [ 0.61925477  1.        ]]\n",
      "Minibatch total | class | pred loss at step 1274: 20629.187500 | 0.722071 | 20628.464844 Learning rate: 0.000282103\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 1276: 15303.818359 | 0.698855 | 15303.119141 Learning rate: 0.000282103\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.72241229  1.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 1278: 19005.214844 | 0.741386 | 19004.472656 Learning rate: 0.000282103\n",
      "Minibatch accuracy: 43.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5  1. ]\n",
      " [ 0.5  1. ]]\n",
      "Minibatch total | class | pred loss at step 1280: 19608.964844 | 0.690638 | 19608.273438 Learning rate: 0.000270819\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Validation accuracy: 52.0\n",
      "Minibatch total | class | pred loss at step 1282: 13202.051758 | 0.725455 | 13201.326172 Learning rate: 0.000270819\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5  1. ]\n",
      " [ 0.5  1. ]]\n",
      "Minibatch total | class | pred loss at step 1284: 15950.400391 | 0.761389 | 15949.638672 Learning rate: 0.000270819\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5  1. ]\n",
      " [ 0.5  1. ]]\n",
      "Minibatch total | class | pred loss at step 1286: 18595.990234 | 0.691903 | 18595.298828 Learning rate: 0.000270819\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.66257167  1.        ]\n",
      " [ 0.5         1.        ]]\n",
      "Minibatch total | class | pred loss at step 1288: 14489.786133 | 0.867237 | 14488.918945 Learning rate: 0.000270819\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.66772354  1.        ]\n",
      " [ 0.74008864  0.        ]]\n",
      "Minibatch total | class | pred loss at step 1290: 19734.294922 | 0.669603 | 19733.625000 Learning rate: 0.000270819\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.51205909  0.        ]]\n",
      "Minibatch total | class | pred loss at step 1292: 14672.432617 | 0.651653 | 14671.781250 Learning rate: 0.000270819\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.71982223  1.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 1294: 18664.498047 | 0.653120 | 18663.845703 Learning rate: 0.000270819\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 1296: 17902.177734 | 0.563831 | 17901.613281 Learning rate: 0.000270819\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5  1. ]\n",
      " [ 0.5  1. ]]\n",
      "Minibatch total | class | pred loss at step 1298: 18897.794922 | 0.691680 | 18897.103516 Learning rate: 0.000270819\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.63183039  0.        ]\n",
      " [ 0.5         1.        ]]\n",
      "Minibatch total | class | pred loss at step 1300: 18757.851562 | 0.766562 | 18757.085938 Learning rate: 0.000270819\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5         1.        ]\n",
      " [ 0.84233612  1.        ]]\n",
      "Validation accuracy: 51.3333333333\n",
      "Minibatch total | class | pred loss at step 1302: 18825.046875 | 0.690658 | 18824.355469 Learning rate: 0.000270819\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.78822219  1.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 1304: 14710.824219 | 0.718559 | 14710.105469 Learning rate: 0.000270819\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.59618324  0.        ]\n",
      " [ 0.72094578  0.        ]]\n",
      "Minibatch total | class | pred loss at step 1306: 14336.644531 | 0.661720 | 14335.982422 Learning rate: 0.000270819\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.80459875  0.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 1308: 15069.676758 | 0.664805 | 15069.011719 Learning rate: 0.000270819\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5         1.        ]\n",
      " [ 0.78459316  0.        ]]\n",
      "Minibatch total | class | pred loss at step 1310: 19322.564453 | 0.628424 | 19321.935547 Learning rate: 0.000270819\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.76785964  1.        ]\n",
      " [ 0.64215773  1.        ]]\n",
      "Minibatch total | class | pred loss at step 1312: 19931.242188 | 0.722362 | 19930.519531 Learning rate: 0.000270819\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.61914778  1.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 1314: 19381.050781 | 0.620023 | 19380.431641 Learning rate: 0.000270819\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.92321634  1.        ]\n",
      " [ 0.68671101  1.        ]]\n",
      "Minibatch total | class | pred loss at step 1316: 14611.079102 | 0.559685 | 14610.519531 Learning rate: 0.000270819\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.73127246  1.        ]\n",
      " [ 0.89795542  1.        ]]\n",
      "Minibatch total | class | pred loss at step 1318: 18917.253906 | 0.666143 | 18916.587891 Learning rate: 0.000270819\n",
      "Minibatch accuracy: 43.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5  1. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 1320: 15923.583008 | 0.817000 | 15922.765625 Learning rate: 0.000259986\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.63702452  0.        ]\n",
      " [ 0.55089438  0.        ]]\n",
      "Validation accuracy: 52.0\n",
      "Minibatch total | class | pred loss at step 1322: 15351.240234 | 0.747837 | 15350.492188 Learning rate: 0.000259986\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 1324: 20637.851562 | 0.644890 | 20637.207031 Learning rate: 0.000259986\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5  1. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 1326: 19367.775391 | 0.716570 | 19367.058594 Learning rate: 0.000259986\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  1. ]]\n",
      "Minibatch total | class | pred loss at step 1328: 14257.413086 | 0.700025 | 14256.712891 Learning rate: 0.000259986\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.51437044  0.        ]\n",
      " [ 0.57797492  1.        ]]\n",
      "Minibatch total | class | pred loss at step 1330: 15017.260742 | 0.700037 | 15016.560547 Learning rate: 0.000259986\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.62520266  0.        ]\n",
      " [ 0.80785656  1.        ]]\n",
      "Minibatch total | class | pred loss at step 1332: 14841.120117 | 0.672539 | 14840.447266 Learning rate: 0.000259986\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.62482536  0.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 1334: 20210.566406 | 0.757369 | 20209.808594 Learning rate: 0.000259986\n",
      "Minibatch accuracy: 43.75\n",
      "Predictions | Labels:\n",
      " [[ 0.81558293  1.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 1336: 19749.160156 | 0.729048 | 19748.431641 Learning rate: 0.000259986\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.51091945  0.        ]\n",
      " [ 0.5         1.        ]]\n",
      "Minibatch total | class | pred loss at step 1338: 14858.833984 | 0.737676 | 14858.096680 Learning rate: 0.000259986\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.58426768  0.        ]\n",
      " [ 0.57260263  1.        ]]\n",
      "Minibatch total | class | pred loss at step 1340: 16125.720703 | 0.816673 | 16124.904297 Learning rate: 0.000259986\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.83712471  0.        ]\n",
      " [ 0.80974823  0.        ]]\n",
      "Validation accuracy: 50.0\n",
      "Minibatch total | class | pred loss at step 1342: 14952.809570 | 0.625800 | 14952.183594 Learning rate: 0.000259986\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.60389173  1.        ]\n",
      " [ 0.8403452   1.        ]]\n",
      "Minibatch total | class | pred loss at step 1344: 19387.412109 | 0.630507 | 19386.781250 Learning rate: 0.000259986\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 1346: 15363.863281 | 0.634360 | 15363.228516 Learning rate: 0.000259986\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.81832528  1.        ]]\n",
      "Minibatch total | class | pred loss at step 1348: 14572.096680 | 0.717675 | 14571.378906 Learning rate: 0.000259986\n",
      "Minibatch accuracy: 43.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.73487139  0.        ]]\n",
      "Minibatch total | class | pred loss at step 1350: 15386.557617 | 0.615665 | 15385.942383 Learning rate: 0.000259986\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.87220228  1.        ]]\n",
      "Minibatch total | class | pred loss at step 1352: 19200.923828 | 0.676003 | 19200.248047 Learning rate: 0.000259986\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.77929664  0.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 1354: 19013.687500 | 0.691641 | 19012.996094 Learning rate: 0.000259986\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5         1.        ]\n",
      " [ 0.54977757  0.        ]]\n",
      "Minibatch total | class | pred loss at step 1356: 14235.602539 | 0.597176 | 14235.004883 Learning rate: 0.000259986\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 1358: 15092.646484 | 0.593084 | 15092.053711 Learning rate: 0.000259986\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.83193058  1.        ]\n",
      " [ 0.5         1.        ]]\n",
      "Minibatch total | class | pred loss at step 1360: 21684.767578 | 0.643247 | 21684.125000 Learning rate: 0.000249587\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.86097687  1.        ]]\n",
      "Validation accuracy: 50.6666666667\n",
      "Minibatch total | class | pred loss at step 1362: 19782.068359 | 0.653843 | 19781.414062 Learning rate: 0.000249587\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.78958291  1.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 1364: 13541.705078 | 0.767493 | 13540.937500 Learning rate: 0.000249587\n",
      "Minibatch accuracy: 37.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5  1. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 1366: 18857.734375 | 0.588042 | 18857.146484 Learning rate: 0.000249587\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5  1. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 1368: 14119.454102 | 0.825206 | 14118.628906 Learning rate: 0.000249587\n",
      "Minibatch accuracy: 43.75\n",
      "Predictions | Labels:\n",
      " [[ 0.56408453  0.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 1370: 20177.541016 | 0.725194 | 20176.816406 Learning rate: 0.000249587\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 1372: 14492.445312 | 0.686582 | 14491.758789 Learning rate: 0.000249587\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5         1.        ]\n",
      " [ 0.58176506  0.        ]]\n",
      "Minibatch total | class | pred loss at step 1374: 20414.392578 | 0.652913 | 20413.740234 Learning rate: 0.000249587\n",
      "Minibatch accuracy: 43.75\n",
      "Predictions | Labels:\n",
      " [[ 0.76331985  1.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 1376: 19171.605469 | 0.562505 | 19171.042969 Learning rate: 0.000249587\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.50435704  1.        ]]\n",
      "Minibatch total | class | pred loss at step 1378: 14438.577148 | 0.565534 | 14438.011719 Learning rate: 0.000249587\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5         1.        ]\n",
      " [ 0.76283765  1.        ]]\n",
      "Minibatch total | class | pred loss at step 1380: 18415.929688 | 0.530274 | 18415.398438 Learning rate: 0.000249587\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5  1. ]\n",
      " [ 0.5  0. ]]\n",
      "Validation accuracy: 51.3333333333\n",
      "Minibatch total | class | pred loss at step 1382: 17024.216797 | 0.614640 | 17023.601562 Learning rate: 0.000249587\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.60011631  0.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 1384: 17245.728516 | 0.769442 | 17244.958984 Learning rate: 0.000249587\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  1. ]]\n",
      "Minibatch total | class | pred loss at step 1386: 21497.816406 | 0.601093 | 21497.214844 Learning rate: 0.000249587\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5         1.        ]\n",
      " [ 0.65970224  1.        ]]\n",
      "Minibatch total | class | pred loss at step 1388: 19774.228516 | 0.735372 | 19773.492188 Learning rate: 0.000249587\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.68274909  0.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 1390: 15189.371094 | 0.755600 | 15188.615234 Learning rate: 0.000249587\n",
      "Minibatch accuracy: 37.5\n",
      "Predictions | Labels:\n",
      " [[ 0.53309435  0.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 1392: 16222.515625 | 0.758026 | 16221.757812 Learning rate: 0.000249587\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.70419341  1.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 1394: 15259.288086 | 0.643450 | 15258.644531 Learning rate: 0.000249587\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 1396: 19945.994141 | 0.799151 | 19945.195312 Learning rate: 0.000249587\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5         1.        ]\n",
      " [ 0.57414752  1.        ]]\n",
      "Minibatch total | class | pred loss at step 1398: 13979.357422 | 0.638421 | 13978.718750 Learning rate: 0.000249587\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.74076766  0.        ]]\n",
      "Minibatch total | class | pred loss at step 1400: 17644.097656 | 0.629789 | 17643.468750 Learning rate: 0.000239603\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Validation accuracy: 50.6666666667\n",
      "Minibatch total | class | pred loss at step 1402: 14092.613281 | 0.666415 | 14091.947266 Learning rate: 0.000239603\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.91678137  1.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 1404: 14940.328125 | 0.673105 | 14939.655273 Learning rate: 0.000239603\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.60934907  0.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 1406: 20879.240234 | 0.657902 | 20878.582031 Learning rate: 0.000239603\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.71746612  1.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 1408: 20126.494141 | 0.696990 | 20125.796875 Learning rate: 0.000239603\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.76203167  1.        ]\n",
      " [ 0.5         1.        ]]\n",
      "Minibatch total | class | pred loss at step 1410: 19006.746094 | 0.621629 | 19006.125000 Learning rate: 0.000239603\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.58600283  0.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 1412: 19189.199219 | 0.672533 | 19188.527344 Learning rate: 0.000239603\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.74435347  0.        ]\n",
      " [ 0.93939841  1.        ]]\n",
      "Minibatch total | class | pred loss at step 1414: 19452.792969 | 0.613193 | 19452.179688 Learning rate: 0.000239603\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.7017411  1.       ]\n",
      " [ 0.5        0.       ]]\n",
      "Minibatch total | class | pred loss at step 1416: 19816.832031 | 0.756939 | 19816.074219 Learning rate: 0.000239603\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.85199028  0.        ]\n",
      " [ 0.69272184  0.        ]]\n",
      "Minibatch total | class | pred loss at step 1418: 14705.494141 | 0.664428 | 14704.830078 Learning rate: 0.000239603\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.78705722  1.        ]]\n",
      "Minibatch total | class | pred loss at step 1420: 19826.302734 | 0.599247 | 19825.703125 Learning rate: 0.000239603\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5  1. ]\n",
      " [ 0.5  0. ]]\n",
      "Validation accuracy: 50.0\n",
      "Minibatch total | class | pred loss at step 1422: 18096.306641 | 0.549734 | 18095.757812 Learning rate: 0.000239603\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.72420824  1.        ]\n",
      " [ 0.5         1.        ]]\n",
      "Minibatch total | class | pred loss at step 1424: 20586.470703 | 0.600397 | 20585.871094 Learning rate: 0.000239603\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  1. ]]\n",
      "Minibatch total | class | pred loss at step 1426: 14522.271484 | 0.640525 | 14521.630859 Learning rate: 0.000239603\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 1428: 19061.181641 | 0.650235 | 19060.531250 Learning rate: 0.000239603\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 1430: 15468.821289 | 0.636021 | 15468.185547 Learning rate: 0.000239603\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  1. ]]\n",
      "Minibatch total | class | pred loss at step 1432: 14132.689453 | 0.827701 | 14131.861328 Learning rate: 0.000239603\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.89068872  0.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 1434: 20382.908203 | 0.635744 | 20382.271484 Learning rate: 0.000239603\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5         1.        ]\n",
      " [ 0.95753872  1.        ]]\n",
      "Minibatch total | class | pred loss at step 1436: 13801.941406 | 0.632649 | 13801.308594 Learning rate: 0.000239603\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5  1. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 1438: 20319.830078 | 0.625187 | 20319.205078 Learning rate: 0.000239603\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 1440: 13642.795898 | 0.862374 | 13641.933594 Learning rate: 0.000230019\n",
      "Minibatch accuracy: 43.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Validation accuracy: 50.6666666667\n",
      "Minibatch total | class | pred loss at step 1442: 13674.305664 | 0.615942 | 13673.689453 Learning rate: 0.000230019\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 1444: 13695.643555 | 0.656898 | 13694.986328 Learning rate: 0.000230019\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.58538485  0.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 1446: 19333.996094 | 0.563367 | 19333.433594 Learning rate: 0.000230019\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.51734382  1.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 1448: 18728.654297 | 0.596184 | 18728.058594 Learning rate: 0.000230019\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.69927353  1.        ]\n",
      " [ 0.63950568  0.        ]]\n",
      "Minibatch total | class | pred loss at step 1450: 14383.194336 | 0.592635 | 14382.601562 Learning rate: 0.000230019\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.55910301  1.        ]\n",
      " [ 0.73355651  1.        ]]\n",
      "Minibatch total | class | pred loss at step 1452: 21519.705078 | 0.612034 | 21519.093750 Learning rate: 0.000230019\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.75867862  1.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 1454: 14838.707031 | 0.589720 | 14838.117188 Learning rate: 0.000230019\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.57950211  1.        ]]\n",
      "Minibatch total | class | pred loss at step 1456: 19806.759766 | 0.647547 | 19806.111328 Learning rate: 0.000230019\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.65446812  1.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 1458: 14195.499023 | 0.684872 | 14194.814453 Learning rate: 0.000230019\n",
      "Minibatch accuracy: 43.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  1. ]]\n",
      "Minibatch total | class | pred loss at step 1460: 18679.203125 | 0.775554 | 18678.427734 Learning rate: 0.000230019\n",
      "Minibatch accuracy: 43.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  1. ]]\n",
      "Validation accuracy: 50.6666666667\n",
      "Minibatch total | class | pred loss at step 1462: 20315.681641 | 0.682865 | 20314.998047 Learning rate: 0.000230019\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.62229538  1.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 1464: 19716.181641 | 0.679862 | 19715.501953 Learning rate: 0.000230019\n",
      "Minibatch accuracy: 37.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  1. ]]\n",
      "Minibatch total | class | pred loss at step 1466: 14318.445312 | 0.634051 | 14317.811523 Learning rate: 0.000230019\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.59479707  0.        ]\n",
      " [ 0.89004821  1.        ]]\n",
      "Minibatch total | class | pred loss at step 1468: 14087.587891 | 0.594190 | 14086.994141 Learning rate: 0.000230019\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5  1. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 1470: 15909.400391 | 0.696287 | 15908.704102 Learning rate: 0.000230019\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.56925386  1.        ]\n",
      " [ 0.5         1.        ]]\n",
      "Minibatch total | class | pred loss at step 1472: 20141.812500 | 0.758783 | 20141.054688 Learning rate: 0.000230019\n",
      "Minibatch accuracy: 37.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5         1.        ]\n",
      " [ 0.85397315  0.        ]]\n",
      "Minibatch total | class | pred loss at step 1474: 19267.572266 | 0.865767 | 19266.707031 Learning rate: 0.000230019\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  1. ]]\n",
      "Minibatch total | class | pred loss at step 1476: 16069.770508 | 0.585195 | 16069.185547 Learning rate: 0.000230019\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5  1. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 1478: 21639.673828 | 0.691358 | 21638.982422 Learning rate: 0.000230019\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.77979183  1.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 1480: 20377.490234 | 0.732872 | 20376.757812 Learning rate: 0.000220818\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.93229055  1.        ]\n",
      " [ 0.5         1.        ]]\n",
      "Validation accuracy: 50.0\n",
      "Minibatch total | class | pred loss at step 1482: 20430.580078 | 0.584658 | 20429.996094 Learning rate: 0.000220818\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.74086708  1.        ]\n",
      " [ 0.5         1.        ]]\n",
      "Minibatch total | class | pred loss at step 1484: 14026.317383 | 0.801406 | 14025.515625 Learning rate: 0.000220818\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.72641575  0.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 1486: 14433.260742 | 0.616576 | 14432.644531 Learning rate: 0.000220818\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5         1.        ]\n",
      " [ 0.69925231  1.        ]]\n",
      "Minibatch total | class | pred loss at step 1488: 19503.070312 | 0.605893 | 19502.464844 Learning rate: 0.000220818\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.52736318  0.        ]\n",
      " [ 0.5         1.        ]]\n",
      "Minibatch total | class | pred loss at step 1490: 18647.892578 | 0.835360 | 18647.056641 Learning rate: 0.000220818\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 1492: 14619.766602 | 0.655701 | 14619.111328 Learning rate: 0.000220818\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.85173988  1.        ]\n",
      " [ 0.72137648  1.        ]]\n",
      "Minibatch total | class | pred loss at step 1494: 13091.310547 | 0.758617 | 13090.551758 Learning rate: 0.000220818\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.73448682  1.        ]\n",
      " [ 0.79951847  1.        ]]\n",
      "Minibatch total | class | pred loss at step 1496: 13779.298828 | 0.698417 | 13778.600586 Learning rate: 0.000220818\n",
      "Minibatch accuracy: 43.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 1498: 20303.851562 | 0.683857 | 20303.167969 Learning rate: 0.000220818\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.73113078  1.        ]]\n",
      "Minibatch total | class | pred loss at step 1500: 21012.261719 | 0.785541 | 21011.476562 Learning rate: 0.000220818\n",
      "Minibatch accuracy: 25.0\n",
      "Predictions | Labels:\n",
      " [[ 0.65393829  0.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Validation accuracy: 50.6666666667\n",
      "Minibatch total | class | pred loss at step 1502: 13465.696289 | 0.644310 | 13465.051758 Learning rate: 0.000220818\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 1504: 19440.466797 | 0.717195 | 19439.750000 Learning rate: 0.000220818\n",
      "Minibatch accuracy: 43.75\n",
      "Predictions | Labels:\n",
      " [[ 0.89256036  0.        ]\n",
      " [ 0.82652581  1.        ]]\n",
      "Minibatch total | class | pred loss at step 1506: 20302.011719 | 0.612982 | 20301.398438 Learning rate: 0.000220818\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 1508: 14773.139648 | 0.602441 | 14772.537109 Learning rate: 0.000220818\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5  1. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 1510: 14688.435547 | 0.786143 | 14687.649414 Learning rate: 0.000220818\n",
      "Minibatch accuracy: 43.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.66808814  0.        ]]\n",
      "Minibatch total | class | pred loss at step 1512: 13933.545898 | 0.636631 | 13932.909180 Learning rate: 0.000220818\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 1514: 14039.209961 | 0.678668 | 14038.531250 Learning rate: 0.000220818\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.57065815  1.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 1516: 13792.868164 | 0.598046 | 13792.270508 Learning rate: 0.000220818\n",
      "Minibatch accuracy: 43.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5  1. ]\n",
      " [ 0.5  1. ]]\n",
      "Minibatch total | class | pred loss at step 1518: 20895.599609 | 0.647241 | 20894.953125 Learning rate: 0.000220818\n",
      "Minibatch accuracy: 37.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5         1.        ]\n",
      " [ 0.72794712  0.        ]]\n",
      "Minibatch total | class | pred loss at step 1520: 21013.285156 | 0.680116 | 21012.605469 Learning rate: 0.000211986\n",
      "Minibatch accuracy: 43.75\n",
      "Predictions | Labels:\n",
      " [[ 0.62453085  0.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Validation accuracy: 50.6666666667\n",
      "Minibatch total | class | pred loss at step 1522: 13714.748047 | 0.576419 | 13714.171875 Learning rate: 0.000211986\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.6260016  1.       ]\n",
      " [ 0.5        0.       ]]\n",
      "Minibatch total | class | pred loss at step 1524: 20767.527344 | 0.678256 | 20766.849609 Learning rate: 0.000211986\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 1526: 15484.051758 | 0.719789 | 15483.332031 Learning rate: 0.000211986\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 1528: 20214.697266 | 0.747561 | 20213.949219 Learning rate: 0.000211986\n",
      "Minibatch accuracy: 43.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5         1.        ]\n",
      " [ 0.68051732  0.        ]]\n",
      "Minibatch total | class | pred loss at step 1530: 15679.221680 | 0.680328 | 15678.541016 Learning rate: 0.000211986\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.61565822  1.        ]]\n",
      "Minibatch total | class | pred loss at step 1532: 13751.263672 | 0.661890 | 13750.601562 Learning rate: 0.000211986\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5  1. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 1534: 17040.931641 | 0.717569 | 17040.214844 Learning rate: 0.000211986\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5         1.        ]\n",
      " [ 0.74222279  1.        ]]\n",
      "Minibatch total | class | pred loss at step 1536: 19337.804688 | 0.707481 | 19337.097656 Learning rate: 0.000211986\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5         1.        ]\n",
      " [ 0.65745419  0.        ]]\n",
      "Minibatch total | class | pred loss at step 1538: 14428.326172 | 0.903947 | 14427.421875 Learning rate: 0.000211986\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.7782349  0.       ]\n",
      " [ 0.5        1.       ]]\n",
      "Minibatch total | class | pred loss at step 1540: 20776.130859 | 0.680733 | 20775.449219 Learning rate: 0.000211986\n",
      "Minibatch accuracy: 37.5\n",
      "Predictions | Labels:\n",
      " [[ 0.50152856  0.        ]\n",
      " [ 0.64852595  0.        ]]\n",
      "Validation accuracy: 48.6666666667\n",
      "Minibatch total | class | pred loss at step 1542: 18835.166016 | 0.646096 | 18834.519531 Learning rate: 0.000211986\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 1544: 19832.337891 | 0.612117 | 19831.726562 Learning rate: 0.000211986\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  1. ]]\n",
      "Minibatch total | class | pred loss at step 1546: 19156.988281 | 0.562939 | 19156.425781 Learning rate: 0.000211986\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5  1. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 1548: 20264.005859 | 0.669214 | 20263.335938 Learning rate: 0.000211986\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5  1. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 1550: 20193.634766 | 0.684098 | 20192.951172 Learning rate: 0.000211986\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.80431414  1.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 1552: 19327.517578 | 0.718771 | 19326.798828 Learning rate: 0.000211986\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.82350719  1.        ]]\n",
      "Minibatch total | class | pred loss at step 1554: 15841.508789 | 0.707673 | 15840.800781 Learning rate: 0.000211986\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.72647011  0.        ]\n",
      " [ 0.5         1.        ]]\n",
      "Minibatch total | class | pred loss at step 1556: 14523.408203 | 0.596776 | 14522.811523 Learning rate: 0.000211986\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.89393735  1.        ]]\n",
      "Minibatch total | class | pred loss at step 1558: 14498.856445 | 0.665414 | 14498.191406 Learning rate: 0.000211986\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.80833799  0.        ]\n",
      " [ 0.5         1.        ]]\n",
      "Minibatch total | class | pred loss at step 1560: 18611.537109 | 0.683991 | 18610.853516 Learning rate: 0.000203506\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.66338235  1.        ]\n",
      " [ 0.86140692  1.        ]]\n",
      "Validation accuracy: 50.6666666667\n",
      "Minibatch total | class | pred loss at step 1562: 19772.445312 | 0.745919 | 19771.699219 Learning rate: 0.000203506\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.76575977  0.        ]]\n",
      "Minibatch total | class | pred loss at step 1564: 19717.253906 | 0.686801 | 19716.566406 Learning rate: 0.000203506\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.81416148  1.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 1566: 15422.817383 | 0.586789 | 15422.230469 Learning rate: 0.000203506\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.9194805  1.       ]\n",
      " [ 0.5        0.       ]]\n",
      "Minibatch total | class | pred loss at step 1568: 20262.818359 | 0.609942 | 20262.208984 Learning rate: 0.000203506\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 1570: 15837.342773 | 0.818331 | 15836.524414 Learning rate: 0.000203506\n",
      "Minibatch accuracy: 43.75\n",
      "Predictions | Labels:\n",
      " [[ 0.52480191  0.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 1572: 15726.850586 | 0.706226 | 15726.144531 Learning rate: 0.000203506\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 1574: 20776.785156 | 0.626170 | 20776.158203 Learning rate: 0.000203506\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  1. ]]\n",
      "Minibatch total | class | pred loss at step 1576: 19942.177734 | 0.696503 | 19941.480469 Learning rate: 0.000203506\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5         1.        ]\n",
      " [ 0.70469308  0.        ]]\n",
      "Minibatch total | class | pred loss at step 1578: 14677.184570 | 0.703714 | 14676.480469 Learning rate: 0.000203506\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.54524809  1.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 1580: 15635.714844 | 0.656785 | 15635.057617 Learning rate: 0.000203506\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.82247317  1.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Validation accuracy: 51.3333333333\n",
      "Minibatch total | class | pred loss at step 1582: 15104.329102 | 0.647772 | 15103.681641 Learning rate: 0.000203506\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 1584: 21055.833984 | 0.836463 | 21054.998047 Learning rate: 0.000203506\n",
      "Minibatch accuracy: 31.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5        0.       ]\n",
      " [ 0.5001061  0.       ]]\n",
      "Minibatch total | class | pred loss at step 1586: 20944.925781 | 0.728426 | 20944.197266 Learning rate: 0.000203506\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5         1.        ]\n",
      " [ 0.87010372  1.        ]]\n",
      "Minibatch total | class | pred loss at step 1588: 15078.443359 | 0.711278 | 15077.732422 Learning rate: 0.000203506\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.67118585  1.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 1590: 15699.083984 | 0.812846 | 15698.271484 Learning rate: 0.000203506\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.72437996  0.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 1592: 15627.732422 | 0.650366 | 15627.082031 Learning rate: 0.000203506\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.79937619  1.        ]\n",
      " [ 0.5         1.        ]]\n",
      "Minibatch total | class | pred loss at step 1594: 20455.632812 | 0.631449 | 20455.001953 Learning rate: 0.000203506\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  1. ]]\n",
      "Minibatch total | class | pred loss at step 1596: 15502.987305 | 0.639369 | 15502.347656 Learning rate: 0.000203506\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.81564111  1.        ]\n",
      " [ 0.56350791  0.        ]]\n",
      "Minibatch total | class | pred loss at step 1598: 14634.105469 | 0.729642 | 14633.375977 Learning rate: 0.000203506\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.72597748  0.        ]\n",
      " [ 0.74491012  0.        ]]\n",
      "Minibatch total | class | pred loss at step 1600: 15590.946289 | 0.628289 | 15590.318359 Learning rate: 0.000195366\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.83326697  1.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Validation accuracy: 50.0\n",
      "Minibatch total | class | pred loss at step 1602: 20029.664062 | 0.599360 | 20029.064453 Learning rate: 0.000195366\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.85456795  1.        ]]\n",
      "Minibatch total | class | pred loss at step 1604: 20366.697266 | 0.694026 | 20366.003906 Learning rate: 0.000195366\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.51169133  0.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 1606: 14637.183594 | 0.601583 | 14636.582031 Learning rate: 0.000195366\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.70192868  1.        ]]\n",
      "Minibatch total | class | pred loss at step 1608: 15691.428711 | 0.596767 | 15690.832031 Learning rate: 0.000195366\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5         1.        ]\n",
      " [ 0.67944747  1.        ]]\n",
      "Minibatch total | class | pred loss at step 1610: 15252.148438 | 0.641110 | 15251.507812 Learning rate: 0.000195366\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.86584216  1.        ]\n",
      " [ 0.85145891  1.        ]]\n",
      "Minibatch total | class | pred loss at step 1612: 20641.285156 | 0.624681 | 20640.660156 Learning rate: 0.000195366\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 1614: 14370.474609 | 0.786588 | 14369.688477 Learning rate: 0.000195366\n",
      "Minibatch accuracy: 31.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  1. ]]\n",
      "Minibatch total | class | pred loss at step 1616: 20787.251953 | 0.592264 | 20786.660156 Learning rate: 0.000195366\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.71746612  1.        ]]\n",
      "Minibatch total | class | pred loss at step 1618: 14946.585938 | 0.749749 | 14945.835938 Learning rate: 0.000195366\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.55947399  0.        ]]\n",
      "Minibatch total | class | pred loss at step 1620: 20629.445312 | 0.629358 | 20628.816406 Learning rate: 0.000195366\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.70715588  1.        ]]\n",
      "Validation accuracy: 49.3333333333\n",
      "Minibatch total | class | pred loss at step 1622: 14980.558594 | 0.669542 | 14979.888672 Learning rate: 0.000195366\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.53097463  0.        ]\n",
      " [ 0.6987009   1.        ]]\n",
      "Minibatch total | class | pred loss at step 1624: 21103.250000 | 0.687153 | 21102.562500 Learning rate: 0.000195366\n",
      "Minibatch accuracy: 43.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  1. ]]\n",
      "Minibatch total | class | pred loss at step 1626: 20595.093750 | 0.561998 | 20594.531250 Learning rate: 0.000195366\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5  1. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 1628: 14985.112305 | 0.593196 | 14984.519531 Learning rate: 0.000195366\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.64449501  1.        ]\n",
      " [ 0.78704745  1.        ]]\n",
      "Minibatch total | class | pred loss at step 1630: 19908.970703 | 0.538696 | 19908.431641 Learning rate: 0.000195366\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.71569574  1.        ]]\n",
      "Minibatch total | class | pred loss at step 1632: 18981.011719 | 0.595119 | 18980.416016 Learning rate: 0.000195366\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.50563365  0.        ]]\n",
      "Minibatch total | class | pred loss at step 1634: 18809.384766 | 0.782029 | 18808.603516 Learning rate: 0.000195366\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5  1. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 1636: 21914.144531 | 0.599531 | 21913.544922 Learning rate: 0.000195366\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.65725094  1.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 1638: 21353.029297 | 0.678962 | 21352.349609 Learning rate: 0.000195366\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 1640: 14404.464844 | 0.715887 | 14403.749023 Learning rate: 0.000187551\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.52190679  1.        ]]\n",
      "Validation accuracy: 51.3333333333\n",
      "Minibatch total | class | pred loss at step 1642: 17230.765625 | 0.780880 | 17229.984375 Learning rate: 0.000187551\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 1644: 15879.941406 | 0.646727 | 15879.294922 Learning rate: 0.000187551\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.73528916  1.        ]]\n",
      "Minibatch total | class | pred loss at step 1646: 20796.855469 | 0.767422 | 20796.087891 Learning rate: 0.000187551\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.61388111  1.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 1648: 14300.735352 | 0.637915 | 14300.097656 Learning rate: 0.000187551\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.72477716  0.        ]\n",
      " [ 0.5         1.        ]]\n",
      "Minibatch total | class | pred loss at step 1650: 19084.529297 | 0.627522 | 19083.902344 Learning rate: 0.000187551\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5        0.       ]\n",
      " [ 0.8735624  1.       ]]\n",
      "Minibatch total | class | pred loss at step 1652: 13705.788086 | 0.778717 | 13705.009766 Learning rate: 0.000187551\n",
      "Minibatch accuracy: 43.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 1654: 14632.478516 | 0.642815 | 14631.835938 Learning rate: 0.000187551\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.78946239  1.        ]]\n",
      "Minibatch total | class | pred loss at step 1656: 22042.232422 | 0.664837 | 22041.568359 Learning rate: 0.000187551\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.51791894  0.        ]\n",
      " [ 0.60818017  1.        ]]\n",
      "Minibatch total | class | pred loss at step 1658: 21157.578125 | 0.757971 | 21156.820312 Learning rate: 0.000187551\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5         1.        ]\n",
      " [ 0.71904767  1.        ]]\n",
      "Minibatch total | class | pred loss at step 1660: 20504.962891 | 0.605503 | 20504.357422 Learning rate: 0.000187551\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Validation accuracy: 50.6666666667\n",
      "Minibatch total | class | pred loss at step 1662: 20465.394531 | 0.610143 | 20464.785156 Learning rate: 0.000187551\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.91411245  1.        ]\n",
      " [ 0.57485163  1.        ]]\n",
      "Minibatch total | class | pred loss at step 1664: 20374.966797 | 0.605992 | 20374.361328 Learning rate: 0.000187551\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.79436356  1.        ]]\n",
      "Minibatch total | class | pred loss at step 1666: 20112.847656 | 0.695621 | 20112.152344 Learning rate: 0.000187551\n",
      "Minibatch accuracy: 43.75\n",
      "Predictions | Labels:\n",
      " [[ 0.74754316  0.        ]\n",
      " [ 0.5         1.        ]]\n",
      "Minibatch total | class | pred loss at step 1668: 15189.820312 | 0.655792 | 15189.164062 Learning rate: 0.000187551\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.80224925  1.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 1670: 21237.976562 | 0.588300 | 21237.388672 Learning rate: 0.000187551\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.58695316  1.        ]]\n",
      "Minibatch total | class | pred loss at step 1672: 18724.994141 | 0.563503 | 18724.429688 Learning rate: 0.000187551\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5         1.        ]\n",
      " [ 0.76602817  1.        ]]\n",
      "Minibatch total | class | pred loss at step 1674: 19798.681641 | 0.599286 | 19798.082031 Learning rate: 0.000187551\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5  1. ]\n",
      " [ 0.5  1. ]]\n",
      "Minibatch total | class | pred loss at step 1676: 15084.642578 | 0.642314 | 15084.000000 Learning rate: 0.000187551\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 1678: 19014.845703 | 0.646167 | 19014.199219 Learning rate: 0.000187551\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 1680: 16045.643555 | 0.614019 | 16045.029297 Learning rate: 0.000180049\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5  1. ]\n",
      " [ 0.5  0. ]]\n",
      "Validation accuracy: 50.0\n",
      "Minibatch total | class | pred loss at step 1682: 15147.583008 | 0.748935 | 15146.833984 Learning rate: 0.000180049\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 1684: 19472.384766 | 0.633013 | 19471.751953 Learning rate: 0.000180049\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.95167893  1.        ]\n",
      " [ 0.5         1.        ]]\n",
      "Minibatch total | class | pred loss at step 1686: 14347.373047 | 0.647805 | 14346.725586 Learning rate: 0.000180049\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.53814733  0.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 1688: 20478.126953 | 0.603606 | 20477.523438 Learning rate: 0.000180049\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 1690: 14270.183594 | 0.882828 | 14269.300781 Learning rate: 0.000180049\n",
      "Minibatch accuracy: 43.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  1. ]]\n",
      "Minibatch total | class | pred loss at step 1692: 13955.140625 | 0.616771 | 13954.523438 Learning rate: 0.000180049\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  1. ]]\n",
      "Minibatch total | class | pred loss at step 1694: 13766.124023 | 0.643848 | 13765.480469 Learning rate: 0.000180049\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 1696: 20738.712891 | 0.545822 | 20738.167969 Learning rate: 0.000180049\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  1. ]]\n",
      "Minibatch total | class | pred loss at step 1698: 20032.697266 | 0.618454 | 20032.078125 Learning rate: 0.000180049\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.63227278  0.        ]\n",
      " [ 0.71563685  1.        ]]\n",
      "Minibatch total | class | pred loss at step 1700: 14716.583984 | 0.588101 | 14715.996094 Learning rate: 0.000180049\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.76892674  1.        ]\n",
      " [ 0.91731721  1.        ]]\n",
      "Validation accuracy: 50.6666666667\n",
      "Minibatch total | class | pred loss at step 1702: 20695.714844 | 0.612506 | 20695.101562 Learning rate: 0.000180049\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  1. ]]\n",
      "Minibatch total | class | pred loss at step 1704: 15175.761719 | 0.636590 | 15175.125000 Learning rate: 0.000180049\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.52782542  1.        ]\n",
      " [ 0.86880141  1.        ]]\n",
      "Minibatch total | class | pred loss at step 1706: 20508.164062 | 0.679647 | 20507.484375 Learning rate: 0.000180049\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  1. ]]\n",
      "Minibatch total | class | pred loss at step 1708: 14738.636719 | 0.694868 | 14737.941406 Learning rate: 0.000180049\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5         1.        ]\n",
      " [ 0.92330295  1.        ]]\n",
      "Minibatch total | class | pred loss at step 1710: 19660.761719 | 0.855321 | 19659.906250 Learning rate: 0.000180049\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5         1.        ]\n",
      " [ 0.62422043  0.        ]]\n",
      "Minibatch total | class | pred loss at step 1712: 21522.826172 | 0.693205 | 21522.132812 Learning rate: 0.000180049\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.83569491  1.        ]]\n",
      "Minibatch total | class | pred loss at step 1714: 20125.052734 | 0.692383 | 20124.359375 Learning rate: 0.000180049\n",
      "Minibatch accuracy: 37.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5         1.        ]\n",
      " [ 0.91010576  1.        ]]\n",
      "Minibatch total | class | pred loss at step 1716: 14399.215820 | 0.618383 | 14398.597656 Learning rate: 0.000180049\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.88825828  1.        ]\n",
      " [ 0.8220157   1.        ]]\n",
      "Minibatch total | class | pred loss at step 1718: 14176.055664 | 0.588763 | 14175.466797 Learning rate: 0.000180049\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5        0.       ]\n",
      " [ 0.6034745  0.       ]]\n",
      "Minibatch total | class | pred loss at step 1720: 16569.466797 | 0.703503 | 16568.763672 Learning rate: 0.000172847\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5  1. ]\n",
      " [ 0.5  0. ]]\n",
      "Validation accuracy: 49.3333333333\n",
      "Minibatch total | class | pred loss at step 1722: 21646.339844 | 0.741898 | 21645.597656 Learning rate: 0.000172847\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.84799731  0.        ]\n",
      " [ 0.5         1.        ]]\n",
      "Minibatch total | class | pred loss at step 1724: 15484.913086 | 0.846866 | 15484.066406 Learning rate: 0.000172847\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5         1.        ]\n",
      " [ 0.61245704  1.        ]]\n",
      "Minibatch total | class | pred loss at step 1726: 16065.810547 | 0.556689 | 16065.253906 Learning rate: 0.000172847\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.62845629  1.        ]]\n",
      "Minibatch total | class | pred loss at step 1728: 22931.521484 | 0.697320 | 22930.824219 Learning rate: 0.000172847\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 1730: 14596.572266 | 0.829740 | 14595.742188 Learning rate: 0.000172847\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.51356047  1.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 1732: 21050.892578 | 0.604388 | 21050.289062 Learning rate: 0.000172847\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5  1. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 1734: 14575.282227 | 0.805352 | 14574.476562 Learning rate: 0.000172847\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.65241379  1.        ]]\n",
      "Minibatch total | class | pred loss at step 1736: 15300.346680 | 0.636222 | 15299.710938 Learning rate: 0.000172847\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5741955  1.       ]\n",
      " [ 0.5        1.       ]]\n",
      "Minibatch total | class | pred loss at step 1738: 20040.882812 | 0.573725 | 20040.308594 Learning rate: 0.000172847\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5         1.        ]\n",
      " [ 0.76969367  1.        ]]\n",
      "Minibatch total | class | pred loss at step 1740: 20856.867188 | 0.816785 | 20856.050781 Learning rate: 0.000172847\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Validation accuracy: 48.6666666667\n",
      "Minibatch total | class | pred loss at step 1742: 14937.761719 | 0.648766 | 14937.113281 Learning rate: 0.000172847\n",
      "Minibatch accuracy: 43.75\n",
      "Predictions | Labels:\n",
      " [[ 0.69692045  1.        ]\n",
      " [ 0.53016788  0.        ]]\n",
      "Minibatch total | class | pred loss at step 1744: 13080.734375 | 0.796510 | 13079.937500 Learning rate: 0.000172847\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.82539338  1.        ]\n",
      " [ 0.6448223   0.        ]]\n",
      "Minibatch total | class | pred loss at step 1746: 14632.324219 | 0.679238 | 14631.644531 Learning rate: 0.000172847\n",
      "Minibatch accuracy: 43.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  1. ]]\n",
      "Minibatch total | class | pred loss at step 1748: 20620.072266 | 0.655037 | 20619.417969 Learning rate: 0.000172847\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.75954992  1.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 1750: 21721.056641 | 0.769015 | 21720.287109 Learning rate: 0.000172847\n",
      "Minibatch accuracy: 31.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  1. ]]\n",
      "Minibatch total | class | pred loss at step 1752: 14081.181641 | 0.635079 | 14080.546875 Learning rate: 0.000172847\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 1754: 18763.931641 | 0.595423 | 18763.335938 Learning rate: 0.000172847\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.90016615  1.        ]\n",
      " [ 0.5         1.        ]]\n",
      "Minibatch total | class | pred loss at step 1756: 20526.070312 | 0.607795 | 20525.462891 Learning rate: 0.000172847\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.70641768  1.        ]]\n",
      "Minibatch total | class | pred loss at step 1758: 15549.221680 | 0.581394 | 15548.640625 Learning rate: 0.000172847\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 1760: 14715.531250 | 0.755287 | 14714.776367 Learning rate: 0.000165933\n",
      "Minibatch accuracy: 43.75\n",
      "Predictions | Labels:\n",
      " [[ 0.6382966  0.       ]\n",
      " [ 0.5        0.       ]]\n",
      "Validation accuracy: 50.6666666667\n",
      "Minibatch total | class | pred loss at step 1762: 14372.951172 | 0.630596 | 14372.320312 Learning rate: 0.000165933\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.58048737  0.        ]]\n",
      "Minibatch total | class | pred loss at step 1764: 14248.281250 | 0.685142 | 14247.595703 Learning rate: 0.000165933\n",
      "Minibatch accuracy: 43.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  1. ]]\n",
      "Minibatch total | class | pred loss at step 1766: 13775.254883 | 0.614465 | 13774.640625 Learning rate: 0.000165933\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5  1. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 1768: 21551.958984 | 0.681192 | 21551.277344 Learning rate: 0.000165933\n",
      "Minibatch accuracy: 37.5\n",
      "Predictions | Labels:\n",
      " [[ 0.69621694  0.        ]\n",
      " [ 0.82935905  1.        ]]\n",
      "Minibatch total | class | pred loss at step 1770: 21168.654297 | 0.675534 | 21167.978516 Learning rate: 0.000165933\n",
      "Minibatch accuracy: 37.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  1. ]]\n",
      "Minibatch total | class | pred loss at step 1772: 14043.072266 | 0.574115 | 14042.498047 Learning rate: 0.000165933\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.69254559  1.        ]]\n",
      "Minibatch total | class | pred loss at step 1774: 21153.503906 | 0.682723 | 21152.820312 Learning rate: 0.000165933\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 1776: 15790.974609 | 0.714950 | 15790.259766 Learning rate: 0.000165933\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.54704863  0.        ]]\n",
      "Minibatch total | class | pred loss at step 1778: 20715.994141 | 0.710943 | 20715.283203 Learning rate: 0.000165933\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.59639305  0.        ]\n",
      " [ 0.5         1.        ]]\n",
      "Minibatch total | class | pred loss at step 1780: 15832.907227 | 0.699173 | 15832.208008 Learning rate: 0.000165933\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.56922716  1.        ]\n",
      " [ 0.80434978  1.        ]]\n",
      "Validation accuracy: 51.3333333333\n",
      "Minibatch total | class | pred loss at step 1782: 14020.836914 | 0.639875 | 14020.197266 Learning rate: 0.000165933\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.52244484  1.        ]]\n",
      "Minibatch total | class | pred loss at step 1784: 17843.359375 | 0.707142 | 17842.652344 Learning rate: 0.000165933\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.76263165  1.        ]\n",
      " [ 0.60046637  0.        ]]\n",
      "Minibatch total | class | pred loss at step 1786: 19648.833984 | 0.699300 | 19648.134766 Learning rate: 0.000165933\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.64054477  0.        ]\n",
      " [ 0.72208273  1.        ]]\n",
      "Minibatch total | class | pred loss at step 1788: 14482.893555 | 0.857924 | 14482.035156 Learning rate: 0.000165933\n",
      "Minibatch accuracy: 37.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5  1. ]\n",
      " [ 0.5  1. ]]\n",
      "Minibatch total | class | pred loss at step 1790: 21476.119141 | 0.683032 | 21475.435547 Learning rate: 0.000165933\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.63984793  0.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 1792: 19621.921875 | 0.639640 | 19621.283203 Learning rate: 0.000165933\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 1794: 21274.789062 | 0.606055 | 21274.183594 Learning rate: 0.000165933\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5  1. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 1796: 20638.562500 | 0.558112 | 20638.003906 Learning rate: 0.000165933\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 1798: 21194.154297 | 0.689536 | 21193.464844 Learning rate: 0.000165933\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 1800: 20560.382812 | 0.708201 | 20559.673828 Learning rate: 0.000159296\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Validation accuracy: 49.3333333333\n",
      "Minibatch total | class | pred loss at step 1802: 19823.425781 | 0.726320 | 19822.699219 Learning rate: 0.000159296\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.81276035  1.        ]\n",
      " [ 0.66795945  1.        ]]\n",
      "Minibatch total | class | pred loss at step 1804: 16233.465820 | 0.649785 | 16232.816406 Learning rate: 0.000159296\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5         1.        ]\n",
      " [ 0.74166143  0.        ]]\n",
      "Minibatch total | class | pred loss at step 1806: 14881.741211 | 0.590757 | 14881.150391 Learning rate: 0.000159296\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.87695181  1.        ]\n",
      " [ 0.5         1.        ]]\n",
      "Minibatch total | class | pred loss at step 1808: 14413.569336 | 0.612580 | 14412.957031 Learning rate: 0.000159296\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5  1. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 1810: 19627.001953 | 0.713919 | 19626.287109 Learning rate: 0.000159296\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.8813045  1.       ]\n",
      " [ 0.5        0.       ]]\n",
      "Minibatch total | class | pred loss at step 1812: 19894.845703 | 0.792004 | 19894.052734 Learning rate: 0.000159296\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.67930406  0.        ]\n",
      " [ 0.5         0.        ]]\n",
      "Minibatch total | class | pred loss at step 1814: 20032.365234 | 0.734921 | 20031.630859 Learning rate: 0.000159296\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.59430254  0.        ]]\n",
      "Minibatch total | class | pred loss at step 1816: 16763.929688 | 0.620528 | 16763.308594 Learning rate: 0.000159296\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 1818: 21748.294922 | 0.612239 | 21747.683594 Learning rate: 0.000159296\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 1820: 15185.946289 | 0.798636 | 15185.147461 Learning rate: 0.000159296\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5         0.        ]\n",
      " [ 0.82350743  0.        ]]\n",
      "Validation accuracy: 49.3333333333\n",
      "Minibatch total | class | pred loss at step 1822: 16301.550781 | 0.728927 | 16300.822266 Learning rate: 0.000159296\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 1824: 21733.273438 | 0.596495 | 21732.677734 Learning rate: 0.000159296\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5  1. ]\n",
      " [ 0.5  0. ]]\n",
      "Minibatch total | class | pred loss at step 1826: 21379.394531 | 0.690472 | 21378.703125 Learning rate: 0.000159296\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.66282547  0.        ]\n",
      " [ 0.5         0.        ]]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-105-4072ba3b3174>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mresults_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 382\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    383\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    653\u001b[0m     \u001b[0mmovers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_with_movers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m     results = self._do_run(handle, target_list, unique_fetches,\n\u001b[0;32m--> 655\u001b[0;31m                            feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m     \u001b[0;31m# User may have fetched the same tensor multiple times, but we\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    721\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 723\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m    724\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m    728\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m    710\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m    711\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 712\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m    713\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_steps = 4001\n",
    "\n",
    "trace_file = open('./tracing/timeline.json', 'w')\n",
    "save_path = './checkpoints/model.ckpt'\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    tf.initialize_local_variables().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_dataset.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels}\n",
    "        _, lc, lp, tl, lrate, predictions, summary = session.run(\n",
    "            [optimizer, loss_class, loss_pred, total_loss, learning_rate, train_predictions, merged], \n",
    "            feed_dict=feed_dict, \n",
    "            options=run_options,\n",
    "            run_metadata=run_metadata)\n",
    "        results_writer.add_summary(summary, step)\n",
    "        if (step % 2 == 0):\n",
    "            print('Minibatch total | class | pred loss at step %d: %f | %f | %f' % (step, tl, lc, lp), 'Learning rate:', lrate)\n",
    "            print('Minibatch accuracy:', accuracy(predictions, batch_labels.astype(np.bool_)))\n",
    "            print('Predictions | Labels:\\n', np.concatenate((predictions[:2], batch_labels[:2]), axis=1))\n",
    "        if (step % 20 == 0):\n",
    "            print('Validation accuracy:', accuracy(valid_predictions.eval(), valid_labels.astype(np.bool_)))\n",
    "            \n",
    "    # Save tracing into disl\n",
    "    trace = timeline.Timeline(step_stats=run_metadata.step_stats)\n",
    "    trace_file.write(trace.generate_chrome_trace_format(show_memory=True))\n",
    "            \n",
    "    # Save the variables to disk.\n",
    "    saver.save(session, save_path)\n",
    "    print(\"Model saved in file: %s\" % save_path)\n",
    "            \n",
    "    results_writer.flush()\n",
    "    results_writer.close()\n",
    "\n",
    "    print('Finished training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "valid_batch_size = 1\n",
    "\n",
    "def accuracy_notpercent(predictions, labels):\n",
    "  return np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    saver.restore(session, save_path)\n",
    "    print('Model Loaded')\n",
    "    data_split = np.array_split(valid_dataset, valid_dataset.shape[0]//valid_batch_size, axis=0)\n",
    "    labels_split = np.array_split(valid_labels, valid_labels.shape[0]//valid_batch_size, axis=0)\n",
    "    correct_predictions = 0\n",
    "    for idx, batch_data in enumerate(data_split):\n",
    "        correct_predictions += accuracy_notpercent(\n",
    "            train_prediction.eval(feed_dict={tf_train_dataset: batch_data}), \n",
    "            labels_split[idx])\n",
    "        print('accuracy:', (100.0*correct_predictions)/((idx+1)*valid_batch_size))\n",
    "        \n",
    "        \n",
    "    print('Finished validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
