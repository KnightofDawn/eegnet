{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import done\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from six.moves import cPickle as pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import timeline\n",
    "\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"import done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load pickled dataset into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unpickling ./data/trainsh1.pickle\n",
      "dataset shape: (30, 240000, 16)\n",
      "labels shape: (30,)\n"
     ]
    }
   ],
   "source": [
    "name_pickle = './data/trainsh1.pickle'\n",
    "\n",
    "with open(name_pickle, 'rb') as f:\n",
    "    print('Unpickling ' + name_pickle)\n",
    "    load = pickle.load(f)\n",
    "    dataset = load['data']\n",
    "    labels = load['labels']\n",
    "    del load\n",
    "    print('dataset shape:', dataset.shape)\n",
    "    print('labels shape:', labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reformat data for training\n",
    "- Divide each file with 240000 samples into smaller batch_samples ~= size of receptive field of eegnet\n",
    "- Keep valid_dataset nr of samples intact for proper validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_array(array):\n",
    "    # Normalize mean=0 and sigma=0.25: axis=0 is along columns, vertical lines.\n",
    "    array -= np.mean(array, axis=0) \n",
    "    array /= 2*np.ptp(array, axis=0)\n",
    "    return array\n",
    "    \n",
    "def clean_normalize_data_labels(data, labels, sigma=0.5):\n",
    "    data_tmp = list()\n",
    "    labels_tmp = list()\n",
    "    for idx, d in enumerate(data):\n",
    "        if (np.count_nonzero(d) < 10) or (np.any(np.std(d, axis=0) < sigma)):\n",
    "            continue\n",
    "        d = normalize_array(d)\n",
    "        data_tmp.append(d)\n",
    "        labels_tmp.append(labels[idx])\n",
    "    return np.asarray(data_tmp), np.asarray(labels_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset shape: (5923, 1, 800, 16) train_labels shape: (5923, 2) mix: 0.560864426811\n",
      "valid_dataset shape: (200, 1, 800, 16) valid_labels shape: (200, 2) mix: 0.32\n"
     ]
    }
   ],
   "source": [
    "#Output size of the layer\n",
    "num_labels = 2\n",
    "\n",
    "#60% for train and 40% for validation\n",
    "split_idx = int(dataset.shape[0]*0.8)\n",
    "#nr of splits\n",
    "nrOfSplits = 300\n",
    "\n",
    "def format_data(data, labels, nr_splits):\n",
    "    shape = data.shape\n",
    "    # reshape [batch, samples, channels] into [batch * samples, channels]\n",
    "    data = np.reshape(data, (shape[0]*shape[1], shape[2]))\n",
    "    # Split 2D array into the desired smaller chuncks\n",
    "    data = np.asarray(np.split(data, shape[0]*nr_splits, axis=0))\n",
    "    # labels are obtained by repeating original labels nr_splits times\n",
    "    labels = np.repeat((np.arange(num_labels) == labels[:,None]).astype(np.float32), nr_splits, axis=0)\n",
    "    # normalize and eliminate batches that only contain drop-outs\n",
    "    data, labels = clean_normalize_data_labels(data, labels, 0.01)\n",
    "    # data has to be 4D for tensorflow (insert an empty dimension)\n",
    "    data = data[:,None,:,:]\n",
    "    # shuffle data and labels mantaining relation between them. Important after the small batches.\n",
    "    shuffle_idx = np.random.permutation(data.shape[0])\n",
    "    data = data[shuffle_idx,:,:,:]\n",
    "    labels = labels[shuffle_idx]\n",
    "    return data, labels\n",
    "\n",
    "# shuffle file data\n",
    "shuffle_idx = np.random.permutation(dataset.shape[0])\n",
    "dataset = dataset[shuffle_idx,:,:]\n",
    "labels = labels[shuffle_idx]\n",
    "# format and split data into smaller chunks\n",
    "train_dataset, train_labels = format_data(dataset[:split_idx], labels[:split_idx], nrOfSplits)\n",
    "valid_dataset, valid_labels = format_data(dataset[split_idx:-1], labels[split_idx:-1], nrOfSplits)\n",
    "del dataset, labels\n",
    "\n",
    "valid_dataset = valid_dataset[:200]\n",
    "valid_labels = valid_labels[:200]\n",
    "\n",
    "print('train_dataset shape:', train_dataset.shape, 'train_labels shape:', train_labels.shape, \n",
    "      'mix:', float(np.count_nonzero(train_labels[:,1], axis=0))/train_labels.shape[0])\n",
    "print('valid_dataset shape:', valid_dataset.shape, 'valid_labels shape:', valid_labels.shape, \n",
    "      'mix:', float(np.count_nonzero(valid_labels[:,1], axis=0))/valid_labels.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot some data to have an idea of how data looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f3cd0faf6d0>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhkAAAC7CAYAAADIUYxHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\njUaj0WiyghYZGo1Go9FossL/BzG/FX0PNC0kAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3cfcc33550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplot(211)\n",
    "plt.plot(train_dataset[1,0,:,0])\n",
    "plt.plot(train_dataset[3,0,:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EEGNET implementation\n",
    "\n",
    "Ideas\n",
    "  - Condition classification based on sensor?\n",
    "  \n",
    "Part of https://arxiv.org/pdf/1609.03499.pdf that most concerns classification:\n",
    "\"As a last experiment we looked at speech recognition with WaveNets on the TIMIT (Garofolo et al., 1993) dataset. For this task we added a mean-pooling layer after the dilation convolutions that aggregated the activations to coarser frames spanning 10 milliseconds (160 x downsampling). The pooling layer was followed by a few non-causal convolutions. We trained WaveNet with two loss terms, one to predict the next sample and one to classify the frame, the model generalized better than with a single loss and achieved 18.8 PER on the test set, which is to our knowledge the best score obtained from a model trained directly on raw audio on TIMIT.\"\n",
    "\n",
    "Look into: http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43290.pdf\n",
    "\"Input: This layer extracts 275 ms waveform segments from each of M input microphones. Successive inputs are hopped by 10ms. At the 16kHz sampling rate used in our experiments each segment contains M X 4401 dimensions.\"\n",
    "...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computational graph created\n"
     ]
    }
   ],
   "source": [
    "#How many files are supplied per batch.\n",
    "batch_size=16\n",
    "#Number of samples in each batch entry\n",
    "batch_samples=train_dataset.shape[2]\n",
    "#How many filters to learn for the input.\n",
    "input_channels=16\n",
    "#How many filters to learn for the residual.\n",
    "residual_channels=2*input_channels\n",
    "# size after pooling layer\n",
    "pool_size = 600\n",
    "#number of steps after which learning rate is decayed\n",
    "decay_steps=500\n",
    "\n",
    "filter_width=3\n",
    "\n",
    "slim = tf.contrib.slim\n",
    "\n",
    "#Construct computation graph\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    \n",
    "    # Input data\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, 1, batch_samples, input_channels))\n",
    "    tf_train_labels = tf.placeholder(tf.uint8, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset, dtype=tf.float32)\n",
    "    tf_valid_labels = tf.constant(valid_labels, dtype=tf.float32)\n",
    "    \n",
    "    def accuracy(logits, labels):\n",
    "        return tf.div(\n",
    "            tf.mul(\n",
    "                tf.to_float(\n",
    "                tf.reduce_sum(tf.to_int32(tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))))\n",
    "                ), \n",
    "                100), \n",
    "                tf.to_float(tf.shape(logits)[0]))\n",
    "    \n",
    "    def network(batch_data, reuse=False, is_training=True):\n",
    "        with tf.variable_scope('eegnet_network', reuse=reuse):\n",
    "            with slim.arg_scope([slim.batch_norm], \n",
    "                                is_training=is_training):\n",
    "                with slim.arg_scope([slim.conv2d, slim.fully_connected], \n",
    "                                    weights_initializer=slim.xavier_initializer(), \n",
    "                                    normalizer_fn=slim.batch_norm):\n",
    "                    with tf.variable_scope('input_layer'):\n",
    "                        hidden = slim.conv2d(batch_data, residual_channels, [1, filter_width], stride=1, rate=1, \n",
    "                                             activation_fn=None, scope='conv1')\n",
    "\n",
    "                    with tf.variable_scope('hidden'):\n",
    "                        with tf.variable_scope('layer1'):\n",
    "                            layer_input = hidden\n",
    "                            hidden = slim.conv2d(hidden, 2*residual_channels, [1, filter_width], stride=1, rate=2, \n",
    "                                                 activation_fn=None, scope='dilconv')\n",
    "                            filtr, gate = tf.split(3, 2, hidden) # split features in half\n",
    "                            hidden = tf.mul(tf.tanh(filtr), tf.sigmoid(gate), name='filterXgate')\n",
    "                            hidden = slim.conv2d(hidden, residual_channels, 1, activation_fn=None, scope='1x1skip')\n",
    "                            skip = hidden # skip conn\n",
    "                            hidden = tf.add(hidden, layer_input) # residual conn\n",
    "                        with tf.variable_scope('layer2'):\n",
    "                            layer_input = hidden\n",
    "                            hidden = slim.conv2d(hidden, 2*residual_channels, [1, filter_width], stride=1, rate=4, \n",
    "                                                 activation_fn=None, scope='dilconv')\n",
    "                            filtr, gate = tf.split(3, 2, hidden) # split features in half\n",
    "                            hidden = tf.mul(tf.tanh(filtr), tf.sigmoid(gate), name='filterXgate')\n",
    "                            hidden = slim.conv2d(hidden, residual_channels, 1, activation_fn=None, scope='1x1skip')\n",
    "                            skip = tf.add(skip, hidden) # skip conn\n",
    "                            hidden = tf.add(hidden, layer_input) # residual conn\n",
    "                        with tf.variable_scope('layer3'):\n",
    "                            hidden = slim.conv2d(hidden, 2*residual_channels, [1, filter_width], stride=1, rate=8, \n",
    "                                                 activation_fn=None, scope='dilconv')\n",
    "                            filtr, gate = tf.split(3, 2, hidden) # split features in half\n",
    "                            hidden = tf.mul(tf.tanh(filtr), tf.sigmoid(gate), name='filterXgate')\n",
    "                            hidden = slim.conv2d(hidden, residual_channels, 1, activation_fn=None, scope='1x1skip')\n",
    "                            skip = tf.add(skip, hidden) # skip conn\n",
    "                        \n",
    "                    with tf.variable_scope('skip_processing'):\n",
    "                        hidden = tf.nn.relu(skip)\n",
    "                        hidden = slim.conv2d(skip, 4, 1, activation_fn=tf.nn.relu, scope='1x1conv1')\n",
    "\n",
    "                    with tf.variable_scope('classification'):                        \n",
    "                        hidden = slim.avg_pool2d(hidden, [1, batch_samples*2//pool_size], [1, batch_samples//pool_size])\n",
    "                        shape = hidden.get_shape().as_list()\n",
    "                        hidden = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "                        classf = slim.fully_connected(hidden, num_labels, activation_fn=None, scope='fc1')\n",
    "        return classf \n",
    "\n",
    "    with tf.name_scope('eegnet_handling'):\n",
    "        with tf.name_scope('network'):\n",
    "            classification = network(tf_train_dataset)\n",
    "        with tf.name_scope('loss'):\n",
    "            loss = slim.losses.softmax_cross_entropy(classification, tf_train_labels, scope='classification_loss')\n",
    "            tf.scalar_summary('loss', loss_class)\n",
    "        with tf.name_scope('optimizer'):\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=1e-3, epsilon=1e-4)\n",
    "                                .minimize(loss, var_list=tf.trainable_variables())\n",
    "        with tf.name_scope('accuracy'):\n",
    "            train_predictions = tf.nn.softmax(classification)\n",
    "            valid_predictions = tf.nn.softmax(network(tf_valid_dataset, True, True))\n",
    "            train_accuracy = accuracy(train_predictions, tf_train_labels)\n",
    "            valid_accuracy = accuracy(valid_predictions, tf_valid_labels)\n",
    "\n",
    "    # Add histograms for trainable variables.\n",
    "    for var in tf.trainable_variables():\n",
    "        tf.histogram_summary(var.op.name, var)\n",
    "        \n",
    "    # Add summaries for activations: NOT WORKING YET. TF ERROR.\n",
    "    #slim.summarize_activations()\n",
    "    \n",
    "    #Merge all summaries and write to a folder\n",
    "    merged = tf.merge_all_summaries()\n",
    "    results_writer = tf.train.SummaryWriter('./results', graph)\n",
    "    \n",
    "    # Add ops to save and restore all the variables.\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    #tracing for timeline\n",
    "    run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "    run_metadata = tf.RunMetadata()    \n",
    "    \n",
    "print('computational graph created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch total loss at step 0: 0.841751\n",
      "Minibatch accuracy: 43.75\n",
      "Predictions | Labels:\n",
      " [[ 0.78327972  0.21672028  0.          1.        ]\n",
      " [ 0.09624994  0.90375006  0.          1.        ]]\n",
      "Validation accuracy: 51.5\n",
      "Minibatch total loss at step 13: 0.864857\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.38631848  0.61368155  1.          0.        ]\n",
      " [ 0.47363889  0.52636105  0.          1.        ]]\n",
      "Minibatch total loss at step 26: 0.589013\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.74266088  0.25733912  1.          0.        ]\n",
      " [ 0.13986848  0.8601315   0.          1.        ]]\n",
      "Minibatch total loss at step 39: 0.762747\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.62429917  0.3757008   0.          1.        ]\n",
      " [ 0.54838437  0.45161566  1.          0.        ]]\n",
      "Minibatch total loss at step 52: 0.541155\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.34092394  0.65907604  1.          0.        ]\n",
      " [ 0.41385239  0.58614761  1.          0.        ]]\n",
      "Minibatch total loss at step 65: 0.497695\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.76222903  0.23777093  1.          0.        ]\n",
      " [ 0.65147495  0.34852502  0.          1.        ]]\n",
      "Minibatch total loss at step 78: 0.589841\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.50750339  0.49249661  0.          1.        ]\n",
      " [ 0.74580067  0.2541993   1.          0.        ]]\n",
      "Minibatch total loss at step 91: 0.635983\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.63429022  0.36570978  0.          1.        ]\n",
      " [ 0.7467317   0.25326821  1.          0.        ]]\n",
      "Validation accuracy: 62.0\n",
      "Minibatch total loss at step 104: 0.612702\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.56937265  0.43062738  0.          1.        ]\n",
      " [ 0.41517159  0.58482844  1.          0.        ]]\n",
      "Minibatch total loss at step 117: 0.692124\n",
      "Minibatch accuracy: 43.75\n",
      "Predictions | Labels:\n",
      " [[ 0.64863265  0.35136732  0.          1.        ]\n",
      " [ 0.67460078  0.32539925  0.          1.        ]]\n",
      "Minibatch total loss at step 130: 0.648530\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.78409213  0.21590793  0.          1.        ]\n",
      " [ 0.77996314  0.22003686  1.          0.        ]]\n",
      "Minibatch total loss at step 143: 0.896834\n",
      "Minibatch accuracy: 37.5\n",
      "Predictions | Labels:\n",
      " [[ 0.52870178  0.47129816  0.          1.        ]\n",
      " [ 0.88361192  0.11638807  0.          1.        ]]\n",
      "Minibatch total loss at step 156: 0.644567\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.60900193  0.3909981   0.          1.        ]\n",
      " [ 0.78533226  0.21466781  0.          1.        ]]\n",
      "Minibatch total loss at step 169: 0.654433\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.35900456  0.64099538  0.          1.        ]\n",
      " [ 0.16043077  0.83956921  1.          0.        ]]\n",
      "Minibatch total loss at step 182: 0.684413\n",
      "Minibatch accuracy: 43.75\n",
      "Predictions | Labels:\n",
      " [[ 0.6799168   0.32008323  0.          1.        ]\n",
      " [ 0.4613705   0.53862947  0.          1.        ]]\n",
      "Minibatch total loss at step 195: 0.484101\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.3447969   0.6552031   1.          0.        ]\n",
      " [ 0.27918589  0.72081411  0.          1.        ]]\n",
      "Validation accuracy: 59.0\n",
      "Minibatch total loss at step 208: 0.707866\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.60637933  0.39362067  1.          0.        ]\n",
      " [ 0.432785    0.56721497  0.          1.        ]]\n",
      "Minibatch total loss at step 221: 0.588212\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.08319166  0.91680843  0.          1.        ]\n",
      " [ 0.78352952  0.21647054  0.          1.        ]]\n",
      "Minibatch total loss at step 234: 0.828376\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.22494163  0.77505839  1.          0.        ]\n",
      " [ 0.09737827  0.90262175  0.          1.        ]]\n",
      "Minibatch total loss at step 247: 0.777453\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.43905783  0.56094217  0.          1.        ]\n",
      " [ 0.68647248  0.31352749  1.          0.        ]]\n",
      "Minibatch total loss at step 260: 0.615746\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.16368261  0.83631742  0.          1.        ]\n",
      " [ 0.29875693  0.7012431   0.          1.        ]]\n",
      "Minibatch total loss at step 273: 0.532498\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.49255744  0.50744253  0.          1.        ]\n",
      " [ 0.25582942  0.74417061  0.          1.        ]]\n",
      "Minibatch total loss at step 286: 0.622275\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.40412346  0.59587651  1.          0.        ]\n",
      " [ 0.77328205  0.22671795  0.          1.        ]]\n",
      "Minibatch total loss at step 299: 0.663003\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.45463902  0.54536098  0.          1.        ]\n",
      " [ 0.04883841  0.95116162  0.          1.        ]]\n",
      "Validation accuracy: 55.5\n",
      "Minibatch total loss at step 312: 0.645941\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.84441793  0.15558204  1.          0.        ]\n",
      " [ 0.09715259  0.90284741  0.          1.        ]]\n",
      "Minibatch total loss at step 325: 0.363649\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.52769321  0.47230685  1.          0.        ]\n",
      " [ 0.10932576  0.89067423  0.          1.        ]]\n",
      "Minibatch total loss at step 338: 0.831010\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.36888054  0.63111943  0.          1.        ]\n",
      " [ 0.07279831  0.92720169  1.          0.        ]]\n",
      "Minibatch total loss at step 351: 0.783380\n",
      "Minibatch accuracy: 43.75\n",
      "Predictions | Labels:\n",
      " [[ 0.22779801  0.77220201  0.          1.        ]\n",
      " [ 0.37971377  0.62028623  1.          0.        ]]\n",
      "Minibatch total loss at step 364: 0.530636\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.56940794  0.43059209  1.          0.        ]\n",
      " [ 0.43600926  0.56399071  1.          0.        ]]\n",
      "Minibatch total loss at step 377: 0.607841\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.42472303  0.57527697  0.          1.        ]\n",
      " [ 0.40426055  0.59573948  0.          1.        ]]\n",
      "Minibatch total loss at step 390: 0.417120\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.7242555   0.27574456  1.          0.        ]\n",
      " [ 0.62546539  0.37453458  1.          0.        ]]\n",
      "Validation accuracy: 55.5\n",
      "Minibatch total loss at step 403: 0.559459\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.69592899  0.30407104  1.          0.        ]\n",
      " [ 0.48987415  0.51012582  1.          0.        ]]\n",
      "Minibatch total loss at step 416: 0.615397\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.82458228  0.17541775  1.          0.        ]\n",
      " [ 0.46064144  0.53935856  0.          1.        ]]\n",
      "Minibatch total loss at step 429: 0.687404\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.28945473  0.7105453   0.          1.        ]\n",
      " [ 0.13976428  0.86023575  0.          1.        ]]\n",
      "Minibatch total loss at step 442: 0.663020\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.37887472  0.62112528  0.          1.        ]\n",
      " [ 0.0678119   0.93218809  0.          1.        ]]\n",
      "Minibatch total loss at step 455: 0.569542\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.23039545  0.76960456  0.          1.        ]\n",
      " [ 0.78175998  0.21824005  1.          0.        ]]\n",
      "Minibatch total loss at step 468: 0.431671\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.61192793  0.38807201  0.          1.        ]\n",
      " [ 0.26892814  0.73107183  0.          1.        ]]\n",
      "Minibatch total loss at step 481: 0.429669\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.651568    0.348432    1.          0.        ]\n",
      " [ 0.62561607  0.37438396  0.          1.        ]]\n",
      "Minibatch total loss at step 494: 0.885568\n",
      "Minibatch accuracy: 43.75\n",
      "Predictions | Labels:\n",
      " [[ 0.44795206  0.55204791  0.          1.        ]\n",
      " [ 0.28677455  0.71322548  1.          0.        ]]\n",
      "Validation accuracy: 54.0\n",
      "Minibatch total loss at step 507: 0.511880\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.91454059  0.0854594   0.          1.        ]\n",
      " [ 0.84227097  0.15772896  1.          0.        ]]\n",
      "Minibatch total loss at step 520: 0.552088\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.55447036  0.44552955  1.          0.        ]\n",
      " [ 0.58024913  0.41975084  1.          0.        ]]\n",
      "Minibatch total loss at step 533: 0.355395\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.95587337  0.04412663  1.          0.        ]\n",
      " [ 0.65869164  0.34130839  1.          0.        ]]\n",
      "Minibatch total loss at step 546: 0.635736\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.48003373  0.5199663   0.          1.        ]\n",
      " [ 0.29348537  0.70651466  0.          1.        ]]\n",
      "Minibatch total loss at step 559: 0.432117\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.91523635  0.08476368  1.          0.        ]\n",
      " [ 0.6822533   0.3177467   0.          1.        ]]\n",
      "Minibatch total loss at step 572: 0.488694\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.64953542  0.35046455  1.          0.        ]\n",
      " [ 0.45527792  0.54472208  1.          0.        ]]\n",
      "Minibatch total loss at step 585: 0.388487\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.15909007  0.84090996  0.          1.        ]\n",
      " [ 0.90941614  0.09058387  1.          0.        ]]\n",
      "Minibatch total loss at step 598: 0.406326\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.82901388  0.17098613  1.          0.        ]\n",
      " [ 0.16783865  0.83216131  0.          1.        ]]\n",
      "Validation accuracy: 54.5\n",
      "Minibatch total loss at step 611: 0.570115\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.60525751  0.39474249  0.          1.        ]\n",
      " [ 0.03206551  0.96793449  0.          1.        ]]\n",
      "Minibatch total loss at step 624: 0.637993\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.35465294  0.64534706  0.          1.        ]\n",
      " [ 0.30786106  0.69213897  1.          0.        ]]\n",
      "Minibatch total loss at step 637: 0.818762\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.15721759  0.84278244  1.          0.        ]\n",
      " [ 0.56129599  0.43870398  1.          0.        ]]\n",
      "Minibatch total loss at step 650: 0.710138\n",
      "Minibatch accuracy: 43.75\n",
      "Predictions | Labels:\n",
      " [[ 0.98048556  0.01951443  1.          0.        ]\n",
      " [ 0.18243623  0.81756377  1.          0.        ]]\n",
      "Minibatch total loss at step 663: 0.563070\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.8671996   0.13280042  1.          0.        ]\n",
      " [ 0.30780026  0.69219971  1.          0.        ]]\n",
      "Minibatch total loss at step 676: 0.650028\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.17325233  0.8267476   0.          1.        ]\n",
      " [ 0.57562733  0.42437264  0.          1.        ]]\n",
      "Minibatch total loss at step 689: 0.397439\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.85028541  0.14971462  1.          0.        ]\n",
      " [ 0.0846239   0.91537607  0.          1.        ]]\n",
      "Validation accuracy: 55.5\n",
      "Minibatch total loss at step 702: 0.649867\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.02029198  0.97970802  0.          1.        ]\n",
      " [ 0.06036028  0.93963969  0.          1.        ]]\n",
      "Minibatch total loss at step 715: 0.666151\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.34494865  0.65505129  0.          1.        ]\n",
      " [ 0.74652767  0.25347236  0.          1.        ]]\n",
      "Minibatch total loss at step 728: 0.580711\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.27799097  0.722009    1.          0.        ]\n",
      " [ 0.47079691  0.52920306  1.          0.        ]]\n",
      "Minibatch total loss at step 741: 0.428805\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.56776905  0.43223098  0.          1.        ]\n",
      " [ 0.60165632  0.39834359  1.          0.        ]]\n",
      "Minibatch total loss at step 754: 0.515673\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.54712063  0.4528794   1.          0.        ]\n",
      " [ 0.48644665  0.51355332  0.          1.        ]]\n",
      "Minibatch total loss at step 767: 0.603197\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.39136559  0.60863447  0.          1.        ]\n",
      " [ 0.66036016  0.33963984  0.          1.        ]]\n",
      "Minibatch total loss at step 780: 0.483394\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.20441997  0.79557997  0.          1.        ]\n",
      " [ 0.90379846  0.09620152  1.          0.        ]]\n",
      "Minibatch total loss at step 793: 0.485385\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.25160453  0.7483955   0.          1.        ]\n",
      " [ 0.44531554  0.55468452  0.          1.        ]]\n",
      "Validation accuracy: 54.0\n",
      "Minibatch total loss at step 806: 0.455148\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.30587673  0.69412327  0.          1.        ]\n",
      " [ 0.96630847  0.03369156  1.          0.        ]]\n",
      "Minibatch total loss at step 819: 0.515550\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.34551701  0.65448302  0.          1.        ]\n",
      " [ 0.36655194  0.63344806  0.          1.        ]]\n",
      "Minibatch total loss at step 832: 0.558485\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.56935221  0.43064779  1.          0.        ]\n",
      " [ 0.56442231  0.43557766  1.          0.        ]]\n",
      "Minibatch total loss at step 845: 0.314030\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.06254052  0.93745953  0.          1.        ]\n",
      " [ 0.21143399  0.78856599  0.          1.        ]]\n",
      "Minibatch total loss at step 858: 0.506051\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.38008651  0.61991352  0.          1.        ]\n",
      " [ 0.59510136  0.40489867  0.          1.        ]]\n",
      "Minibatch total loss at step 871: 0.494090\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.22667404  0.77332598  0.          1.        ]\n",
      " [ 0.23503228  0.76496768  1.          0.        ]]\n",
      "Minibatch total loss at step 884: 0.584478\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.28206286  0.71793711  0.          1.        ]\n",
      " [ 0.26392475  0.73607522  1.          0.        ]]\n",
      "Minibatch total loss at step 897: 0.407394\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.57574528  0.42425472  1.          0.        ]\n",
      " [ 0.41861406  0.58138597  0.          1.        ]]\n",
      "Validation accuracy: 49.0\n",
      "Minibatch total loss at step 910: 0.535755\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.62289202  0.37710801  0.          1.        ]\n",
      " [ 0.63759643  0.3624036   0.          1.        ]]\n",
      "Minibatch total loss at step 923: 0.397511\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.03017863  0.96982139  0.          1.        ]\n",
      " [ 0.27656704  0.72343302  0.          1.        ]]\n",
      "Minibatch total loss at step 936: 0.861111\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.43239862  0.56760144  0.          1.        ]\n",
      " [ 0.72957206  0.27042797  0.          1.        ]]\n",
      "Minibatch total loss at step 949: 0.685985\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.09096852  0.90903151  0.          1.        ]\n",
      " [ 0.0117092   0.98829079  0.          1.        ]]\n",
      "Minibatch total loss at step 962: 0.489392\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.91741931  0.08258072  1.          0.        ]\n",
      " [ 0.39320761  0.60679233  0.          1.        ]]\n",
      "Minibatch total loss at step 975: 0.440282\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.727467    0.27253306  1.          0.        ]\n",
      " [ 0.06782302  0.93217695  0.          1.        ]]\n",
      "Minibatch total loss at step 988: 0.352109\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.6265381   0.3734619   1.          0.        ]\n",
      " [ 0.89711946  0.10288051  1.          0.        ]]\n",
      "Validation accuracy: 52.0\n",
      "Minibatch total loss at step 1001: 0.603569\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.74745613  0.25254387  1.          0.        ]\n",
      " [ 0.62956786  0.37043217  0.          1.        ]]\n",
      "Minibatch total loss at step 1014: 0.487275\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.22769114  0.77230889  0.          1.        ]\n",
      " [ 0.56561571  0.43438435  0.          1.        ]]\n",
      "Minibatch total loss at step 1027: 0.729416\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.49842629  0.50157374  0.          1.        ]\n",
      " [ 0.27066574  0.72933418  1.          0.        ]]\n",
      "Minibatch total loss at step 1040: 0.574051\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.52931911  0.47068089  1.          0.        ]\n",
      " [ 0.67922848  0.32077157  1.          0.        ]]\n",
      "Minibatch total loss at step 1053: 0.509855\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.43507212  0.56492794  0.          1.        ]\n",
      " [ 0.70127255  0.29872748  0.          1.        ]]\n",
      "Minibatch total loss at step 1066: 0.458839\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.79338396  0.20661604  1.          0.        ]\n",
      " [ 0.10984585  0.89015412  0.          1.        ]]\n",
      "Minibatch total loss at step 1079: 0.339877\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.25141469  0.74858528  0.          1.        ]\n",
      " [ 0.47780389  0.52219611  0.          1.        ]]\n",
      "Minibatch total loss at step 1092: 0.712709\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.95315486  0.04684517  1.          0.        ]\n",
      " [ 0.29521927  0.70478076  0.          1.        ]]\n",
      "Validation accuracy: 51.0\n",
      "Minibatch total loss at step 1105: 0.491576\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.0675602   0.9324398   0.          1.        ]\n",
      " [ 0.47700533  0.52299464  0.          1.        ]]\n",
      "Minibatch total loss at step 1118: 0.347869\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.72251916  0.27748081  1.          0.        ]\n",
      " [ 0.04743246  0.95256758  0.          1.        ]]\n",
      "Minibatch total loss at step 1131: 0.645258\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.15359312  0.84640688  0.          1.        ]\n",
      " [ 0.59420955  0.40579045  1.          0.        ]]\n",
      "Minibatch total loss at step 1144: 0.543817\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.49798813  0.5020119   1.          0.        ]\n",
      " [ 0.46243653  0.53756356  0.          1.        ]]\n",
      "Minibatch total loss at step 1157: 0.496822\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.43953314  0.56046683  0.          1.        ]\n",
      " [ 0.48658991  0.51341009  0.          1.        ]]\n",
      "Minibatch total loss at step 1170: 0.440500\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.37343016  0.62656987  0.          1.        ]\n",
      " [ 0.45100504  0.54899496  0.          1.        ]]\n",
      "Minibatch total loss at step 1183: 0.577613\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.66965365  0.33034635  1.          0.        ]\n",
      " [ 0.88127792  0.11872203  1.          0.        ]]\n",
      "Minibatch total loss at step 1196: 0.538325\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.51260126  0.48739871  1.          0.        ]\n",
      " [ 0.94802886  0.0519711   1.          0.        ]]\n",
      "Validation accuracy: 54.5\n",
      "Minibatch total loss at step 1209: 0.386521\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.17659885  0.82340109  0.          1.        ]\n",
      " [ 0.44391146  0.55608845  0.          1.        ]]\n",
      "Minibatch total loss at step 1222: 0.437167\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.03854138  0.96145862  0.          1.        ]\n",
      " [ 0.96420825  0.0357917   1.          0.        ]]\n",
      "Minibatch total loss at step 1235: 0.384332\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.9670943   0.0329057   1.          0.        ]\n",
      " [ 0.49438244  0.50561756  1.          0.        ]]\n",
      "Minibatch total loss at step 1248: 0.358696\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.45659512  0.54340482  0.          1.        ]\n",
      " [ 0.39696607  0.60303396  1.          0.        ]]\n",
      "Minibatch total loss at step 1261: 0.532624\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.75713402  0.24286601  0.          1.        ]\n",
      " [ 0.56030875  0.43969128  0.          1.        ]]\n",
      "Minibatch total loss at step 1274: 0.446015\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.14082114  0.85917884  0.          1.        ]\n",
      " [ 0.844845    0.15515496  1.          0.        ]]\n",
      "Minibatch total loss at step 1287: 0.927226\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.18260407  0.81739599  0.          1.        ]\n",
      " [ 0.90710384  0.09289617  1.          0.        ]]\n",
      "Minibatch total loss at step 1300: 0.562952\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.0808759   0.91912413  0.          1.        ]\n",
      " [ 0.12048116  0.87951887  0.          1.        ]]\n",
      "Validation accuracy: 53.0\n",
      "Minibatch total loss at step 1313: 0.492312\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.36948803  0.63051194  0.          1.        ]\n",
      " [ 0.37226978  0.62773019  0.          1.        ]]\n",
      "Minibatch total loss at step 1326: 0.455486\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.38297901  0.61702102  1.          0.        ]\n",
      " [ 0.74003398  0.25996602  1.          0.        ]]\n",
      "Minibatch total loss at step 1339: 0.367617\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.11752404  0.88247591  0.          1.        ]\n",
      " [ 0.78447241  0.21552767  1.          0.        ]]\n",
      "Minibatch total loss at step 1352: 0.587130\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.03579436  0.96420562  0.          1.        ]\n",
      " [ 0.62171245  0.37828752  1.          0.        ]]\n",
      "Minibatch total loss at step 1365: 0.416591\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.12583198  0.87416804  0.          1.        ]\n",
      " [ 0.75941354  0.24058646  1.          0.        ]]\n",
      "Minibatch total loss at step 1378: 0.381652\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.11407077  0.88592923  0.          1.        ]\n",
      " [ 0.70141107  0.29858893  1.          0.        ]]\n",
      "Minibatch total loss at step 1391: 0.529876\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.9126004   0.08739968  1.          0.        ]\n",
      " [ 0.1168192   0.88318074  0.          1.        ]]\n",
      "Validation accuracy: 54.0\n",
      "Minibatch total loss at step 1404: 0.376573\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.93688667  0.06311326  1.          0.        ]\n",
      " [ 0.16400261  0.8359974   0.          1.        ]]\n",
      "Minibatch total loss at step 1417: 0.379078\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.06733706  0.9326629   0.          1.        ]\n",
      " [ 0.53238541  0.46761459  1.          0.        ]]\n",
      "Minibatch total loss at step 1430: 0.761757\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.2094349   0.79056507  1.          0.        ]\n",
      " [ 0.13924251  0.86075741  1.          0.        ]]\n",
      "Minibatch total loss at step 1443: 0.420119\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.63663071  0.36336923  1.          0.        ]\n",
      " [ 0.66828996  0.33171007  1.          0.        ]]\n",
      "Minibatch total loss at step 1456: 0.310829\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.54511726  0.45488277  1.          0.        ]\n",
      " [ 0.0124923   0.98750764  0.          1.        ]]\n",
      "Minibatch total loss at step 1469: 0.339603\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.55083656  0.44916344  1.          0.        ]\n",
      " [ 0.0245122   0.97548783  0.          1.        ]]\n",
      "Minibatch total loss at step 1482: 0.517971\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.49065462  0.50934541  0.          1.        ]\n",
      " [ 0.66717571  0.33282429  0.          1.        ]]\n",
      "Minibatch total loss at step 1495: 0.402589\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.72776926  0.27223077  1.          0.        ]\n",
      " [ 0.43933743  0.56066257  1.          0.        ]]\n",
      "Validation accuracy: 54.5\n",
      "Minibatch total loss at step 1508: 0.482919\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.09229166  0.90770829  0.          1.        ]\n",
      " [ 0.09994066  0.90005934  0.          1.        ]]\n",
      "Minibatch total loss at step 1521: 0.346070\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.3190375   0.68096256  0.          1.        ]\n",
      " [ 0.74922878  0.25077119  1.          0.        ]]\n",
      "Minibatch total loss at step 1534: 0.450822\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5331713   0.46682876  0.          1.        ]\n",
      " [ 0.51518202  0.48481801  0.          1.        ]]\n",
      "Minibatch total loss at step 1547: 0.665226\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.02320585  0.97679406  0.          1.        ]\n",
      " [ 0.27881742  0.72118258  1.          0.        ]]\n",
      "Minibatch total loss at step 1560: 0.392750\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.7138325   0.28616756  0.          1.        ]\n",
      " [ 0.17184187  0.8281582   0.          1.        ]]\n",
      "Minibatch total loss at step 1573: 0.531309\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.07859638  0.92140365  0.          1.        ]\n",
      " [ 0.9536922   0.04630782  1.          0.        ]]\n",
      "Minibatch total loss at step 1586: 0.391334\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.27357468  0.72642535  0.          1.        ]\n",
      " [ 0.95276922  0.04723083  1.          0.        ]]\n",
      "Minibatch total loss at step 1599: 0.680825\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.84716523  0.15283473  1.          0.        ]\n",
      " [ 0.68720776  0.31279215  0.          1.        ]]\n",
      "Validation accuracy: 52.0\n",
      "Minibatch total loss at step 1612: 0.312358\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.65076792  0.34923205  1.          0.        ]\n",
      " [ 0.04755493  0.95244509  0.          1.        ]]\n",
      "Minibatch total loss at step 1625: 0.348327\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.50879115  0.49120876  0.          1.        ]\n",
      " [ 0.05403937  0.94596064  0.          1.        ]]\n",
      "Minibatch total loss at step 1638: 0.421264\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.52479488  0.47520515  1.          0.        ]\n",
      " [ 0.3939667   0.60603333  1.          0.        ]]\n",
      "Minibatch total loss at step 1651: 0.994895\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.21020213  0.7897979   1.          0.        ]\n",
      " [ 0.16530941  0.83469063  1.          0.        ]]\n",
      "Minibatch total loss at step 1664: 0.505705\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.52326268  0.47673732  1.          0.        ]\n",
      " [ 0.89542764  0.10457234  1.          0.        ]]\n",
      "Minibatch total loss at step 1677: 0.807276\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.90691304  0.09308699  1.          0.        ]\n",
      " [ 0.05994069  0.9400593   0.          1.        ]]\n",
      "Minibatch total loss at step 1690: 0.253973\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.12486563  0.87513441  0.          1.        ]\n",
      " [ 0.20402949  0.7959705   0.          1.        ]]\n",
      "Validation accuracy: 55.5\n",
      "Minibatch total loss at step 1703: 0.297141\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.85231423  0.14768577  1.          0.        ]\n",
      " [ 0.03605659  0.96394348  0.          1.        ]]\n",
      "Minibatch total loss at step 1716: 0.284255\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.20393778  0.79606229  0.          1.        ]\n",
      " [ 0.5221417   0.4778583   0.          1.        ]]\n",
      "Minibatch total loss at step 1729: 0.493566\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.62923706  0.37076303  1.          0.        ]\n",
      " [ 0.70176017  0.29823989  0.          1.        ]]\n",
      "Minibatch total loss at step 1742: 0.598358\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.30162051  0.69837946  0.          1.        ]\n",
      " [ 0.95621461  0.04378533  1.          0.        ]]\n",
      "Minibatch total loss at step 1755: 0.467586\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.60851985  0.39148018  0.          1.        ]\n",
      " [ 0.73145217  0.26854777  1.          0.        ]]\n",
      "Minibatch total loss at step 1768: 0.347131\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.05278923  0.94721073  0.          1.        ]\n",
      " [ 0.40118408  0.59881598  1.          0.        ]]\n",
      "Minibatch total loss at step 1781: 0.690566\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.02255186  0.97744817  0.          1.        ]\n",
      " [ 0.67805791  0.32194206  1.          0.        ]]\n",
      "Minibatch total loss at step 1794: 0.786726\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.44989538  0.55010462  1.          0.        ]\n",
      " [ 0.92272723  0.07727278  0.          1.        ]]\n",
      "Validation accuracy: 55.5\n",
      "Minibatch total loss at step 1807: 0.462364\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.05903471  0.94096529  0.          1.        ]\n",
      " [ 0.40632612  0.59367383  0.          1.        ]]\n",
      "Minibatch total loss at step 1820: 0.485423\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.46603298  0.53396702  0.          1.        ]\n",
      " [ 0.4135856   0.58641434  1.          0.        ]]\n",
      "Minibatch total loss at step 1833: 0.391694\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.2073708   0.79262918  0.          1.        ]\n",
      " [ 0.39208958  0.60791039  0.          1.        ]]\n",
      "Minibatch total loss at step 1846: 0.423677\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.30723026  0.69276971  0.          1.        ]\n",
      " [ 0.94855982  0.05144009  1.          0.        ]]\n",
      "Minibatch total loss at step 1859: 0.415191\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.00959467  0.99040526  0.          1.        ]\n",
      " [ 0.06912196  0.9308781   0.          1.        ]]\n",
      "Minibatch total loss at step 1872: 0.370116\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.01411102  0.98588902  0.          1.        ]\n",
      " [ 0.40149042  0.59850961  1.          0.        ]]\n",
      "Minibatch total loss at step 1885: 0.391456\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.66669106  0.33330891  1.          0.        ]\n",
      " [ 0.65407801  0.34592202  1.          0.        ]]\n",
      "Minibatch total loss at step 1898: 0.475644\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.61907578  0.38092422  1.          0.        ]\n",
      " [ 0.56032121  0.43967879  1.          0.        ]]\n",
      "Validation accuracy: 58.5\n",
      "Minibatch total loss at step 1911: 0.444367\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.09079446  0.90920556  0.          1.        ]\n",
      " [ 0.41731554  0.58268446  0.          1.        ]]\n",
      "Minibatch total loss at step 1924: 0.298418\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.95951885  0.04048115  1.          0.        ]\n",
      " [ 0.96001738  0.03998262  1.          0.        ]]\n",
      "Minibatch total loss at step 1937: 0.314718\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.84982497  0.15017505  1.          0.        ]\n",
      " [ 0.94527143  0.05472862  1.          0.        ]]\n",
      "Minibatch total loss at step 1950: 0.339461\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.77836299  0.221637    1.          0.        ]\n",
      " [ 0.91179252  0.08820743  1.          0.        ]]\n",
      "Minibatch total loss at step 1963: 0.230374\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.39156839  0.60843164  0.          1.        ]\n",
      " [ 0.07922024  0.92077971  0.          1.        ]]\n",
      "Minibatch total loss at step 1976: 0.349869\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.96759677  0.03240332  1.          0.        ]\n",
      " [ 0.77756518  0.22243485  1.          0.        ]]\n",
      "Minibatch total loss at step 1989: 0.685375\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.14190651  0.8580935   0.          1.        ]\n",
      " [ 0.53787208  0.46212795  1.          0.        ]]\n",
      "Validation accuracy: 52.5\n",
      "Minibatch total loss at step 2002: 0.388997\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.29167426  0.70832574  0.          1.        ]\n",
      " [ 0.49805763  0.5019424   0.          1.        ]]\n",
      "Minibatch total loss at step 2015: 0.422319\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.20888947  0.79111046  1.          0.        ]\n",
      " [ 0.01388557  0.98611438  0.          1.        ]]\n",
      "Minibatch total loss at step 2028: 0.512963\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.13879509  0.86120492  0.          1.        ]\n",
      " [ 0.5704003   0.42959973  0.          1.        ]]\n",
      "Minibatch total loss at step 2041: 0.471515\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.02320096  0.97679895  0.          1.        ]\n",
      " [ 0.02972122  0.97027874  0.          1.        ]]\n",
      "Minibatch total loss at step 2054: 0.587103\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.09055129  0.90944874  0.          1.        ]\n",
      " [ 0.01483931  0.98516071  0.          1.        ]]\n",
      "Minibatch total loss at step 2067: 0.344798\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.51495177  0.4850482   0.          1.        ]\n",
      " [ 0.37795228  0.62204772  0.          1.        ]]\n",
      "Minibatch total loss at step 2080: 0.404717\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.01657302  0.98342705  0.          1.        ]\n",
      " [ 0.71910465  0.28089535  1.          0.        ]]\n",
      "Minibatch total loss at step 2093: 0.645324\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.95135444  0.04864551  1.          0.        ]\n",
      " [ 0.08204293  0.91795701  0.          1.        ]]\n",
      "Validation accuracy: 56.5\n",
      "Minibatch total loss at step 2106: 0.349330\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.20868464  0.79131538  0.          1.        ]\n",
      " [ 0.04762915  0.95237082  0.          1.        ]]\n",
      "Minibatch total loss at step 2119: 0.347128\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.31656465  0.68343538  0.          1.        ]\n",
      " [ 0.69334662  0.30665335  0.          1.        ]]\n",
      "Minibatch total loss at step 2132: 0.568919\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.62459087  0.37540913  0.          1.        ]\n",
      " [ 0.27002037  0.72997963  1.          0.        ]]\n",
      "Minibatch total loss at step 2145: 0.358705\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.01032005  0.98967987  0.          1.        ]\n",
      " [ 0.57871354  0.42128652  1.          0.        ]]\n",
      "Minibatch total loss at step 2158: 0.454875\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.04447768  0.95552236  0.          1.        ]\n",
      " [ 0.70120841  0.29879162  1.          0.        ]]\n",
      "Minibatch total loss at step 2171: 0.228876\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.08111604  0.91888392  0.          1.        ]\n",
      " [ 0.16446818  0.83553189  0.          1.        ]]\n",
      "Minibatch total loss at step 2184: 0.519499\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.04942119  0.95057881  1.          0.        ]\n",
      " [ 0.43660256  0.56339741  0.          1.        ]]\n",
      "Minibatch total loss at step 2197: 0.380647\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.4156734   0.58432662  1.          0.        ]\n",
      " [ 0.79847622  0.2015238   1.          0.        ]]\n",
      "Validation accuracy: 49.5\n",
      "Minibatch total loss at step 2210: 0.276642\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.6639601   0.3360399   1.          0.        ]\n",
      " [ 0.81010962  0.18989041  1.          0.        ]]\n",
      "Minibatch total loss at step 2223: 0.322395\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.10229523  0.89770484  0.          1.        ]\n",
      " [ 0.44838923  0.55161077  0.          1.        ]]\n",
      "Minibatch total loss at step 2236: 0.448264\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.62710947  0.37289056  1.          0.        ]\n",
      " [ 0.23389688  0.76610309  0.          1.        ]]\n",
      "Minibatch total loss at step 2249: 0.441850\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.56517047  0.43482959  1.          0.        ]\n",
      " [ 0.46490231  0.53509772  0.          1.        ]]\n",
      "Minibatch total loss at step 2262: 0.566760\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.25208503  0.74791503  0.          1.        ]\n",
      " [ 0.82840151  0.17159852  0.          1.        ]]\n",
      "Minibatch total loss at step 2275: 0.452345\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.27212182  0.72787815  0.          1.        ]\n",
      " [ 0.04556869  0.95443135  0.          1.        ]]\n",
      "Minibatch total loss at step 2288: 0.561852\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.0273014   0.97269857  0.          1.        ]\n",
      " [ 0.36620533  0.63379467  1.          0.        ]]\n",
      "Validation accuracy: 54.5\n",
      "Minibatch total loss at step 2301: 0.451482\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.88381648  0.11618355  1.          0.        ]\n",
      " [ 0.56265515  0.43734482  1.          0.        ]]\n",
      "Minibatch total loss at step 2314: 0.346120\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.088693    0.91130698  0.          1.        ]\n",
      " [ 0.01022384  0.98977613  0.          1.        ]]\n",
      "Minibatch total loss at step 2327: 0.260610\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.22289591  0.77710408  0.          1.        ]\n",
      " [ 0.60959148  0.39040846  1.          0.        ]]\n",
      "Minibatch total loss at step 2340: 0.533881\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.31287321  0.68712676  1.          0.        ]\n",
      " [ 0.62646377  0.3735362   1.          0.        ]]\n",
      "Minibatch total loss at step 2353: 0.327661\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.89825755  0.10174251  1.          0.        ]\n",
      " [ 0.33896741  0.66103262  0.          1.        ]]\n",
      "Minibatch total loss at step 2366: 0.338345\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.77790594  0.22209409  1.          0.        ]\n",
      " [ 0.86113542  0.13886455  1.          0.        ]]\n",
      "Minibatch total loss at step 2379: 0.258249\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.79440165  0.20559832  1.          0.        ]\n",
      " [ 0.9639411   0.03605885  1.          0.        ]]\n",
      "Minibatch total loss at step 2392: 0.358307\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.1923995   0.8076005   0.          1.        ]\n",
      " [ 0.15351149  0.84648854  0.          1.        ]]\n",
      "Validation accuracy: 56.5\n",
      "Minibatch total loss at step 2405: 0.317801\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.80460429  0.19539572  0.          1.        ]\n",
      " [ 0.33577576  0.66422415  0.          1.        ]]\n",
      "Minibatch total loss at step 2418: 0.385639\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.64152032  0.35847971  1.          0.        ]\n",
      " [ 0.07722438  0.92277563  0.          1.        ]]\n",
      "Minibatch total loss at step 2431: 0.405294\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.69634938  0.30365059  1.          0.        ]\n",
      " [ 0.571055    0.42894506  1.          0.        ]]\n",
      "Minibatch total loss at step 2444: 0.376653\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.22351596  0.77648407  0.          1.        ]\n",
      " [ 0.76061857  0.23938149  1.          0.        ]]\n",
      "Minibatch total loss at step 2457: 0.325086\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.08152936  0.91847062  0.          1.        ]\n",
      " [ 0.61245316  0.38754687  1.          0.        ]]\n",
      "Minibatch total loss at step 2470: 0.363914\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.32972425  0.67027581  1.          0.        ]\n",
      " [ 0.41478714  0.58521289  1.          0.        ]]\n",
      "Minibatch total loss at step 2483: 0.287854\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.63050383  0.36949611  1.          0.        ]\n",
      " [ 0.12924086  0.87075913  0.          1.        ]]\n",
      "Minibatch total loss at step 2496: 0.322924\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.18821087  0.8117891   1.          0.        ]\n",
      " [ 0.29610956  0.7038905   0.          1.        ]]\n",
      "Validation accuracy: 53.0\n",
      "Minibatch total loss at step 2509: 0.460995\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.12497307  0.87502688  1.          0.        ]\n",
      " [ 0.09259811  0.90740186  0.          1.        ]]\n",
      "Minibatch total loss at step 2522: 0.685214\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.09628887  0.90371108  0.          1.        ]\n",
      " [ 0.87115622  0.1288438   0.          1.        ]]\n",
      "Minibatch total loss at step 2535: 0.497353\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.00946501  0.99053496  0.          1.        ]\n",
      " [ 0.89001119  0.10998882  1.          0.        ]]\n",
      "Minibatch total loss at step 2548: 0.515246\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.04276766  0.95723236  0.          1.        ]\n",
      " [ 0.82785243  0.17214754  1.          0.        ]]\n",
      "Minibatch total loss at step 2561: 0.420287\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.83955663  0.16044343  0.          1.        ]\n",
      " [ 0.49902177  0.50097823  1.          0.        ]]\n",
      "Minibatch total loss at step 2574: 0.346303\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.71975011  0.28024992  1.          0.        ]\n",
      " [ 0.85483211  0.14516787  1.          0.        ]]\n",
      "Minibatch total loss at step 2587: 0.232547\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.72814703  0.27185297  1.          0.        ]\n",
      " [ 0.80914193  0.19085814  1.          0.        ]]\n",
      "Minibatch total loss at step 2600: 0.328581\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.42314228  0.57685775  0.          1.        ]\n",
      " [ 0.54357666  0.45642328  0.          1.        ]]\n",
      "Validation accuracy: 51.5\n",
      "Minibatch total loss at step 2613: 0.645481\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.41760716  0.58239281  0.          1.        ]\n",
      " [ 0.41340199  0.58659804  1.          0.        ]]\n",
      "Minibatch total loss at step 2626: 0.451596\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.93339908  0.06660091  1.          0.        ]\n",
      " [ 0.32811061  0.67188942  1.          0.        ]]\n",
      "Minibatch total loss at step 2639: 0.467661\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.44775143  0.55224854  0.          1.        ]\n",
      " [ 0.84075814  0.1592419   1.          0.        ]]\n",
      "Minibatch total loss at step 2652: 0.281665\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.94325465  0.05674531  1.          0.        ]\n",
      " [ 0.0154534   0.9845466   0.          1.        ]]\n",
      "Minibatch total loss at step 2665: 0.382807\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.50735575  0.49264422  0.          1.        ]\n",
      " [ 0.07479206  0.92520797  0.          1.        ]]\n",
      "Minibatch total loss at step 2678: 0.395458\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.79790246  0.20209752  1.          0.        ]\n",
      " [ 0.18375364  0.81624639  0.          1.        ]]\n",
      "Minibatch total loss at step 2691: 0.216039\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.20645189  0.79354811  0.          1.        ]\n",
      " [ 0.92008245  0.07991758  1.          0.        ]]\n",
      "Validation accuracy: 54.5\n",
      "Minibatch total loss at step 2704: 0.437522\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.44586539  0.55413461  0.          1.        ]\n",
      " [ 0.23916422  0.76083577  0.          1.        ]]\n",
      "Minibatch total loss at step 2717: 0.404939\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.1730565   0.82694358  1.          0.        ]\n",
      " [ 0.75543612  0.24456391  1.          0.        ]]\n",
      "Minibatch total loss at step 2730: 0.519032\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.42936578  0.57063425  1.          0.        ]\n",
      " [ 0.33127984  0.66872013  1.          0.        ]]\n",
      "Minibatch total loss at step 2743: 0.375080\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.1219088   0.87809122  0.          1.        ]\n",
      " [ 0.02783422  0.97216582  0.          1.        ]]\n",
      "Minibatch total loss at step 2756: 0.334403\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.67088157  0.32911846  0.          1.        ]\n",
      " [ 0.66394681  0.33605319  0.          1.        ]]\n",
      "Minibatch total loss at step 2769: 0.432624\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.4676865   0.53231359  0.          1.        ]\n",
      " [ 0.19466153  0.80533844  0.          1.        ]]\n",
      "Minibatch total loss at step 2782: 0.409485\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.03696844  0.96303159  0.          1.        ]\n",
      " [ 0.98253089  0.01746915  1.          0.        ]]\n",
      "Minibatch total loss at step 2795: 0.495251\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.02183311  0.97816694  0.          1.        ]\n",
      " [ 0.43964782  0.56035215  1.          0.        ]]\n",
      "Validation accuracy: 56.0\n",
      "Minibatch total loss at step 2808: 0.382796\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.62544513  0.37455484  0.          1.        ]\n",
      " [ 0.64409441  0.35590562  1.          0.        ]]\n",
      "Minibatch total loss at step 2821: 0.319210\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.36826202  0.63173801  0.          1.        ]\n",
      " [ 0.83250785  0.16749217  1.          0.        ]]\n",
      "Minibatch total loss at step 2834: 0.263432\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.90797484  0.09202519  1.          0.        ]\n",
      " [ 0.05725911  0.94274086  0.          1.        ]]\n",
      "Minibatch total loss at step 2847: 0.383458\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.33217204  0.66782796  0.          1.        ]\n",
      " [ 0.69688129  0.30311877  1.          0.        ]]\n",
      "Minibatch total loss at step 2860: 0.261573\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.22201358  0.77798647  0.          1.        ]\n",
      " [ 0.82181197  0.17818801  1.          0.        ]]\n",
      "Minibatch total loss at step 2873: 0.382179\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.86951172  0.13048828  1.          0.        ]\n",
      " [ 0.83264709  0.16735294  1.          0.        ]]\n",
      "Minibatch total loss at step 2886: 0.317341\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.89971548  0.10028454  1.          0.        ]\n",
      " [ 0.77362764  0.22637235  1.          0.        ]]\n",
      "Minibatch total loss at step 2899: 0.410000\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.56981742  0.43018261  0.          1.        ]\n",
      " [ 0.46671215  0.53328788  0.          1.        ]]\n",
      "Validation accuracy: 56.0\n",
      "Minibatch total loss at step 2912: 0.375142\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.02193403  0.97806603  0.          1.        ]\n",
      " [ 0.86628145  0.13371855  1.          0.        ]]\n",
      "Minibatch total loss at step 2925: 0.285945\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.31650558  0.68349445  0.          1.        ]\n",
      " [ 0.06599704  0.93400294  0.          1.        ]]\n",
      "Minibatch total loss at step 2938: 0.378713\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.02835787  0.97164208  0.          1.        ]\n",
      " [ 0.89074898  0.10925105  1.          0.        ]]\n",
      "Minibatch total loss at step 2951: 0.266995\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.31918797  0.68081206  0.          1.        ]\n",
      " [ 0.05499974  0.94500017  0.          1.        ]]\n",
      "Minibatch total loss at step 2964: 0.230572\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.06457058  0.93542939  0.          1.        ]\n",
      " [ 0.10333899  0.89666098  0.          1.        ]]\n",
      "Minibatch total loss at step 2977: 0.513982\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.7320255   0.26797447  1.          0.        ]\n",
      " [ 0.86444116  0.13555883  1.          0.        ]]\n",
      "Validation accuracy: 51.0\n",
      "Minibatch total loss at step 3003: 0.285952\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.16379084  0.83620918  0.          1.        ]\n",
      " [ 0.40795067  0.5920493   1.          0.        ]]\n",
      "Minibatch total loss at step 3016: 0.310790\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.18559991  0.81440014  0.          1.        ]\n",
      " [ 0.83136851  0.16863148  1.          0.        ]]\n",
      "Minibatch total loss at step 3029: 0.340101\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.69082385  0.30917609  1.          0.        ]\n",
      " [ 0.31171718  0.68828285  0.          1.        ]]\n",
      "Minibatch total loss at step 3042: 0.328541\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.62619168  0.37380829  1.          0.        ]\n",
      " [ 0.91800654  0.08199351  1.          0.        ]]\n",
      "Minibatch total loss at step 3055: 0.255514\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.20517921  0.79482073  0.          1.        ]\n",
      " [ 0.91680557  0.08319438  1.          0.        ]]\n",
      "Minibatch total loss at step 3068: 0.242630\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.78085381  0.21914618  1.          0.        ]\n",
      " [ 0.90617323  0.09382673  1.          0.        ]]\n",
      "Minibatch total loss at step 3081: 0.220809\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.8955633   0.10443669  1.          0.        ]\n",
      " [ 0.14795528  0.85204464  0.          1.        ]]\n",
      "Minibatch total loss at step 3094: 0.190790\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.76868469  0.23131534  1.          0.        ]\n",
      " [ 0.08915783  0.91084218  0.          1.        ]]\n",
      "Validation accuracy: 50.5\n",
      "Minibatch total loss at step 3107: 0.299662\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.06485754  0.93514252  0.          1.        ]\n",
      " [ 0.47992986  0.52007014  0.          1.        ]]\n",
      "Minibatch total loss at step 3120: 0.323803\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.65914154  0.34085855  1.          0.        ]\n",
      " [ 0.27992424  0.72007573  0.          1.        ]]\n",
      "Minibatch total loss at step 3133: 0.467936\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.96952373  0.0304763   1.          0.        ]\n",
      " [ 0.04786531  0.95213467  0.          1.        ]]\n",
      "Minibatch total loss at step 3146: 0.233223\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.10492672  0.89507329  0.          1.        ]\n",
      " [ 0.19527535  0.80472463  0.          1.        ]]\n",
      "Minibatch total loss at step 3159: 0.393601\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.23973645  0.76026356  0.          1.        ]\n",
      " [ 0.44158918  0.55841082  1.          0.        ]]\n",
      "Minibatch total loss at step 3172: 0.219272\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.60667568  0.39332429  1.          0.        ]\n",
      " [ 0.97246933  0.02753063  1.          0.        ]]\n",
      "Minibatch total loss at step 3185: 0.215885\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.87963641  0.1203636   1.          0.        ]\n",
      " [ 0.91132993  0.08867005  1.          0.        ]]\n",
      "Minibatch total loss at step 3198: 0.509188\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.26099539  0.73900467  1.          0.        ]\n",
      " [ 0.03099184  0.96900821  0.          1.        ]]\n",
      "Validation accuracy: 54.5\n",
      "Minibatch total loss at step 3211: 0.390259\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.83765489  0.16234508  1.          0.        ]\n",
      " [ 0.77446699  0.22553304  1.          0.        ]]\n",
      "Minibatch total loss at step 3224: 0.339058\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.82487172  0.17512825  1.          0.        ]\n",
      " [ 0.0114419   0.98855805  0.          1.        ]]\n",
      "Minibatch total loss at step 3237: 0.321418\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.10317845  0.89682156  0.          1.        ]\n",
      " [ 0.06898193  0.93101811  0.          1.        ]]\n",
      "Minibatch total loss at step 3250: 0.232939\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.23954518  0.76045483  0.          1.        ]\n",
      " [ 0.97498089  0.02501905  1.          0.        ]]\n",
      "Minibatch total loss at step 3263: 0.296579\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.6716013   0.3283987   1.          0.        ]\n",
      " [ 0.68153596  0.31846395  0.          1.        ]]\n",
      "Minibatch total loss at step 3276: 0.402098\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.60507321  0.39492679  1.          0.        ]\n",
      " [ 0.1022668   0.89773327  0.          1.        ]]\n",
      "Minibatch total loss at step 3289: 0.188574\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.93507922  0.06492071  1.          0.        ]\n",
      " [ 0.27998236  0.72001761  0.          1.        ]]\n",
      "Validation accuracy: 53.0\n",
      "Minibatch total loss at step 3302: 0.206732\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.02087344  0.97912657  0.          1.        ]\n",
      " [ 0.06005049  0.93994957  0.          1.        ]]\n",
      "Minibatch total loss at step 3315: 0.283061\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.01243963  0.98756033  0.          1.        ]\n",
      " [ 0.65121412  0.34878594  1.          0.        ]]\n",
      "Minibatch total loss at step 3328: 0.388497\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.20071405  0.79928589  0.          1.        ]\n",
      " [ 0.59858596  0.40141407  0.          1.        ]]\n",
      "Minibatch total loss at step 3341: 0.257282\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.77040225  0.22959781  1.          0.        ]\n",
      " [ 0.10469086  0.89530915  0.          1.        ]]\n",
      "Minibatch total loss at step 3354: 0.287816\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.07252741  0.92747253  0.          1.        ]\n",
      " [ 0.93634897  0.06365101  1.          0.        ]]\n",
      "Minibatch total loss at step 3367: 0.227047\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.95130098  0.04869909  1.          0.        ]\n",
      " [ 0.1501565   0.8498435   0.          1.        ]]\n",
      "Minibatch total loss at step 3380: 0.228749\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.37431023  0.6256898   0.          1.        ]\n",
      " [ 0.82191294  0.17808706  1.          0.        ]]\n",
      "Minibatch total loss at step 3393: 0.287719\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.67939627  0.32060364  1.          0.        ]\n",
      " [ 0.10419057  0.89580935  0.          1.        ]]\n",
      "Validation accuracy: 51.0\n",
      "Minibatch total loss at step 3406: 0.325636\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.15666509  0.84333485  0.          1.        ]\n",
      " [ 0.36758789  0.63241208  0.          1.        ]]\n",
      "Minibatch total loss at step 3419: 0.343443\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.95847654  0.04152345  1.          0.        ]\n",
      " [ 0.05619536  0.94380462  0.          1.        ]]\n",
      "Minibatch total loss at step 3432: 0.342179\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.68618882  0.31381121  1.          0.        ]\n",
      " [ 0.82733798  0.172662    1.          0.        ]]\n",
      "Minibatch total loss at step 3445: 0.458376\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.6889503   0.31104967  0.          1.        ]\n",
      " [ 0.26430091  0.73569912  0.          1.        ]]\n",
      "Minibatch total loss at step 3458: 0.291877\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.12293974  0.87706029  0.          1.        ]\n",
      " [ 0.84969938  0.15030055  1.          0.        ]]\n",
      "Minibatch total loss at step 3471: 0.248583\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.10306557  0.89693445  0.          1.        ]\n",
      " [ 0.86668986  0.13331015  1.          0.        ]]\n",
      "Minibatch total loss at step 3484: 0.377313\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.69096172  0.30903834  1.          0.        ]\n",
      " [ 0.532628    0.467372    1.          0.        ]]\n",
      "Minibatch total loss at step 3497: 0.443900\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.2259008   0.77409917  1.          0.        ]\n",
      " [ 0.01374299  0.98625702  0.          1.        ]]\n",
      "Validation accuracy: 57.0\n",
      "Minibatch total loss at step 3510: 0.284177\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.83818632  0.16181371  1.          0.        ]\n",
      " [ 0.12255105  0.87744892  0.          1.        ]]\n",
      "Minibatch total loss at step 3523: 0.563011\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.00506013  0.99493992  0.          1.        ]\n",
      " [ 0.65178919  0.34821072  1.          0.        ]]\n",
      "Minibatch total loss at step 3536: 0.231634\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.1209037   0.87909633  0.          1.        ]\n",
      " [ 0.08694518  0.91305482  0.          1.        ]]\n",
      "Minibatch total loss at step 3549: 0.224537\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.03305839  0.9669416   0.          1.        ]\n",
      " [ 0.38876522  0.61123478  0.          1.        ]]\n",
      "Minibatch total loss at step 3562: 0.212129\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.11503206  0.88496786  0.          1.        ]\n",
      " [ 0.58895761  0.41104242  1.          0.        ]]\n",
      "Minibatch total loss at step 3575: 0.275133\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.11990848  0.88009149  0.          1.        ]\n",
      " [ 0.55564409  0.44435588  0.          1.        ]]\n",
      "Minibatch total loss at step 3588: 0.278196\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.94967526  0.05032472  1.          0.        ]\n",
      " [ 0.64943153  0.35056847  1.          0.        ]]\n",
      "Validation accuracy: 55.0\n",
      "Minibatch total loss at step 3601: 0.356642\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.75376606  0.24623397  1.          0.        ]\n",
      " [ 0.3052119   0.6947881   0.          1.        ]]\n",
      "Minibatch total loss at step 3614: 0.303562\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.82412422  0.17587575  1.          0.        ]\n",
      " [ 0.90759045  0.09240958  1.          0.        ]]\n",
      "Minibatch total loss at step 3627: 0.349862\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.85276133  0.14723869  1.          0.        ]\n",
      " [ 0.96222973  0.03777024  1.          0.        ]]\n",
      "Minibatch total loss at step 3640: 0.284283\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.62590456  0.37409541  0.          1.        ]\n",
      " [ 0.86014122  0.1398588   1.          0.        ]]\n",
      "Minibatch total loss at step 3653: 0.326828\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.14829268  0.85170728  0.          1.        ]\n",
      " [ 0.26116681  0.73883313  0.          1.        ]]\n",
      "Minibatch total loss at step 3666: 0.291260\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.73037994  0.26962006  1.          0.        ]\n",
      " [ 0.01391032  0.98608971  0.          1.        ]]\n",
      "Minibatch total loss at step 3679: 0.277564\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.50840306  0.49159697  0.          1.        ]\n",
      " [ 0.86145538  0.13854457  1.          0.        ]]\n",
      "Minibatch total loss at step 3692: 0.318314\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.82143474  0.17856528  1.          0.        ]\n",
      " [ 0.45780104  0.54219896  1.          0.        ]]\n",
      "Validation accuracy: 53.5\n",
      "Minibatch total loss at step 3705: 0.224248\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.29158866  0.70841134  0.          1.        ]\n",
      " [ 0.37596065  0.62403935  0.          1.        ]]\n",
      "Minibatch total loss at step 3718: 0.252436\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.54761386  0.45238611  1.          0.        ]\n",
      " [ 0.31268382  0.68731618  0.          1.        ]]\n",
      "Minibatch total loss at step 3731: 0.188637\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.76534653  0.23465353  1.          0.        ]\n",
      " [ 0.74631727  0.25368276  1.          0.        ]]\n",
      "Minibatch total loss at step 3744: 0.219428\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.57585782  0.42414227  1.          0.        ]\n",
      " [ 0.04638371  0.95361632  0.          1.        ]]\n",
      "Minibatch total loss at step 3757: 0.341506\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.39549699  0.60450304  0.          1.        ]\n",
      " [ 0.79968297  0.20031703  1.          0.        ]]\n",
      "Minibatch total loss at step 3770: 0.221537\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.93748194  0.06251805  1.          0.        ]\n",
      " [ 0.77681345  0.22318658  1.          0.        ]]\n",
      "Minibatch total loss at step 3783: 0.262366\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.87634259  0.12365741  1.          0.        ]\n",
      " [ 0.00896991  0.9910301   0.          1.        ]]\n",
      "Minibatch total loss at step 3796: 0.225851\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.88550436  0.11449566  1.          0.        ]\n",
      " [ 0.27675322  0.72324675  0.          1.        ]]\n",
      "Validation accuracy: 51.0\n",
      "Minibatch total loss at step 3809: 0.233019\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.0535177   0.94648224  0.          1.        ]\n",
      " [ 0.85429907  0.1457009   1.          0.        ]]\n",
      "Minibatch total loss at step 3822: 0.407196\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.96716702  0.03283297  1.          0.        ]\n",
      " [ 0.02249778  0.97750217  0.          1.        ]]\n",
      "Minibatch total loss at step 3835: 0.397407\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.33346358  0.66653645  1.          0.        ]\n",
      " [ 0.06601813  0.9339819   0.          1.        ]]\n",
      "Minibatch total loss at step 3848: 0.227398\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.14817631  0.85182369  0.          1.        ]\n",
      " [ 0.06066402  0.93933594  0.          1.        ]]\n",
      "Minibatch total loss at step 3861: 0.319508\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.03193006  0.96806997  0.          1.        ]\n",
      " [ 0.53207046  0.46792951  1.          0.        ]]\n",
      "Minibatch total loss at step 3874: 0.420414\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.36184067  0.63815933  0.          1.        ]\n",
      " [ 0.71856803  0.28143191  0.          1.        ]]\n",
      "Minibatch total loss at step 3887: 0.338229\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.00780627  0.9921937   0.          1.        ]\n",
      " [ 0.43884298  0.56115705  1.          0.        ]]\n",
      "Minibatch total loss at step 3900: 0.411153\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.07669191  0.92330813  0.          1.        ]\n",
      " [ 0.07592596  0.92407399  0.          1.        ]]\n",
      "Validation accuracy: 53.5\n",
      "Minibatch total loss at step 3913: 0.241517\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.14775503  0.85224497  0.          1.        ]\n",
      " [ 0.66436887  0.33563116  1.          0.        ]]\n",
      "Minibatch total loss at step 3926: 0.206883\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.84491205  0.1550879   1.          0.        ]\n",
      " [ 0.95698619  0.04301377  1.          0.        ]]\n",
      "Minibatch total loss at step 3939: 0.202292\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.13699856  0.86300147  0.          1.        ]\n",
      " [ 0.65279508  0.34720492  1.          0.        ]]\n",
      "Minibatch total loss at step 3952: 0.236419\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.06599453  0.9340055   0.          1.        ]\n",
      " [ 0.80207461  0.19792537  1.          0.        ]]\n",
      "Minibatch total loss at step 3965: 0.213712\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.20110042  0.79889953  0.          1.        ]\n",
      " [ 0.76112717  0.23887281  1.          0.        ]]\n",
      "Minibatch total loss at step 3978: 0.336497\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.80100679  0.19899313  1.          0.        ]\n",
      " [ 0.98041403  0.01958599  1.          0.        ]]\n",
      "Minibatch total loss at step 3991: 0.247837\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.89952368  0.10047633  1.          0.        ]\n",
      " [ 0.14339294  0.85660702  0.          1.        ]]\n",
      "Validation accuracy: 53.0\n",
      "Minibatch total loss at step 4004: 0.301615\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.71627134  0.28372869  1.          0.        ]\n",
      " [ 0.48553923  0.51446086  0.          1.        ]]\n",
      "Minibatch total loss at step 4017: 0.211946\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.26546213  0.7345379   0.          1.        ]\n",
      " [ 0.0968091   0.90319091  0.          1.        ]]\n",
      "Minibatch total loss at step 4030: 0.225004\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.12690102  0.87309903  0.          1.        ]\n",
      " [ 0.05226381  0.94773614  0.          1.        ]]\n",
      "Minibatch total loss at step 4043: 0.196106\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.80289376  0.19710629  1.          0.        ]\n",
      " [ 0.27329403  0.72670597  0.          1.        ]]\n",
      "Minibatch total loss at step 4056: 0.258908\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.65169948  0.34830049  1.          0.        ]\n",
      " [ 0.75781244  0.24218756  1.          0.        ]]\n",
      "Minibatch total loss at step 4069: 0.245954\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.14843452  0.85156548  0.          1.        ]\n",
      " [ 0.72282648  0.27717352  1.          0.        ]]\n",
      "Minibatch total loss at step 4082: 0.243242\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.12753339  0.87246662  0.          1.        ]\n",
      " [ 0.17549691  0.82450306  0.          1.        ]]\n",
      "Minibatch total loss at step 4095: 0.220822\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.03354863  0.96645129  0.          1.        ]\n",
      " [ 0.8465783   0.15342171  1.          0.        ]]\n",
      "Validation accuracy: 54.5\n",
      "Minibatch total loss at step 4108: 0.367660\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.85871834  0.14128168  0.          1.        ]\n",
      " [ 0.33212781  0.66787219  0.          1.        ]]\n",
      "Minibatch total loss at step 4121: 0.299040\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.0330058   0.96699423  0.          1.        ]\n",
      " [ 0.71296865  0.28703138  1.          0.        ]]\n",
      "Minibatch total loss at step 4134: 0.295831\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.57109511  0.42890483  1.          0.        ]\n",
      " [ 0.4738498   0.52615017  1.          0.        ]]\n",
      "Minibatch total loss at step 4147: 0.271072\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.49435914  0.50564086  1.          0.        ]\n",
      " [ 0.82113779  0.1788622   1.          0.        ]]\n",
      "Minibatch total loss at step 4160: 0.259660\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.20636764  0.79363233  0.          1.        ]\n",
      " [ 0.09774933  0.90225065  0.          1.        ]]\n",
      "Minibatch total loss at step 4173: 0.314035\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.63684118  0.36315882  1.          0.        ]\n",
      " [ 0.96611494  0.03388501  1.          0.        ]]\n",
      "Minibatch total loss at step 4186: 0.229369\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.89287198  0.10712808  1.          0.        ]\n",
      " [ 0.20203553  0.79796445  0.          1.        ]]\n",
      "Minibatch total loss at step 4199: 0.235536\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.12823536  0.87176466  0.          1.        ]\n",
      " [ 0.26624417  0.73375583  0.          1.        ]]\n",
      "Validation accuracy: 47.5\n",
      "Minibatch total loss at step 4212: 0.296462\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.92146629  0.07853368  1.          0.        ]\n",
      " [ 0.96021819  0.03978183  1.          0.        ]]\n",
      "Minibatch total loss at step 4225: 0.230099\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.96548605  0.03451394  1.          0.        ]\n",
      " [ 0.15766157  0.8423385   0.          1.        ]]\n",
      "Minibatch total loss at step 4238: 0.330347\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.33457658  0.66542339  0.          1.        ]\n",
      " [ 0.16915275  0.83084726  0.          1.        ]]\n",
      "Minibatch total loss at step 4251: 0.210709\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.12096767  0.87903231  0.          1.        ]\n",
      " [ 0.33328235  0.66671765  0.          1.        ]]\n",
      "Minibatch total loss at step 4264: 0.359115\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.09033827  0.90966177  0.          1.        ]\n",
      " [ 0.98897004  0.01102994  1.          0.        ]]\n",
      "Minibatch total loss at step 4277: 0.224455\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.8381142   0.1618858   1.          0.        ]\n",
      " [ 0.93851829  0.06148164  1.          0.        ]]\n",
      "Minibatch total loss at step 4290: 0.260832\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.84523559  0.15476446  1.          0.        ]\n",
      " [ 0.23752171  0.76247829  0.          1.        ]]\n",
      "Validation accuracy: 51.5\n",
      "Minibatch total loss at step 4303: 0.252191\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.66824019  0.33175987  1.          0.        ]\n",
      " [ 0.52206838  0.47793168  1.          0.        ]]\n",
      "Minibatch total loss at step 4316: 0.246271\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.65748483  0.34251526  1.          0.        ]\n",
      " [ 0.02645628  0.97354376  0.          1.        ]]\n",
      "Minibatch total loss at step 4329: 0.244683\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.1144769   0.88552308  0.          1.        ]\n",
      " [ 0.15507053  0.8449294   0.          1.        ]]\n",
      "Minibatch total loss at step 4342: 0.180751\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.1133182   0.88668174  0.          1.        ]\n",
      " [ 0.96918386  0.03081617  1.          0.        ]]\n",
      "Minibatch total loss at step 4355: 0.224187\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.02032651  0.9796735   0.          1.        ]\n",
      " [ 0.24249649  0.75750357  0.          1.        ]]\n",
      "Minibatch total loss at step 4368: 0.304333\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.73130882  0.26869118  0.          1.        ]\n",
      " [ 0.06026025  0.9397397   0.          1.        ]]\n",
      "Minibatch total loss at step 4381: 0.187359\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.88726991  0.1127301   1.          0.        ]\n",
      " [ 0.13944089  0.86055911  0.          1.        ]]\n",
      "Minibatch total loss at step 4394: 0.444260\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.80438209  0.19561794  1.          0.        ]\n",
      " [ 0.15325391  0.84674615  0.          1.        ]]\n",
      "Validation accuracy: 52.0\n",
      "Minibatch total loss at step 4407: 0.282403\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.75758117  0.2424188   1.          0.        ]\n",
      " [ 0.32819796  0.67180204  0.          1.        ]]\n",
      "Minibatch total loss at step 4420: 0.238533\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.64978635  0.35021368  1.          0.        ]\n",
      " [ 0.42665932  0.57334071  1.          0.        ]]\n",
      "Minibatch total loss at step 4433: 0.246533\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.84546065  0.15453936  1.          0.        ]\n",
      " [ 0.30438712  0.69561291  0.          1.        ]]\n",
      "Minibatch total loss at step 4446: 0.211172\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.13791361  0.86208636  0.          1.        ]\n",
      " [ 0.34496972  0.65503031  1.          0.        ]]\n",
      "Minibatch total loss at step 4459: 0.218856\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.92077196  0.07922801  1.          0.        ]\n",
      " [ 0.17624851  0.82375157  0.          1.        ]]\n",
      "Minibatch total loss at step 4472: 0.307627\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.37122744  0.62877256  1.          0.        ]\n",
      " [ 0.14291379  0.85708624  0.          1.        ]]\n",
      "Minibatch total loss at step 4485: 0.283696\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.93959886  0.06040111  1.          0.        ]\n",
      " [ 0.59054196  0.40945807  0.          1.        ]]\n",
      "Minibatch total loss at step 4498: 0.269161\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.00836235  0.99163765  0.          1.        ]\n",
      " [ 0.95479125  0.0452087   1.          0.        ]]\n",
      "Validation accuracy: 45.0\n",
      "Minibatch total loss at step 4511: 0.260104\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.21484694  0.78515303  0.          1.        ]\n",
      " [ 0.9043715   0.09562849  1.          0.        ]]\n",
      "Minibatch total loss at step 4524: 0.226410\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.09042644  0.90957361  0.          1.        ]\n",
      " [ 0.73491889  0.26508108  1.          0.        ]]\n",
      "Minibatch total loss at step 4537: 0.192299\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.92563319  0.0743668   1.          0.        ]\n",
      " [ 0.05791331  0.94208664  0.          1.        ]]\n",
      "Minibatch total loss at step 4550: 0.302989\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.17216662  0.82783341  0.          1.        ]\n",
      " [ 0.01804153  0.98195845  0.          1.        ]]\n",
      "Minibatch total loss at step 4563: 0.256162\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.83850342  0.16149661  1.          0.        ]\n",
      " [ 0.67001605  0.32998395  1.          0.        ]]\n",
      "Minibatch total loss at step 4576: 0.294883\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5397284   0.4602716   1.          0.        ]\n",
      " [ 0.92895311  0.07104692  1.          0.        ]]\n",
      "Minibatch total loss at step 4589: 0.254193\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.20236036  0.79763967  0.          1.        ]\n",
      " [ 0.20548944  0.7945106   0.          1.        ]]\n",
      "Validation accuracy: 50.5\n",
      "Minibatch total loss at step 4602: 0.212554\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.21148638  0.7885136   0.          1.        ]\n",
      " [ 0.94925475  0.05074519  1.          0.        ]]\n",
      "Minibatch total loss at step 4615: 0.274380\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.14616737  0.85383266  0.          1.        ]\n",
      " [ 0.69554836  0.30445164  1.          0.        ]]\n",
      "Minibatch total loss at step 4628: 0.318211\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.98421073  0.01578926  1.          0.        ]\n",
      " [ 0.02071355  0.97928649  0.          1.        ]]\n",
      "Minibatch total loss at step 4641: 0.273647\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.82755589  0.17244406  1.          0.        ]\n",
      " [ 0.51278937  0.48721072  0.          1.        ]]\n",
      "Minibatch total loss at step 4654: 0.222058\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.74557412  0.25442591  1.          0.        ]\n",
      " [ 0.03366622  0.96633375  0.          1.        ]]\n",
      "Minibatch total loss at step 4667: 0.211594\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.79344511  0.2065549   1.          0.        ]\n",
      " [ 0.15922788  0.84077215  0.          1.        ]]\n",
      "Minibatch total loss at step 4680: 0.258348\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.03186383  0.96813625  0.          1.        ]\n",
      " [ 0.25303265  0.74696732  0.          1.        ]]\n",
      "Minibatch total loss at step 4693: 0.242250\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.9019872   0.09801283  1.          0.        ]\n",
      " [ 0.05793113  0.94206887  0.          1.        ]]\n",
      "Validation accuracy: 55.0\n",
      "Minibatch total loss at step 4706: 0.228073\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.92955899  0.07044106  1.          0.        ]\n",
      " [ 0.429885    0.57011503  1.          0.        ]]\n",
      "Minibatch total loss at step 4719: 0.253122\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.78726667  0.2127334   1.          0.        ]\n",
      " [ 0.01838119  0.98161882  0.          1.        ]]\n",
      "Minibatch total loss at step 4732: 0.262529\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.85159546  0.14840454  1.          0.        ]\n",
      " [ 0.26858699  0.73141301  0.          1.        ]]\n",
      "Minibatch total loss at step 4745: 0.272708\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.13268864  0.86731142  0.          1.        ]\n",
      " [ 0.29116091  0.70883906  0.          1.        ]]\n",
      "Minibatch total loss at step 4758: 0.318850\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.62420064  0.37579936  1.          0.        ]\n",
      " [ 0.03041703  0.96958297  0.          1.        ]]\n",
      "Minibatch total loss at step 4771: 0.227391\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.0386494   0.96135062  0.          1.        ]\n",
      " [ 0.18391754  0.81608248  0.          1.        ]]\n",
      "Minibatch total loss at step 4784: 0.293012\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.77423173  0.2257683   1.          0.        ]\n",
      " [ 0.18644409  0.81355596  0.          1.        ]]\n",
      "Minibatch total loss at step 4797: 0.202027\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.04698023  0.9530198   0.          1.        ]\n",
      " [ 0.44727153  0.55272853  0.          1.        ]]\n",
      "Validation accuracy: 46.5\n",
      "Minibatch total loss at step 4810: 0.211535\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.02243877  0.97756118  0.          1.        ]\n",
      " [ 0.19428976  0.80571026  0.          1.        ]]\n",
      "Minibatch total loss at step 4823: 0.327718\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.74401271  0.25598729  1.          0.        ]\n",
      " [ 0.39295214  0.60704786  1.          0.        ]]\n",
      "Minibatch total loss at step 4836: 0.236681\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.83203357  0.16796649  1.          0.        ]\n",
      " [ 0.92242342  0.07757664  1.          0.        ]]\n",
      "Minibatch total loss at step 4849: 0.209107\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.83279628  0.16720372  1.          0.        ]\n",
      " [ 0.1232476   0.87675238  0.          1.        ]]\n",
      "Minibatch total loss at step 4862: 0.232319\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.67433637  0.32566363  1.          0.        ]\n",
      " [ 0.81109726  0.18890269  1.          0.        ]]\n",
      "Minibatch total loss at step 4875: 0.169583\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.26085803  0.739142    0.          1.        ]\n",
      " [ 0.09473313  0.90526682  0.          1.        ]]\n",
      "Minibatch total loss at step 4888: 0.233485\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.36030889  0.63969111  1.          0.        ]\n",
      " [ 0.02141754  0.97858238  0.          1.        ]]\n",
      "Validation accuracy: 50.5\n",
      "Minibatch total loss at step 4901: 0.203725\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.91797221  0.08202784  1.          0.        ]\n",
      " [ 0.12879518  0.87120479  0.          1.        ]]\n",
      "Minibatch total loss at step 4914: 0.157162\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.81501389  0.1849861   1.          0.        ]\n",
      " [ 0.19581264  0.80418736  0.          1.        ]]\n",
      "Minibatch total loss at step 4927: 0.216808\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.17316803  0.82683194  0.          1.        ]\n",
      " [ 0.38680807  0.61319196  0.          1.        ]]\n",
      "Minibatch total loss at step 4940: 0.152832\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.16295432  0.83704567  0.          1.        ]\n",
      " [ 0.24096352  0.75903642  0.          1.        ]]\n",
      "Minibatch total loss at step 4953: 0.230791\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.26000404  0.73999596  0.          1.        ]\n",
      " [ 0.8460235   0.15397649  1.          0.        ]]\n",
      "Minibatch total loss at step 4966: 0.167593\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.13208354  0.86791646  0.          1.        ]\n",
      " [ 0.27942154  0.72057849  0.          1.        ]]\n",
      "Minibatch total loss at step 4979: 0.283515\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.05988835  0.9401117   0.          1.        ]\n",
      " [ 0.79491633  0.20508371  0.          1.        ]]\n",
      "Minibatch total loss at step 4992: 0.201177\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.15454179  0.84545827  0.          1.        ]\n",
      " [ 0.04759308  0.95240694  0.          1.        ]]\n",
      "Validation accuracy: 55.0\n",
      "Minibatch total loss at step 5005: 0.280304\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5333426   0.46665734  1.          0.        ]\n",
      " [ 0.20476496  0.79523498  0.          1.        ]]\n",
      "Minibatch total loss at step 5018: 0.152810\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.92560202  0.07439794  1.          0.        ]\n",
      " [ 0.91237485  0.08762512  1.          0.        ]]\n",
      "Minibatch total loss at step 5031: 0.250187\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.63464481  0.36535525  1.          0.        ]\n",
      " [ 0.70129043  0.2987096   1.          0.        ]]\n",
      "Minibatch total loss at step 5044: 0.217107\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.02998946  0.97001046  0.          1.        ]\n",
      " [ 0.8696745   0.1303255   1.          0.        ]]\n",
      "Minibatch total loss at step 5057: 0.302762\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.73782277  0.2621772   1.          0.        ]\n",
      " [ 0.44229877  0.55770123  1.          0.        ]]\n",
      "Minibatch total loss at step 5070: 0.318582\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.01246824  0.98753172  0.          1.        ]\n",
      " [ 0.17534569  0.82465434  0.          1.        ]]\n",
      "Minibatch total loss at step 5083: 0.220532\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.03748856  0.96251148  0.          1.        ]\n",
      " [ 0.01869474  0.9813053   0.          1.        ]]\n",
      "Minibatch total loss at step 5096: 0.149216\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.94985121  0.05014883  1.          0.        ]\n",
      " [ 0.83991307  0.16008693  1.          0.        ]]\n",
      "Validation accuracy: 54.5\n",
      "Minibatch total loss at step 5109: 0.201734\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5535664   0.44643363  0.          1.        ]\n",
      " [ 0.13117333  0.86882663  0.          1.        ]]\n",
      "Minibatch total loss at step 5122: 0.265299\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.1018781   0.89812195  0.          1.        ]\n",
      " [ 0.70185077  0.29814926  1.          0.        ]]\n",
      "Minibatch total loss at step 5135: 0.325433\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.14853069  0.85146934  0.          1.        ]\n",
      " [ 0.01344977  0.98655021  0.          1.        ]]\n",
      "Minibatch total loss at step 5148: 0.184567\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.04505609  0.9549439   0.          1.        ]\n",
      " [ 0.61552739  0.38447264  1.          0.        ]]\n",
      "Minibatch total loss at step 5161: 0.243027\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.53529817  0.4647018   1.          0.        ]\n",
      " [ 0.75746047  0.2425395   1.          0.        ]]\n",
      "Minibatch total loss at step 5174: 0.234777\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.2462724   0.75372756  0.          1.        ]\n",
      " [ 0.79398566  0.20601429  1.          0.        ]]\n",
      "Minibatch total loss at step 5187: 0.219070\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.21338245  0.78661752  0.          1.        ]\n",
      " [ 0.18466812  0.81533188  0.          1.        ]]\n",
      "Minibatch total loss at step 5200: 0.241870\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.95785636  0.04214368  1.          0.        ]\n",
      " [ 0.90767658  0.09232343  1.          0.        ]]\n",
      "Validation accuracy: 51.5\n",
      "Minibatch total loss at step 5213: 0.206966\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.12242649  0.87757355  0.          1.        ]\n",
      " [ 0.74674088  0.25325912  1.          0.        ]]\n",
      "Minibatch total loss at step 5226: 0.211849\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.80736291  0.19263709  1.          0.        ]\n",
      " [ 0.10048436  0.89951557  0.          1.        ]]\n",
      "Minibatch total loss at step 5239: 0.300717\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.09870791  0.90129203  0.          1.        ]\n",
      " [ 0.05046249  0.94953758  0.          1.        ]]\n",
      "Minibatch total loss at step 5252: 0.208161\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.29373476  0.70626521  0.          1.        ]\n",
      " [ 0.12697081  0.87302923  0.          1.        ]]\n",
      "Minibatch total loss at step 5265: 0.205107\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.06650294  0.93349707  0.          1.        ]\n",
      " [ 0.2463882   0.7536118   0.          1.        ]]\n",
      "Minibatch total loss at step 5278: 0.248888\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.96800542  0.03199461  1.          0.        ]\n",
      " [ 0.50067222  0.49932778  0.          1.        ]]\n",
      "Minibatch total loss at step 5291: 0.212682\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.14385782  0.85614222  0.          1.        ]\n",
      " [ 0.07115877  0.92884117  0.          1.        ]]\n",
      "Validation accuracy: 54.0\n",
      "Minibatch total loss at step 5304: 0.226064\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.78070897  0.219291    1.          0.        ]\n",
      " [ 0.25812113  0.74187887  0.          1.        ]]\n",
      "Minibatch total loss at step 5317: 0.356213\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.63617575  0.36382428  1.          0.        ]\n",
      " [ 0.06616376  0.93383622  0.          1.        ]]\n",
      "Minibatch total loss at step 5330: 0.294751\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.73744786  0.26255211  1.          0.        ]\n",
      " [ 0.7329486   0.26705137  1.          0.        ]]\n",
      "Minibatch total loss at step 5343: 0.248001\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.04337035  0.95662963  0.          1.        ]\n",
      " [ 0.77348614  0.22651383  1.          0.        ]]\n",
      "Minibatch total loss at step 5356: 0.177549\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.06242201  0.93757802  0.          1.        ]\n",
      " [ 0.14048405  0.85951602  0.          1.        ]]\n",
      "Minibatch total loss at step 5369: 0.496376\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.69054729  0.30945274  1.          0.        ]\n",
      " [ 0.44083196  0.55916804  1.          0.        ]]\n",
      "Minibatch total loss at step 5382: 0.211545\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.17346336  0.82653666  0.          1.        ]\n",
      " [ 0.86819351  0.13180648  1.          0.        ]]\n",
      "Minibatch total loss at step 5395: 0.201339\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.03648527  0.96351475  0.          1.        ]\n",
      " [ 0.12117739  0.87882262  0.          1.        ]]\n",
      "Validation accuracy: 53.0\n",
      "Minibatch total loss at step 5408: 0.189947\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.65920633  0.3407937   1.          0.        ]\n",
      " [ 0.96695238  0.03304756  1.          0.        ]]\n",
      "Minibatch total loss at step 5421: 0.201797\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.19529185  0.80470818  0.          1.        ]\n",
      " [ 0.86095834  0.13904169  1.          0.        ]]\n",
      "Minibatch total loss at step 5434: 0.229437\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5368858   0.4631142   1.          0.        ]\n",
      " [ 0.77216291  0.22783707  1.          0.        ]]\n",
      "Minibatch total loss at step 5447: 0.318560\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.49523649  0.50476354  0.          1.        ]\n",
      " [ 0.78559893  0.21440111  1.          0.        ]]\n",
      "Minibatch total loss at step 5460: 0.218382\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.9461838   0.05381626  1.          0.        ]\n",
      " [ 0.09707293  0.9029271   0.          1.        ]]\n",
      "Minibatch total loss at step 5473: 0.221173\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.9532246   0.04677546  1.          0.        ]\n",
      " [ 0.25878644  0.74121356  0.          1.        ]]\n",
      "Minibatch total loss at step 5486: 0.202330\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.87090296  0.129097    1.          0.        ]\n",
      " [ 0.46407866  0.53592134  1.          0.        ]]\n",
      "Minibatch total loss at step 5499: 0.208825\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.18268326  0.81731671  0.          1.        ]\n",
      " [ 0.68142891  0.31857103  1.          0.        ]]\n",
      "Validation accuracy: 53.0\n",
      "Minibatch total loss at step 5512: 0.271310\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.02772146  0.97227854  0.          1.        ]\n",
      " [ 0.93290478  0.06709523  1.          0.        ]]\n",
      "Minibatch total loss at step 5525: 0.197075\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.90061945  0.09938062  1.          0.        ]\n",
      " [ 0.83161873  0.16838132  1.          0.        ]]\n",
      "Minibatch total loss at step 5538: 0.275731\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.54236764  0.45763242  1.          0.        ]\n",
      " [ 0.10634986  0.89365011  0.          1.        ]]\n",
      "Minibatch total loss at step 5551: 0.202232\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.24210943  0.75789058  0.          1.        ]\n",
      " [ 0.01325862  0.98674136  0.          1.        ]]\n",
      "Minibatch total loss at step 5564: 0.171762\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.17675215  0.82324791  0.          1.        ]\n",
      " [ 0.13116643  0.86883354  0.          1.        ]]\n",
      "Minibatch total loss at step 5577: 0.176188\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.68069041  0.31930965  1.          0.        ]\n",
      " [ 0.97903126  0.02096878  1.          0.        ]]\n",
      "Minibatch total loss at step 5590: 0.212330\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.17017093  0.8298291   0.          1.        ]\n",
      " [ 0.12737656  0.8726235   0.          1.        ]]\n",
      "Validation accuracy: 52.0\n",
      "Minibatch total loss at step 5603: 0.175303\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.81000078  0.18999921  1.          0.        ]\n",
      " [ 0.18385635  0.81614363  0.          1.        ]]\n",
      "Minibatch total loss at step 5616: 0.154227\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.90412295  0.09587706  1.          0.        ]\n",
      " [ 0.95085239  0.04914756  1.          0.        ]]\n",
      "Minibatch total loss at step 5629: 0.194332\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.01524785  0.98475212  0.          1.        ]\n",
      " [ 0.07539452  0.92460549  0.          1.        ]]\n",
      "Minibatch total loss at step 5642: 0.152930\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.14390242  0.85609758  0.          1.        ]\n",
      " [ 0.87237066  0.12762931  1.          0.        ]]\n",
      "Minibatch total loss at step 5655: 0.208857\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.80835313  0.19164684  1.          0.        ]\n",
      " [ 0.86893481  0.13106516  1.          0.        ]]\n",
      "Minibatch total loss at step 5668: 0.308669\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.22877984  0.77122015  0.          1.        ]\n",
      " [ 0.38322657  0.61677343  0.          1.        ]]\n",
      "Minibatch total loss at step 5681: 0.267387\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.0394394   0.96056056  0.          1.        ]\n",
      " [ 0.81634104  0.18365899  1.          0.        ]]\n",
      "Minibatch total loss at step 5694: 0.152884\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.15138914  0.84861082  0.          1.        ]\n",
      " [ 0.85148913  0.14851083  1.          0.        ]]\n",
      "Validation accuracy: 52.0\n",
      "Minibatch total loss at step 5707: 0.224592\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.40750158  0.59249842  1.          0.        ]\n",
      " [ 0.65990061  0.3400993   1.          0.        ]]\n",
      "Minibatch total loss at step 5720: 0.262169\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.56988746  0.43011257  0.          1.        ]\n",
      " [ 0.26837909  0.73162091  0.          1.        ]]\n",
      "Minibatch total loss at step 5733: 0.335646\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.78124475  0.21875526  1.          0.        ]\n",
      " [ 0.76333594  0.23666401  1.          0.        ]]\n",
      "Minibatch total loss at step 5746: 0.212072\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.08111833  0.91888171  0.          1.        ]\n",
      " [ 0.38457939  0.61542064  0.          1.        ]]\n",
      "Minibatch total loss at step 5759: 0.178947\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.82522756  0.17477241  1.          0.        ]\n",
      " [ 0.17460752  0.82539248  0.          1.        ]]\n",
      "Minibatch total loss at step 5772: 0.274104\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.85171634  0.14828362  1.          0.        ]\n",
      " [ 0.48044407  0.51955587  1.          0.        ]]\n",
      "Minibatch total loss at step 5785: 0.206351\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.89534259  0.10465746  1.          0.        ]\n",
      " [ 0.09465624  0.90534371  0.          1.        ]]\n",
      "Minibatch total loss at step 5798: 0.238252\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.6783461   0.3216539   1.          0.        ]\n",
      " [ 0.36335212  0.63664794  0.          1.        ]]\n",
      "Validation accuracy: 53.0\n",
      "Minibatch total loss at step 5811: 0.156245\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.85076404  0.14923596  1.          0.        ]\n",
      " [ 0.94560707  0.05439296  1.          0.        ]]\n",
      "Minibatch total loss at step 5824: 0.279164\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.95558697  0.04441296  1.          0.        ]\n",
      " [ 0.57474113  0.4252589   0.          1.        ]]\n",
      "Minibatch total loss at step 5837: 0.249689\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.1078059   0.89219409  0.          1.        ]\n",
      " [ 0.04046061  0.95953935  0.          1.        ]]\n",
      "Minibatch total loss at step 5850: 0.251918\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.20397568  0.79602432  0.          1.        ]\n",
      " [ 0.05064033  0.9493596   0.          1.        ]]\n",
      "Minibatch total loss at step 5863: 0.228068\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.26837134  0.73162872  0.          1.        ]\n",
      " [ 0.85954577  0.14045422  1.          0.        ]]\n",
      "Minibatch total loss at step 5876: 0.230195\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.09159186  0.90840816  0.          1.        ]\n",
      " [ 0.09543664  0.90456343  0.          1.        ]]\n",
      "Minibatch total loss at step 5889: 0.196721\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.04793659  0.95206344  0.          1.        ]\n",
      " [ 0.96004003  0.03995994  1.          0.        ]]\n",
      "Validation accuracy: 47.5\n",
      "Minibatch total loss at step 5902: 0.200874\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.89627969  0.10372032  1.          0.        ]\n",
      " [ 0.09679539  0.90320462  0.          1.        ]]\n",
      "Minibatch total loss at step 5915: 0.239517\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.93126357  0.06873639  1.          0.        ]\n",
      " [ 0.13159022  0.86840975  0.          1.        ]]\n",
      "Minibatch total loss at step 5928: 0.237535\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.13590573  0.86409426  0.          1.        ]\n",
      " [ 0.37334698  0.62665302  1.          0.        ]]\n",
      "Minibatch total loss at step 5941: 0.172083\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.72696388  0.27303609  1.          0.        ]\n",
      " [ 0.05207976  0.94792026  0.          1.        ]]\n",
      "Minibatch total loss at step 5954: 0.210262\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.41621688  0.58378315  0.          1.        ]\n",
      " [ 0.45252058  0.54747945  0.          1.        ]]\n",
      "Minibatch total loss at step 5967: 0.245066\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.88269174  0.11730823  1.          0.        ]\n",
      " [ 0.02694244  0.97305763  0.          1.        ]]\n",
      "Minibatch total loss at step 5980: 0.154601\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.78096837  0.21903165  1.          0.        ]\n",
      " [ 0.85123497  0.1487651   1.          0.        ]]\n",
      "Minibatch total loss at step 5993: 0.206128\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.80518991  0.19481003  1.          0.        ]\n",
      " [ 0.11698591  0.88301408  0.          1.        ]]\n",
      "Validation accuracy: 48.5\n",
      "Minibatch total loss at step 6006: 0.227812\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.07262295  0.9273771   0.          1.        ]\n",
      " [ 0.11186485  0.88813514  0.          1.        ]]\n",
      "Minibatch total loss at step 6019: 0.186692\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.96918696  0.03081306  1.          0.        ]\n",
      " [ 0.18979958  0.81020045  0.          1.        ]]\n",
      "Minibatch total loss at step 6032: 0.143817\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.12992015  0.87007976  0.          1.        ]\n",
      " [ 0.8940919   0.10590812  1.          0.        ]]\n",
      "Minibatch total loss at step 6045: 0.174505\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.0717634   0.9282366   0.          1.        ]\n",
      " [ 0.11630343  0.88369656  0.          1.        ]]\n",
      "Minibatch total loss at step 6058: 0.251713\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.98227304  0.01772697  1.          0.        ]\n",
      " [ 0.21741946  0.78258055  0.          1.        ]]\n",
      "Minibatch total loss at step 6071: 0.187177\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.15124267  0.84875733  0.          1.        ]\n",
      " [ 0.91941559  0.08058444  1.          0.        ]]\n",
      "Minibatch total loss at step 6084: 0.245295\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.06172789  0.93827212  0.          1.        ]\n",
      " [ 0.17719001  0.82280999  0.          1.        ]]\n",
      "Minibatch total loss at step 6097: 0.160489\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.13465969  0.86534029  0.          1.        ]\n",
      " [ 0.2878297   0.7121703   0.          1.        ]]\n",
      "Validation accuracy: 55.5\n",
      "Minibatch total loss at step 6110: 0.412122\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.97877789  0.02122215  1.          0.        ]\n",
      " [ 0.28446835  0.71553165  0.          1.        ]]\n",
      "Minibatch total loss at step 6123: 0.183186\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.96323627  0.03676372  1.          0.        ]\n",
      " [ 0.02166361  0.97833633  0.          1.        ]]\n",
      "Minibatch total loss at step 6136: 0.271267\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.53816211  0.46183798  0.          1.        ]\n",
      " [ 0.8574506   0.14254941  1.          0.        ]]\n",
      "Minibatch total loss at step 6149: 0.215852\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.65824604  0.34175396  1.          0.        ]\n",
      " [ 0.16259634  0.83740366  0.          1.        ]]\n",
      "Minibatch total loss at step 6162: 0.194321\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.04205878  0.95794123  0.          1.        ]\n",
      " [ 0.61875659  0.38124341  1.          0.        ]]\n",
      "Minibatch total loss at step 6175: 0.261452\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.22357473  0.77642524  0.          1.        ]\n",
      " [ 0.41633067  0.58366936  1.          0.        ]]\n",
      "Minibatch total loss at step 6188: 0.165403\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.87837934  0.12162068  1.          0.        ]\n",
      " [ 0.90393597  0.09606405  1.          0.        ]]\n",
      "Validation accuracy: 53.0\n",
      "Minibatch total loss at step 6201: 0.164805\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.11751862  0.8824814   0.          1.        ]\n",
      " [ 0.02807887  0.97192109  0.          1.        ]]\n",
      "Minibatch total loss at step 6214: 0.220976\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.25361413  0.74638581  0.          1.        ]\n",
      " [ 0.06091943  0.9390806   0.          1.        ]]\n",
      "Minibatch total loss at step 6227: 0.163190\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.20534517  0.79465479  0.          1.        ]\n",
      " [ 0.06829853  0.93170148  0.          1.        ]]\n",
      "Minibatch total loss at step 6240: 0.306931\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.05640072  0.94359928  0.          1.        ]\n",
      " [ 0.03992594  0.96007407  0.          1.        ]]\n",
      "Minibatch total loss at step 6253: 0.210590\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.18275489  0.81724507  0.          1.        ]\n",
      " [ 0.95178086  0.04821917  1.          0.        ]]\n",
      "Minibatch total loss at step 6266: 0.209817\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.52477103  0.47522891  1.          0.        ]\n",
      " [ 0.1112278   0.88877219  0.          1.        ]]\n",
      "Minibatch total loss at step 6279: 0.200581\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.21723877  0.78276122  0.          1.        ]\n",
      " [ 0.39654085  0.60345912  0.          1.        ]]\n",
      "Minibatch total loss at step 6292: 0.175753\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.85660535  0.14339459  1.          0.        ]\n",
      " [ 0.10116389  0.89883608  0.          1.        ]]\n",
      "Validation accuracy: 52.0\n",
      "Minibatch total loss at step 6305: 0.173361\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.22808926  0.77191073  0.          1.        ]\n",
      " [ 0.86535817  0.13464189  1.          0.        ]]\n",
      "Minibatch total loss at step 6318: 0.210041\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.07800978  0.92199022  0.          1.        ]\n",
      " [ 0.60714358  0.39285642  1.          0.        ]]\n",
      "Minibatch total loss at step 6331: 0.233271\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.32417646  0.67582351  0.          1.        ]\n",
      " [ 0.2448497   0.75515026  0.          1.        ]]\n",
      "Minibatch total loss at step 6344: 0.146585\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.97266036  0.02733961  1.          0.        ]\n",
      " [ 0.13465776  0.86534226  0.          1.        ]]\n",
      "Minibatch total loss at step 6357: 0.222742\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.87854439  0.12145562  1.          0.        ]\n",
      " [ 0.03948624  0.96051371  0.          1.        ]]\n",
      "Minibatch total loss at step 6370: 0.201161\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.96554536  0.03445463  1.          0.        ]\n",
      " [ 0.0350152   0.96498477  0.          1.        ]]\n",
      "Minibatch total loss at step 6383: 0.181493\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.08606426  0.91393578  0.          1.        ]\n",
      " [ 0.02421159  0.97578841  0.          1.        ]]\n",
      "Minibatch total loss at step 6396: 0.228626\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.01012049  0.98987949  0.          1.        ]\n",
      " [ 0.3995553   0.60044467  0.          1.        ]]\n",
      "Validation accuracy: 54.5\n",
      "Minibatch total loss at step 6409: 0.186040\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.81531739  0.18468267  1.          0.        ]\n",
      " [ 0.82669395  0.17330603  1.          0.        ]]\n",
      "Minibatch total loss at step 6422: 0.183747\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.842112    0.15788803  1.          0.        ]\n",
      " [ 0.67529029  0.32470971  1.          0.        ]]\n",
      "Minibatch total loss at step 6435: 0.190536\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.08107523  0.91892481  0.          1.        ]\n",
      " [ 0.37125427  0.62874573  0.          1.        ]]\n",
      "Minibatch total loss at step 6448: 0.201357\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.96824104  0.03175892  1.          0.        ]\n",
      " [ 0.25952128  0.74047869  0.          1.        ]]\n",
      "Minibatch total loss at step 6461: 0.205705\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.76188654  0.23811345  1.          0.        ]\n",
      " [ 0.90353388  0.09646611  1.          0.        ]]\n",
      "Minibatch total loss at step 6474: 0.220391\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.07476869  0.92523134  0.          1.        ]\n",
      " [ 0.17629626  0.82370377  0.          1.        ]]\n",
      "Minibatch total loss at step 6487: 0.266066\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.6983915   0.30160844  0.          1.        ]\n",
      " [ 0.17925264  0.82074732  0.          1.        ]]\n",
      "Minibatch total loss at step 6500: 0.194708\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.05393666  0.94606328  0.          1.        ]\n",
      " [ 0.7185092   0.28149074  1.          0.        ]]\n",
      "Validation accuracy: 53.0\n",
      "Minibatch total loss at step 6513: 0.178547\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.07474458  0.92525542  0.          1.        ]\n",
      " [ 0.23997416  0.7600258   0.          1.        ]]\n",
      "Minibatch total loss at step 6526: 0.229340\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.22541785  0.77458209  0.          1.        ]\n",
      " [ 0.37526914  0.62473089  0.          1.        ]]\n",
      "Minibatch total loss at step 6539: 0.245245\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.20878354  0.79121643  0.          1.        ]\n",
      " [ 0.27705619  0.72294384  0.          1.        ]]\n",
      "Minibatch total loss at step 6552: 0.191339\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.73824441  0.26175556  1.          0.        ]\n",
      " [ 0.10533293  0.89466709  0.          1.        ]]\n",
      "Minibatch total loss at step 6565: 0.181866\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.03871277  0.96128726  0.          1.        ]\n",
      " [ 0.76883888  0.23116113  1.          0.        ]]\n",
      "Minibatch total loss at step 6578: 0.233108\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.20192493  0.79807508  0.          1.        ]\n",
      " [ 0.15111788  0.84888208  0.          1.        ]]\n",
      "Minibatch total loss at step 6591: 0.185148\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.06849343  0.93150657  0.          1.        ]\n",
      " [ 0.09065019  0.9093498   0.          1.        ]]\n",
      "Validation accuracy: 55.0\n",
      "Minibatch total loss at step 6604: 0.200881\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.01701451  0.98298556  0.          1.        ]\n",
      " [ 0.75530696  0.24469307  1.          0.        ]]\n",
      "Minibatch total loss at step 6617: 0.229530\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.52885449  0.47114557  0.          1.        ]\n",
      " [ 0.94238555  0.05761448  1.          0.        ]]\n",
      "Minibatch total loss at step 6630: 0.296836\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.12676306  0.87323689  0.          1.        ]\n",
      " [ 0.73775536  0.26224467  1.          0.        ]]\n",
      "Minibatch total loss at step 6643: 0.180749\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.24877191  0.75122809  0.          1.        ]\n",
      " [ 0.07525068  0.92474931  0.          1.        ]]\n",
      "Minibatch total loss at step 6656: 0.222937\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.09245231  0.90754765  0.          1.        ]\n",
      " [ 0.58108544  0.4189145   1.          0.        ]]\n",
      "Minibatch total loss at step 6669: 0.256366\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.54025126  0.45974872  1.          0.        ]\n",
      " [ 0.66714215  0.33285782  1.          0.        ]]\n",
      "Minibatch total loss at step 6682: 0.200799\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.94697845  0.05302151  1.          0.        ]\n",
      " [ 0.78388155  0.21611841  1.          0.        ]]\n",
      "Minibatch total loss at step 6695: 0.176974\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.1029737   0.8970263   0.          1.        ]\n",
      " [ 0.24309008  0.75690985  0.          1.        ]]\n",
      "Validation accuracy: 53.5\n",
      "Minibatch total loss at step 6708: 0.170812\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.76248622  0.23751374  1.          0.        ]\n",
      " [ 0.24277346  0.75722647  0.          1.        ]]\n",
      "Minibatch total loss at step 6721: 0.156379\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.13371369  0.86628628  0.          1.        ]\n",
      " [ 0.91018915  0.08981088  1.          0.        ]]\n",
      "Minibatch total loss at step 6734: 0.162182\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.07376214  0.92623782  0.          1.        ]\n",
      " [ 0.05891721  0.94108284  0.          1.        ]]\n",
      "Minibatch total loss at step 6747: 0.205979\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.10020429  0.89979571  0.          1.        ]\n",
      " [ 0.14578997  0.85421002  0.          1.        ]]\n",
      "Minibatch total loss at step 6760: 0.162156\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.1011164   0.89888358  0.          1.        ]\n",
      " [ 0.72600889  0.27399111  1.          0.        ]]\n",
      "Minibatch total loss at step 6773: 0.175882\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.34901005  0.65098995  0.          1.        ]\n",
      " [ 0.097371    0.90262902  0.          1.        ]]\n",
      "Minibatch total loss at step 6786: 0.194320\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.3711009   0.6288991   0.          1.        ]\n",
      " [ 0.04626305  0.95373696  0.          1.        ]]\n",
      "Minibatch total loss at step 6799: 0.142006\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.85261327  0.14738673  1.          0.        ]\n",
      " [ 0.04594825  0.95405179  0.          1.        ]]\n",
      "Validation accuracy: 50.5\n",
      "Minibatch total loss at step 6812: 0.197300\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.23041356  0.76958644  0.          1.        ]\n",
      " [ 0.06167499  0.93832505  0.          1.        ]]\n",
      "Minibatch total loss at step 6825: 0.275996\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.71573222  0.28426772  0.          1.        ]\n",
      " [ 0.76604503  0.23395501  1.          0.        ]]\n",
      "Minibatch total loss at step 6838: 0.183996\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.03056324  0.96943676  0.          1.        ]\n",
      " [ 0.64478779  0.35521221  1.          0.        ]]\n",
      "Minibatch total loss at step 6851: 0.249274\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.11146422  0.8885358   0.          1.        ]\n",
      " [ 0.62632579  0.37367418  1.          0.        ]]\n",
      "Minibatch total loss at step 6864: 0.145825\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.94144416  0.0585559   1.          0.        ]\n",
      " [ 0.0722001   0.92779988  0.          1.        ]]\n",
      "Minibatch total loss at step 6877: 0.232154\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.89438426  0.10561571  1.          0.        ]\n",
      " [ 0.03059725  0.96940267  0.          1.        ]]\n",
      "Minibatch total loss at step 6890: 0.247230\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.87959999  0.12039997  1.          0.        ]\n",
      " [ 0.80382735  0.19617268  1.          0.        ]]\n",
      "Validation accuracy: 53.5\n",
      "Minibatch total loss at step 6903: 0.238136\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.72821009  0.27178994  1.          0.        ]\n",
      " [ 0.67578584  0.32421419  1.          0.        ]]\n",
      "Minibatch total loss at step 6916: 0.215861\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.16077313  0.83922684  0.          1.        ]\n",
      " [ 0.19469625  0.80530369  0.          1.        ]]\n",
      "Minibatch total loss at step 6929: 0.210462\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.07907366  0.92092627  0.          1.        ]\n",
      " [ 0.1276259   0.87237412  0.          1.        ]]\n",
      "Minibatch total loss at step 6942: 0.159403\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.91183186  0.08816815  1.          0.        ]\n",
      " [ 0.89394194  0.10605812  1.          0.        ]]\n",
      "Minibatch total loss at step 6955: 0.177679\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.10909518  0.89090478  0.          1.        ]\n",
      " [ 0.3005814   0.6994186   0.          1.        ]]\n",
      "Minibatch total loss at step 6968: 0.227172\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.86010212  0.13989782  1.          0.        ]\n",
      " [ 0.61456949  0.38543051  1.          0.        ]]\n",
      "Minibatch total loss at step 6981: 0.180638\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.02494197  0.97505808  0.          1.        ]\n",
      " [ 0.32942447  0.6705755   0.          1.        ]]\n",
      "Minibatch total loss at step 6994: 0.171456\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.70771128  0.29228875  1.          0.        ]\n",
      " [ 0.81461835  0.18538168  1.          0.        ]]\n",
      "Validation accuracy: 51.0\n",
      "Minibatch total loss at step 7007: 0.182894\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.71424794  0.28575203  1.          0.        ]\n",
      " [ 0.75108939  0.24891065  1.          0.        ]]\n",
      "Minibatch total loss at step 7020: 0.156377\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.82329595  0.17670403  1.          0.        ]\n",
      " [ 0.03088021  0.96911979  0.          1.        ]]\n",
      "Minibatch total loss at step 7033: 0.172895\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.13857286  0.86142707  0.          1.        ]\n",
      " [ 0.15549158  0.84450835  0.          1.        ]]\n",
      "Minibatch total loss at step 7046: 0.206222\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.97366309  0.02633696  1.          0.        ]\n",
      " [ 0.20921752  0.79078251  0.          1.        ]]\n",
      "Minibatch total loss at step 7059: 0.168259\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.83318669  0.16681327  1.          0.        ]\n",
      " [ 0.81364393  0.18635608  1.          0.        ]]\n",
      "Minibatch total loss at step 7072: 0.232201\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.34719297  0.652807    0.          1.        ]\n",
      " [ 0.91961336  0.08038671  1.          0.        ]]\n",
      "Minibatch total loss at step 7085: 0.302053\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.03534891  0.96465111  0.          1.        ]\n",
      " [ 0.0297463   0.97025371  0.          1.        ]]\n",
      "Minibatch total loss at step 7098: 0.169358\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.12924679  0.87075317  0.          1.        ]\n",
      " [ 0.06417478  0.93582523  0.          1.        ]]\n",
      "Validation accuracy: 48.5\n",
      "Minibatch total loss at step 7111: 0.210774\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.47328201  0.52671802  0.          1.        ]\n",
      " [ 0.20070037  0.7992996   0.          1.        ]]\n",
      "Minibatch total loss at step 7124: 0.216604\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.59828478  0.40171519  0.          1.        ]\n",
      " [ 0.06544439  0.93455553  0.          1.        ]]\n",
      "Minibatch total loss at step 7137: 0.196463\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.04006456  0.95993543  0.          1.        ]\n",
      " [ 0.1464669   0.85353315  0.          1.        ]]\n",
      "Minibatch total loss at step 7150: 0.174206\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.11692757  0.88307244  0.          1.        ]\n",
      " [ 0.725344    0.27465597  1.          0.        ]]\n",
      "Minibatch total loss at step 7163: 0.255919\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.09549087  0.90450919  0.          1.        ]\n",
      " [ 0.02488187  0.97511816  0.          1.        ]]\n",
      "Minibatch total loss at step 7176: 0.214667\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.71894443  0.28105551  1.          0.        ]\n",
      " [ 0.0330735   0.96692657  0.          1.        ]]\n",
      "Minibatch total loss at step 7189: 0.247241\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.69634801  0.30365193  1.          0.        ]\n",
      " [ 0.11215673  0.88784331  0.          1.        ]]\n",
      "Validation accuracy: 54.5\n",
      "Minibatch total loss at step 7202: 0.184428\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.08808255  0.91191745  0.          1.        ]\n",
      " [ 0.18684272  0.81315732  0.          1.        ]]\n",
      "Minibatch total loss at step 7215: 0.350688\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.68406022  0.31593978  1.          0.        ]\n",
      " [ 0.70227975  0.29772028  1.          0.        ]]\n",
      "Minibatch total loss at step 7228: 0.169783\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.77277464  0.22722529  1.          0.        ]\n",
      " [ 0.74974632  0.25025371  1.          0.        ]]\n",
      "Minibatch total loss at step 7241: 0.209782\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.41681194  0.58318806  0.          1.        ]\n",
      " [ 0.07054281  0.92945725  0.          1.        ]]\n",
      "Minibatch total loss at step 7254: 0.156084\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.96456414  0.03543583  1.          0.        ]\n",
      " [ 0.12303751  0.87696242  0.          1.        ]]\n",
      "Minibatch total loss at step 7267: 0.212545\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.79689062  0.20310935  1.          0.        ]\n",
      " [ 0.0165268   0.98347312  0.          1.        ]]\n",
      "Minibatch total loss at step 7280: 0.182859\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.75308299  0.24691698  1.          0.        ]\n",
      " [ 0.10071881  0.89928114  0.          1.        ]]\n",
      "Minibatch total loss at step 7293: 0.281786\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.71182483  0.28817511  1.          0.        ]\n",
      " [ 0.84715223  0.15284778  1.          0.        ]]\n",
      "Validation accuracy: 51.5\n",
      "Minibatch total loss at step 7306: 0.173020\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.2470635   0.75293654  0.          1.        ]\n",
      " [ 0.72208893  0.27791113  1.          0.        ]]\n",
      "Minibatch total loss at step 7319: 0.189174\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.18504959  0.81495041  0.          1.        ]\n",
      " [ 0.22752054  0.77247947  0.          1.        ]]\n",
      "Minibatch total loss at step 7332: 0.156203\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.79552567  0.2044743   1.          0.        ]\n",
      " [ 0.89972222  0.10027777  1.          0.        ]]\n",
      "Minibatch total loss at step 7345: 0.232066\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.7841748   0.21582517  1.          0.        ]\n",
      " [ 0.31099045  0.68900955  1.          0.        ]]\n",
      "Minibatch total loss at step 7358: 0.252076\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.96532106  0.03467892  1.          0.        ]\n",
      " [ 0.08044364  0.91955638  0.          1.        ]]\n",
      "Minibatch total loss at step 7371: 0.168052\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.95455897  0.04544098  1.          0.        ]\n",
      " [ 0.55271137  0.44728866  1.          0.        ]]\n",
      "Minibatch total loss at step 7384: 0.218530\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.03204935  0.96795064  0.          1.        ]\n",
      " [ 0.97866017  0.02133985  1.          0.        ]]\n",
      "Minibatch total loss at step 7397: 0.214088\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.03720377  0.96279627  0.          1.        ]\n",
      " [ 0.69467032  0.30532965  1.          0.        ]]\n",
      "Validation accuracy: 51.0\n",
      "Minibatch total loss at step 7410: 0.185195\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.06300885  0.93699116  0.          1.        ]\n",
      " [ 0.16023625  0.8397637   0.          1.        ]]\n",
      "Minibatch total loss at step 7423: 0.153608\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.93793106  0.06206886  1.          0.        ]\n",
      " [ 0.04170733  0.9582926   0.          1.        ]]\n",
      "Minibatch total loss at step 7436: 0.216020\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.03234559  0.96765447  0.          1.        ]\n",
      " [ 0.66392499  0.33607501  1.          0.        ]]\n",
      "Minibatch total loss at step 7449: 0.215040\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.09984643  0.90015364  0.          1.        ]\n",
      " [ 0.13055593  0.86944413  0.          1.        ]]\n",
      "Minibatch total loss at step 7462: 0.196180\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.96411186  0.0358882   1.          0.        ]\n",
      " [ 0.19384851  0.80615151  0.          1.        ]]\n",
      "Minibatch total loss at step 7475: 0.170953\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.03392432  0.96607566  0.          1.        ]\n",
      " [ 0.0902083   0.90979171  0.          1.        ]]\n",
      "Minibatch total loss at step 7488: 0.162929\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.87237036  0.12762961  1.          0.        ]\n",
      " [ 0.11787538  0.8821246   0.          1.        ]]\n",
      "Validation accuracy: 50.5\n",
      "Minibatch total loss at step 7501: 0.220250\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.60467744  0.39532259  1.          0.        ]\n",
      " [ 0.05062258  0.94937748  0.          1.        ]]\n",
      "Minibatch total loss at step 7514: 0.207537\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.29176414  0.70823592  0.          1.        ]\n",
      " [ 0.99447566  0.00552436  1.          0.        ]]\n",
      "Minibatch total loss at step 7527: 0.283084\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.88358533  0.1164147   1.          0.        ]\n",
      " [ 0.51095933  0.4890407   1.          0.        ]]\n",
      "Minibatch total loss at step 7540: 0.156061\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.70371133  0.29628864  1.          0.        ]\n",
      " [ 0.8946228   0.10537717  1.          0.        ]]\n",
      "Minibatch total loss at step 7553: 0.175155\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.76661873  0.23338129  1.          0.        ]\n",
      " [ 0.19225177  0.8077482   0.          1.        ]]\n",
      "Minibatch total loss at step 7566: 0.188122\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.25651237  0.74348772  0.          1.        ]\n",
      " [ 0.97913456  0.02086541  1.          0.        ]]\n",
      "Minibatch total loss at step 7579: 0.250727\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.74163276  0.2583673   1.          0.        ]\n",
      " [ 0.85791957  0.14208046  1.          0.        ]]\n",
      "Minibatch total loss at step 7592: 0.176460\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.28815293  0.71184707  0.          1.        ]\n",
      " [ 0.83398676  0.16601327  1.          0.        ]]\n",
      "Validation accuracy: 50.5\n",
      "Minibatch total loss at step 7605: 0.192280\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.27411449  0.72588551  0.          1.        ]\n",
      " [ 0.28176609  0.71823394  0.          1.        ]]\n",
      "Minibatch total loss at step 7618: 0.272742\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.45338023  0.54661971  1.          0.        ]\n",
      " [ 0.82736742  0.17263253  1.          0.        ]]\n",
      "Minibatch total loss at step 7631: 0.148882\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.22201112  0.77798885  0.          1.        ]\n",
      " [ 0.1631476   0.83685237  0.          1.        ]]\n",
      "Minibatch total loss at step 7644: 0.277559\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.25933498  0.74066508  0.          1.        ]\n",
      " [ 0.03940669  0.96059334  0.          1.        ]]\n",
      "Minibatch total loss at step 7657: 0.189768\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.874892    0.12510796  1.          0.        ]\n",
      " [ 0.35489321  0.64510685  0.          1.        ]]\n",
      "Minibatch total loss at step 7670: 0.291241\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.28225464  0.71774536  0.          1.        ]\n",
      " [ 0.97910416  0.02089587  1.          0.        ]]\n",
      "Minibatch total loss at step 7683: 0.232501\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.03957476  0.96042526  0.          1.        ]\n",
      " [ 0.68947202  0.31052801  1.          0.        ]]\n",
      "Minibatch total loss at step 7696: 0.249903\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.05925805  0.94074196  0.          1.        ]\n",
      " [ 0.9047963   0.09520367  1.          0.        ]]\n",
      "Validation accuracy: 51.0\n",
      "Minibatch total loss at step 7709: 0.203390\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.75460464  0.24539529  1.          0.        ]\n",
      " [ 0.01865015  0.98134983  0.          1.        ]]\n",
      "Minibatch total loss at step 7722: 0.262748\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.12405837  0.87594157  0.          1.        ]\n",
      " [ 0.91112     0.08887999  1.          0.        ]]\n",
      "Minibatch total loss at step 7735: 0.172667\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.92454433  0.07545564  1.          0.        ]\n",
      " [ 0.20539241  0.79460758  0.          1.        ]]\n",
      "Minibatch total loss at step 7748: 0.153106\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.28077599  0.71922398  0.          1.        ]\n",
      " [ 0.10840211  0.89159793  0.          1.        ]]\n",
      "Minibatch total loss at step 7761: 0.222811\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.10881565  0.89118439  0.          1.        ]\n",
      " [ 0.12905011  0.87094992  0.          1.        ]]\n",
      "Minibatch total loss at step 7774: 0.189846\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.54584992  0.45415008  1.          0.        ]\n",
      " [ 0.04812696  0.95187306  0.          1.        ]]\n",
      "Minibatch total loss at step 7787: 0.178952\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.07876896  0.92123103  0.          1.        ]\n",
      " [ 0.09748923  0.9025107   0.          1.        ]]\n",
      "Minibatch total loss at step 7800: 0.161088\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.09758347  0.90241653  0.          1.        ]\n",
      " [ 0.94661117  0.05338882  1.          0.        ]]\n",
      "Validation accuracy: 52.5\n",
      "Minibatch total loss at step 7813: 0.213008\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.08121033  0.91878974  0.          1.        ]\n",
      " [ 0.11829506  0.88170499  0.          1.        ]]\n",
      "Minibatch total loss at step 7826: 0.174483\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.85603982  0.14396022  1.          0.        ]\n",
      " [ 0.08656608  0.91343391  0.          1.        ]]\n",
      "Minibatch total loss at step 7839: 0.212480\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.22061868  0.77938139  0.          1.        ]\n",
      " [ 0.67281818  0.32718179  1.          0.        ]]\n",
      "Minibatch total loss at step 7852: 0.200235\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.05801233  0.94198763  0.          1.        ]\n",
      " [ 0.06218359  0.93781638  0.          1.        ]]\n",
      "Minibatch total loss at step 7865: 0.153095\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.31854582  0.68145424  0.          1.        ]\n",
      " [ 0.92017996  0.07982004  1.          0.        ]]\n",
      "Minibatch total loss at step 7878: 0.172816\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.96166658  0.03833347  1.          0.        ]\n",
      " [ 0.06731431  0.93268573  0.          1.        ]]\n",
      "Minibatch total loss at step 7891: 0.176305\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.05857713  0.94142288  0.          1.        ]\n",
      " [ 0.33757141  0.66242856  0.          1.        ]]\n",
      "Validation accuracy: 52.0\n",
      "Minibatch total loss at step 7904: 0.195503\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.18739699  0.812603    0.          1.        ]\n",
      " [ 0.18078427  0.81921577  0.          1.        ]]\n",
      "Minibatch total loss at step 7917: 0.172058\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.89391023  0.10608982  1.          0.        ]\n",
      " [ 0.89270467  0.10729535  1.          0.        ]]\n",
      "Minibatch total loss at step 7930: 0.262714\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.3036243   0.69637567  0.          1.        ]\n",
      " [ 0.71185905  0.28814101  1.          0.        ]]\n",
      "Minibatch total loss at step 7943: 0.203056\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.29310146  0.70689851  0.          1.        ]\n",
      " [ 0.05752939  0.94247067  0.          1.        ]]\n",
      "Minibatch total loss at step 7956: 0.427645\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.13653558  0.86346442  0.          1.        ]\n",
      " [ 0.9840644   0.01593559  1.          0.        ]]\n",
      "Minibatch total loss at step 7969: 0.147592\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.10326377  0.89673626  0.          1.        ]\n",
      " [ 0.76610738  0.23389256  1.          0.        ]]\n",
      "Minibatch total loss at step 7982: 0.195245\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.87089455  0.12910542  1.          0.        ]\n",
      " [ 0.26513824  0.73486179  0.          1.        ]]\n",
      "Minibatch total loss at step 7995: 0.166205\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.08270741  0.91729259  0.          1.        ]\n",
      " [ 0.0459097   0.9540903   0.          1.        ]]\n",
      "Validation accuracy: 53.0\n",
      "Minibatch total loss at step 8008: 0.189824\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.64769816  0.35230181  1.          0.        ]\n",
      " [ 0.06484335  0.9351567   0.          1.        ]]\n",
      "Minibatch total loss at step 8021: 0.213442\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.72726154  0.27273843  1.          0.        ]\n",
      " [ 0.07062442  0.92937559  0.          1.        ]]\n",
      "Minibatch total loss at step 8034: 0.190750\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.86752468  0.13247538  1.          0.        ]\n",
      " [ 0.08671074  0.91328931  0.          1.        ]]\n",
      "Minibatch total loss at step 8047: 0.188886\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.03327005  0.96673     0.          1.        ]\n",
      " [ 0.75325376  0.24674627  1.          0.        ]]\n",
      "Minibatch total loss at step 8060: 0.198590\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.14146049  0.85853946  0.          1.        ]\n",
      " [ 0.08680817  0.91319185  0.          1.        ]]\n",
      "Minibatch total loss at step 8073: 0.160661\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.14895409  0.85104591  0.          1.        ]\n",
      " [ 0.96706533  0.03293472  1.          0.        ]]\n",
      "Minibatch total loss at step 8086: 0.279340\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.05993298  0.94006699  0.          1.        ]\n",
      " [ 0.0793158   0.92068422  0.          1.        ]]\n",
      "Minibatch total loss at step 8099: 0.199092\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.96814013  0.03185983  1.          0.        ]\n",
      " [ 0.1314479   0.86855209  0.          1.        ]]\n",
      "Validation accuracy: 49.0\n",
      "Minibatch total loss at step 8112: 0.175787\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.10158037  0.89841962  0.          1.        ]\n",
      " [ 0.81392354  0.18607646  1.          0.        ]]\n",
      "Minibatch total loss at step 8125: 0.178694\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.30637434  0.69362569  0.          1.        ]\n",
      " [ 0.18976815  0.8102318   0.          1.        ]]\n",
      "Minibatch total loss at step 8138: 0.155258\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.29224515  0.70775485  0.          1.        ]\n",
      " [ 0.09724701  0.90275294  0.          1.        ]]\n",
      "Minibatch total loss at step 8151: 0.163876\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.89445037  0.10554969  1.          0.        ]\n",
      " [ 0.73024845  0.26975155  1.          0.        ]]\n",
      "Minibatch total loss at step 8164: 0.190383\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.60922372  0.39077628  1.          0.        ]\n",
      " [ 0.07946719  0.92053276  0.          1.        ]]\n",
      "Minibatch total loss at step 8177: 0.201280\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.32174224  0.67825776  0.          1.        ]\n",
      " [ 0.17024803  0.82975197  0.          1.        ]]\n",
      "Minibatch total loss at step 8190: 0.195013\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.03049973  0.9695003   0.          1.        ]\n",
      " [ 0.18226507  0.81773496  0.          1.        ]]\n",
      "Validation accuracy: 52.5\n",
      "Minibatch total loss at step 8203: 0.158736\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.1853603   0.81463969  0.          1.        ]\n",
      " [ 0.89705771  0.10294233  1.          0.        ]]\n",
      "Minibatch total loss at step 8216: 0.160172\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.05599885  0.9440012   0.          1.        ]\n",
      " [ 0.13402596  0.86597401  0.          1.        ]]\n",
      "Minibatch total loss at step 8229: 0.211244\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.05216226  0.94783777  0.          1.        ]\n",
      " [ 0.04859229  0.95140767  0.          1.        ]]\n",
      "Minibatch total loss at step 8242: 0.168469\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.17687884  0.82312113  0.          1.        ]\n",
      " [ 0.16072094  0.83927912  0.          1.        ]]\n",
      "Minibatch total loss at step 8255: 0.175747\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.64122689  0.35877308  1.          0.        ]\n",
      " [ 0.04119315  0.95880687  0.          1.        ]]\n",
      "Minibatch total loss at step 8268: 0.150540\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.81417626  0.18582372  1.          0.        ]\n",
      " [ 0.70507103  0.29492894  1.          0.        ]]\n",
      "Minibatch total loss at step 8281: 0.159073\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.05887723  0.94112283  0.          1.        ]\n",
      " [ 0.15814286  0.84185714  0.          1.        ]]\n",
      "Minibatch total loss at step 8294: 0.174979\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.07438751  0.92561257  0.          1.        ]\n",
      " [ 0.26986831  0.73013163  0.          1.        ]]\n",
      "Validation accuracy: 50.5\n",
      "Minibatch total loss at step 8307: 0.183053\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.85209858  0.14790142  1.          0.        ]\n",
      " [ 0.11483265  0.88516736  0.          1.        ]]\n",
      "Minibatch total loss at step 8320: 0.176640\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.14550504  0.85449499  0.          1.        ]\n",
      " [ 0.64420825  0.35579172  1.          0.        ]]\n",
      "Minibatch total loss at step 8333: 0.177466\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.20931371  0.79068631  0.          1.        ]\n",
      " [ 0.15560214  0.84439784  0.          1.        ]]\n",
      "Minibatch total loss at step 8346: 0.182731\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.87675089  0.12324913  1.          0.        ]\n",
      " [ 0.14554328  0.85445666  0.          1.        ]]\n",
      "Minibatch total loss at step 8359: 0.185920\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.35669237  0.64330763  0.          1.        ]\n",
      " [ 0.09475737  0.90524262  0.          1.        ]]\n",
      "Minibatch total loss at step 8372: 0.156611\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.23935689  0.76064312  0.          1.        ]\n",
      " [ 0.18383224  0.81616777  0.          1.        ]]\n",
      "Minibatch total loss at step 8385: 0.167489\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.15090254  0.84909743  0.          1.        ]\n",
      " [ 0.23343933  0.76656061  0.          1.        ]]\n",
      "Minibatch total loss at step 8398: 0.211241\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.0175128   0.98248714  0.          1.        ]\n",
      " [ 0.07933617  0.92066383  0.          1.        ]]\n",
      "Validation accuracy: 54.5\n",
      "Minibatch total loss at step 8411: 0.215988\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.70061934  0.29938066  1.          0.        ]\n",
      " [ 0.01679006  0.98320997  0.          1.        ]]\n",
      "Minibatch total loss at step 8424: 0.168171\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.07064651  0.92935342  0.          1.        ]\n",
      " [ 0.07878827  0.92121172  0.          1.        ]]\n",
      "Minibatch total loss at step 8437: 0.182092\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.116189    0.883811    0.          1.        ]\n",
      " [ 0.09498746  0.90501249  0.          1.        ]]\n",
      "Minibatch total loss at step 8450: 0.195806\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5126031   0.48739687  1.          0.        ]\n",
      " [ 0.05324071  0.94675928  0.          1.        ]]\n",
      "Minibatch total loss at step 8463: 0.201103\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.94617063  0.05382938  1.          0.        ]\n",
      " [ 0.08349565  0.91650432  0.          1.        ]]\n",
      "Minibatch total loss at step 8476: 0.265435\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.69230509  0.30769494  1.          0.        ]\n",
      " [ 0.53538132  0.46461865  1.          0.        ]]\n",
      "Minibatch total loss at step 8489: 0.168380\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.09597012  0.90402985  0.          1.        ]\n",
      " [ 0.62689322  0.37310678  1.          0.        ]]\n",
      "Validation accuracy: 48.5\n",
      "Minibatch total loss at step 8502: 0.177915\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.70989347  0.29010653  1.          0.        ]\n",
      " [ 0.05658237  0.94341761  0.          1.        ]]\n",
      "Minibatch total loss at step 8515: 0.200963\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.75862044  0.24137951  1.          0.        ]\n",
      " [ 0.125245    0.87475497  0.          1.        ]]\n",
      "Minibatch total loss at step 8528: 0.160783\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.76571     0.23428997  1.          0.        ]\n",
      " [ 0.0809653   0.91903472  0.          1.        ]]\n",
      "Minibatch total loss at step 8541: 0.149291\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.2509385   0.74906152  0.          1.        ]\n",
      " [ 0.18803275  0.81196725  0.          1.        ]]\n",
      "Minibatch total loss at step 8554: 0.170729\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.08649038  0.91350961  0.          1.        ]\n",
      " [ 0.32998225  0.67001772  0.          1.        ]]\n",
      "Minibatch total loss at step 8567: 0.162796\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.77665323  0.22334684  1.          0.        ]\n",
      " [ 0.09374646  0.90625352  0.          1.        ]]\n",
      "Minibatch total loss at step 8580: 0.170125\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.03061974  0.96938032  0.          1.        ]\n",
      " [ 0.69861197  0.301388    1.          0.        ]]\n",
      "Minibatch total loss at step 8593: 0.150217\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.17959388  0.82040608  0.          1.        ]\n",
      " [ 0.15178545  0.84821457  0.          1.        ]]\n",
      "Validation accuracy: 54.0\n",
      "Minibatch total loss at step 8606: 0.187824\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.80390346  0.19609651  1.          0.        ]\n",
      " [ 0.75314504  0.24685498  1.          0.        ]]\n",
      "Minibatch total loss at step 8619: 0.156435\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.15286504  0.84713495  0.          1.        ]\n",
      " [ 0.89588469  0.10411531  1.          0.        ]]\n",
      "Minibatch total loss at step 8632: 0.176902\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.25512639  0.74487364  0.          1.        ]\n",
      " [ 0.94997114  0.0500289   1.          0.        ]]\n",
      "Minibatch total loss at step 8645: 0.143821\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.09883223  0.90116769  0.          1.        ]\n",
      " [ 0.0727427   0.92725724  0.          1.        ]]\n",
      "Minibatch total loss at step 8658: 0.180648\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.16489552  0.83510453  0.          1.        ]\n",
      " [ 0.8461678   0.15383226  1.          0.        ]]\n",
      "Minibatch total loss at step 8671: 0.174828\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.84366208  0.15633787  1.          0.        ]\n",
      " [ 0.88894057  0.11105943  1.          0.        ]]\n",
      "Minibatch total loss at step 8684: 0.221399\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.71647674  0.28352323  1.          0.        ]\n",
      " [ 0.84207451  0.15792549  1.          0.        ]]\n",
      "Minibatch total loss at step 8697: 0.254443\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.68968362  0.31031641  1.          0.        ]\n",
      " [ 0.44245371  0.55754632  1.          0.        ]]\n",
      "Validation accuracy: 53.0\n",
      "Minibatch total loss at step 8710: 0.160434\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.08428797  0.915712    0.          1.        ]\n",
      " [ 0.20019852  0.79980153  0.          1.        ]]\n",
      "Minibatch total loss at step 8723: 0.241650\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.08263146  0.91736853  0.          1.        ]\n",
      " [ 0.23058391  0.76941609  0.          1.        ]]\n",
      "Minibatch total loss at step 8736: 0.240244\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.81662476  0.18337519  1.          0.        ]\n",
      " [ 0.55837107  0.44162899  1.          0.        ]]\n",
      "Minibatch total loss at step 8749: 0.191673\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.81000865  0.1899914   1.          0.        ]\n",
      " [ 0.0475499   0.9524501   0.          1.        ]]\n",
      "Minibatch total loss at step 8762: 0.189736\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.22470647  0.77529353  0.          1.        ]\n",
      " [ 0.19122611  0.80877393  0.          1.        ]]\n",
      "Minibatch total loss at step 8775: 0.188972\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.16026667  0.8397333   0.          1.        ]\n",
      " [ 0.85193193  0.14806807  1.          0.        ]]\n",
      "Minibatch total loss at step 8788: 0.172219\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.88362384  0.11637618  1.          0.        ]\n",
      " [ 0.96693838  0.03306163  1.          0.        ]]\n",
      "Validation accuracy: 51.5\n",
      "Minibatch total loss at step 8801: 0.196784\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.14450127  0.85549873  0.          1.        ]\n",
      " [ 0.34203264  0.65796727  0.          1.        ]]\n",
      "Minibatch total loss at step 8814: 0.189474\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.72412211  0.27587786  1.          0.        ]\n",
      " [ 0.8346976   0.1653024   1.          0.        ]]\n",
      "Minibatch total loss at step 8827: 0.156620\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.13917154  0.86082846  0.          1.        ]\n",
      " [ 0.13719587  0.86280411  0.          1.        ]]\n",
      "Minibatch total loss at step 8840: 0.147960\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.89972675  0.10027324  1.          0.        ]\n",
      " [ 0.13964324  0.86035681  0.          1.        ]]\n",
      "Minibatch total loss at step 8853: 0.148864\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.9196313   0.08036868  1.          0.        ]\n",
      " [ 0.89605713  0.1039429   1.          0.        ]]\n",
      "Minibatch total loss at step 8866: 0.151720\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.20561266  0.79438734  0.          1.        ]\n",
      " [ 0.16021335  0.83978665  0.          1.        ]]\n",
      "Minibatch total loss at step 8879: 0.174975\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.01820402  0.98179597  0.          1.        ]\n",
      " [ 0.1753072   0.82469273  0.          1.        ]]\n",
      "Minibatch total loss at step 8892: 0.165383\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.08802764  0.91197234  0.          1.        ]\n",
      " [ 0.35846105  0.64153892  0.          1.        ]]\n",
      "Validation accuracy: 53.5\n",
      "Minibatch total loss at step 8905: 0.158720\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.71927828  0.28072175  1.          0.        ]\n",
      " [ 0.90212142  0.09787861  1.          0.        ]]\n",
      "Minibatch total loss at step 8918: 0.171727\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.89412761  0.10587241  1.          0.        ]\n",
      " [ 0.18396425  0.81603581  0.          1.        ]]\n",
      "Minibatch total loss at step 8931: 0.265950\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.04350984  0.95649016  0.          1.        ]\n",
      " [ 0.6999197   0.30008033  1.          0.        ]]\n",
      "Minibatch total loss at step 8944: 0.176642\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.07300723  0.92699277  0.          1.        ]\n",
      " [ 0.82441181  0.17558821  1.          0.        ]]\n",
      "Minibatch total loss at step 8957: 0.158558\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.16084117  0.83915889  0.          1.        ]\n",
      " [ 0.16458087  0.83541912  0.          1.        ]]\n",
      "Minibatch total loss at step 8970: 0.155936\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.06978112  0.93021888  0.          1.        ]\n",
      " [ 0.97356409  0.02643592  1.          0.        ]]\n",
      "Minibatch total loss at step 8983: 0.236260\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.20546643  0.79453361  0.          1.        ]\n",
      " [ 0.5299834   0.47001663  1.          0.        ]]\n",
      "Minibatch total loss at step 8996: 0.208237\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.75308889  0.24691114  1.          0.        ]\n",
      " [ 0.1922701   0.8077299   0.          1.        ]]\n",
      "Validation accuracy: 51.5\n",
      "Minibatch total loss at step 9009: 0.205756\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.01797434  0.98202574  0.          1.        ]\n",
      " [ 0.32447016  0.67552984  0.          1.        ]]\n",
      "Minibatch total loss at step 9022: 0.186411\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.04050839  0.95949161  0.          1.        ]\n",
      " [ 0.80355382  0.19644618  1.          0.        ]]\n",
      "Minibatch total loss at step 9035: 0.210761\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.04443417  0.95556587  0.          1.        ]\n",
      " [ 0.7287308   0.27126917  1.          0.        ]]\n",
      "Minibatch total loss at step 9048: 0.165794\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.13537142  0.86462861  0.          1.        ]\n",
      " [ 0.02795115  0.97204888  0.          1.        ]]\n",
      "Minibatch total loss at step 9061: 0.286359\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.70928359  0.29071632  1.          0.        ]\n",
      " [ 0.58941722  0.41058275  1.          0.        ]]\n",
      "Minibatch total loss at step 9074: 0.147201\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.67246979  0.32753024  1.          0.        ]\n",
      " [ 0.13756213  0.86243784  0.          1.        ]]\n",
      "Minibatch total loss at step 9087: 0.158826\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.04288751  0.95711255  0.          1.        ]\n",
      " [ 0.0521915   0.94780856  0.          1.        ]]\n",
      "Minibatch total loss at step 9100: 0.174824\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.19511075  0.80488926  0.          1.        ]\n",
      " [ 0.08589441  0.91410559  0.          1.        ]]\n",
      "Validation accuracy: 53.0\n",
      "Minibatch total loss at step 9113: 0.173182\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.04700876  0.95299125  0.          1.        ]\n",
      " [ 0.88615721  0.11384275  1.          0.        ]]\n",
      "Minibatch total loss at step 9126: 0.146065\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.12415791  0.87584209  0.          1.        ]\n",
      " [ 0.70871347  0.29128659  1.          0.        ]]\n",
      "Minibatch total loss at step 9139: 0.263495\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.94715363  0.05284635  1.          0.        ]\n",
      " [ 0.21959884  0.78040111  0.          1.        ]]\n",
      "Minibatch total loss at step 9152: 0.190938\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.86300617  0.1369938   1.          0.        ]\n",
      " [ 0.82764637  0.17235363  1.          0.        ]]\n",
      "Minibatch total loss at step 9165: 0.177880\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.14609303  0.85390699  0.          1.        ]\n",
      " [ 0.0720299   0.92797005  0.          1.        ]]\n",
      "Minibatch total loss at step 9178: 0.161682\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.78893864  0.21106131  1.          0.        ]\n",
      " [ 0.19734357  0.80265641  0.          1.        ]]\n",
      "Minibatch total loss at step 9191: 0.149902\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.84194005  0.15806001  1.          0.        ]\n",
      " [ 0.84713435  0.15286569  1.          0.        ]]\n",
      "Validation accuracy: 55.5\n",
      "Minibatch total loss at step 9204: 0.140794\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.16139093  0.83860904  0.          1.        ]\n",
      " [ 0.90373242  0.09626757  1.          0.        ]]\n",
      "Minibatch total loss at step 9217: 0.151190\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.75590599  0.24409398  1.          0.        ]\n",
      " [ 0.92054123  0.0794587   1.          0.        ]]\n",
      "Minibatch total loss at step 9230: 0.176679\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.97688872  0.02311132  1.          0.        ]\n",
      " [ 0.88203287  0.11796709  1.          0.        ]]\n",
      "Minibatch total loss at step 9243: 0.171039\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.87104911  0.12895088  1.          0.        ]\n",
      " [ 0.5793581   0.4206419   1.          0.        ]]\n",
      "Minibatch total loss at step 9256: 0.184254\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.13048758  0.86951244  0.          1.        ]\n",
      " [ 0.61602443  0.38397557  1.          0.        ]]\n",
      "Minibatch total loss at step 9269: 0.169112\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.0538003   0.94619966  0.          1.        ]\n",
      " [ 0.29830998  0.70169002  0.          1.        ]]\n",
      "Minibatch total loss at step 9282: 0.203750\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.87829262  0.12170739  1.          0.        ]\n",
      " [ 0.80393708  0.19606297  1.          0.        ]]\n",
      "Minibatch total loss at step 9295: 0.202548\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.29523161  0.70476842  0.          1.        ]\n",
      " [ 0.20365594  0.79634404  0.          1.        ]]\n",
      "Validation accuracy: 54.5\n",
      "Minibatch total loss at step 9308: 0.194167\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.16251142  0.83748859  0.          1.        ]\n",
      " [ 0.3169508   0.6830492   0.          1.        ]]\n",
      "Minibatch total loss at step 9321: 0.186999\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.06000087  0.93999916  0.          1.        ]\n",
      " [ 0.78344566  0.21655437  1.          0.        ]]\n",
      "Minibatch total loss at step 9334: 0.157192\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.12490404  0.87509596  0.          1.        ]\n",
      " [ 0.07535997  0.92464     0.          1.        ]]\n",
      "Minibatch total loss at step 9347: 0.215857\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.05670144  0.94329858  0.          1.        ]\n",
      " [ 0.25293267  0.74706727  0.          1.        ]]\n",
      "Minibatch total loss at step 9360: 0.194030\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.99083745  0.00916257  1.          0.        ]\n",
      " [ 0.27958775  0.72041225  0.          1.        ]]\n",
      "Minibatch total loss at step 9373: 0.258998\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.70442772  0.29557228  1.          0.        ]\n",
      " [ 0.76120019  0.23879981  1.          0.        ]]\n",
      "Minibatch total loss at step 9386: 0.174894\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.88555664  0.11444332  1.          0.        ]\n",
      " [ 0.14033459  0.85966539  0.          1.        ]]\n",
      "Minibatch total loss at step 9399: 0.169545\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.24093056  0.75906938  0.          1.        ]\n",
      " [ 0.75202829  0.2479717   1.          0.        ]]\n",
      "Validation accuracy: 55.0\n",
      "Minibatch total loss at step 9412: 0.172491\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.96603549  0.03396443  1.          0.        ]\n",
      " [ 0.1330002   0.8669998   0.          1.        ]]\n",
      "Minibatch total loss at step 9425: 0.207459\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.69970894  0.30029106  1.          0.        ]\n",
      " [ 0.06300282  0.93699718  0.          1.        ]]\n",
      "Minibatch total loss at step 9438: 0.161725\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.87211144  0.12788858  1.          0.        ]\n",
      " [ 0.04203025  0.95796978  0.          1.        ]]\n",
      "Minibatch total loss at step 9451: 0.188424\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.29597309  0.70402688  0.          1.        ]\n",
      " [ 0.97355449  0.02644551  1.          0.        ]]\n",
      "Minibatch total loss at step 9464: 0.183045\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.67569244  0.32430753  1.          0.        ]\n",
      " [ 0.13585432  0.8641457   0.          1.        ]]\n",
      "Minibatch total loss at step 9477: 0.162351\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.24527171  0.75472832  0.          1.        ]\n",
      " [ 0.06314127  0.93685877  0.          1.        ]]\n",
      "Minibatch total loss at step 9490: 0.183797\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.2147842   0.7852158   0.          1.        ]\n",
      " [ 0.14881089  0.85118908  0.          1.        ]]\n",
      "Validation accuracy: 58.5\n",
      "Minibatch total loss at step 9503: 0.151229\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.05722741  0.94277263  0.          1.        ]\n",
      " [ 0.10631211  0.89368784  0.          1.        ]]\n",
      "Minibatch total loss at step 9516: 0.327655\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.98370636  0.01629364  1.          0.        ]\n",
      " [ 0.09070472  0.90929526  0.          1.        ]]\n",
      "Minibatch total loss at step 9529: 0.243983\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.52013546  0.47986454  1.          0.        ]\n",
      " [ 0.22860199  0.77139801  0.          1.        ]]\n",
      "Minibatch total loss at step 9542: 0.180786\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.87988371  0.12011632  1.          0.        ]\n",
      " [ 0.87518758  0.12481247  1.          0.        ]]\n",
      "Minibatch total loss at step 9555: 0.181471\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.05002356  0.94997638  0.          1.        ]\n",
      " [ 0.80018353  0.19981647  1.          0.        ]]\n",
      "Minibatch total loss at step 9568: 0.348461\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.70862865  0.29137132  1.          0.        ]\n",
      " [ 0.0219305   0.97806954  0.          1.        ]]\n",
      "Minibatch total loss at step 9581: 0.165288\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.07939492  0.92060512  0.          1.        ]\n",
      " [ 0.84875739  0.15124258  1.          0.        ]]\n",
      "Minibatch total loss at step 9594: 0.151509\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.05412803  0.94587195  0.          1.        ]\n",
      " [ 0.92360133  0.07639867  1.          0.        ]]\n",
      "Validation accuracy: 50.0\n",
      "Minibatch total loss at step 9607: 0.195955\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.07733665  0.92266333  0.          1.        ]\n",
      " [ 0.87095606  0.12904397  1.          0.        ]]\n",
      "Minibatch total loss at step 9620: 0.152308\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.05707517  0.9429248   0.          1.        ]\n",
      " [ 0.14153641  0.85846359  0.          1.        ]]\n",
      "Minibatch total loss at step 9633: 0.181296\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.0445746   0.95542538  0.          1.        ]\n",
      " [ 0.26444671  0.73555321  0.          1.        ]]\n",
      "Minibatch total loss at step 9646: 0.165123\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.94873601  0.05126398  1.          0.        ]\n",
      " [ 0.28356093  0.71643913  0.          1.        ]]\n",
      "Minibatch total loss at step 9659: 0.159866\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.17359351  0.82640654  0.          1.        ]\n",
      " [ 0.81239301  0.18760701  1.          0.        ]]\n",
      "Minibatch total loss at step 9672: 0.160998\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.16090323  0.83909672  0.          1.        ]\n",
      " [ 0.9067995   0.09320056  1.          0.        ]]\n",
      "Minibatch total loss at step 9685: 0.193372\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.59877133  0.4012287   1.          0.        ]\n",
      " [ 0.05581374  0.94418627  0.          1.        ]]\n",
      "Minibatch total loss at step 9698: 0.203386\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.06198822  0.93801183  0.          1.        ]\n",
      " [ 0.65956694  0.34043303  1.          0.        ]]\n",
      "Validation accuracy: 53.5\n",
      "Minibatch total loss at step 9711: 0.162704\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.89410114  0.10589881  1.          0.        ]\n",
      " [ 0.70598161  0.29401836  1.          0.        ]]\n",
      "Minibatch total loss at step 9724: 0.154568\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.12516199  0.87483799  0.          1.        ]\n",
      " [ 0.06364796  0.93635207  0.          1.        ]]\n",
      "Minibatch total loss at step 9737: 0.181892\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.36319956  0.63680047  0.          1.        ]\n",
      " [ 0.42004764  0.57995242  0.          1.        ]]\n",
      "Minibatch total loss at step 9750: 0.176890\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.06047476  0.93952519  0.          1.        ]\n",
      " [ 0.08704624  0.91295373  0.          1.        ]]\n",
      "Minibatch total loss at step 9763: 0.185741\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.61419207  0.38580796  1.          0.        ]\n",
      " [ 0.05266868  0.94733125  0.          1.        ]]\n",
      "Minibatch total loss at step 9776: 0.260904\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.7514751   0.24852493  1.          0.        ]\n",
      " [ 0.78183258  0.21816741  1.          0.        ]]\n",
      "Minibatch total loss at step 9789: 0.151035\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.18075658  0.81924343  0.          1.        ]\n",
      " [ 0.94447982  0.05552017  1.          0.        ]]\n",
      "Validation accuracy: 53.5\n",
      "Minibatch total loss at step 9802: 0.239264\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.94069403  0.05930599  1.          0.        ]\n",
      " [ 0.09392091  0.90607911  0.          1.        ]]\n",
      "Minibatch total loss at step 9815: 0.148297\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.81129104  0.18870899  1.          0.        ]\n",
      " [ 0.05041736  0.94958264  0.          1.        ]]\n",
      "Minibatch total loss at step 9828: 0.183362\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.33181733  0.66818261  0.          1.        ]\n",
      " [ 0.91018784  0.08981214  1.          0.        ]]\n",
      "Minibatch total loss at step 9841: 0.166509\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.01645672  0.98354328  0.          1.        ]\n",
      " [ 0.89676404  0.10323595  1.          0.        ]]\n",
      "Minibatch total loss at step 9854: 0.190997\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.21139663  0.78860337  0.          1.        ]\n",
      " [ 0.73619807  0.26380193  1.          0.        ]]\n",
      "Minibatch total loss at step 9867: 0.155145\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.0805869   0.91941315  0.          1.        ]\n",
      " [ 0.02627455  0.9737255   0.          1.        ]]\n",
      "Minibatch total loss at step 9880: 0.198452\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.02422984  0.97577012  0.          1.        ]\n",
      " [ 0.89997894  0.10002109  1.          0.        ]]\n",
      "Minibatch total loss at step 9893: 0.194921\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.76418483  0.23581515  1.          0.        ]\n",
      " [ 0.02347308  0.97652686  0.          1.        ]]\n",
      "Validation accuracy: 50.0\n",
      "Minibatch total loss at step 9906: 0.172960\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.14226913  0.85773087  0.          1.        ]\n",
      " [ 0.07859554  0.92140442  0.          1.        ]]\n",
      "Minibatch total loss at step 9919: 0.169976\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.98300803  0.01699195  1.          0.        ]\n",
      " [ 0.08362426  0.91637576  0.          1.        ]]\n",
      "Minibatch total loss at step 9932: 0.363353\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.07984479  0.92015523  0.          1.        ]\n",
      " [ 0.83527416  0.16472593  1.          0.        ]]\n",
      "Minibatch total loss at step 9945: 0.245303\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.36576679  0.63423318  0.          1.        ]\n",
      " [ 0.45489722  0.54510283  0.          1.        ]]\n",
      "Minibatch total loss at step 9958: 0.163258\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.91116571  0.08883426  1.          0.        ]\n",
      " [ 0.32367826  0.67632174  0.          1.        ]]\n",
      "Minibatch total loss at step 9971: 0.194884\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.15352473  0.84647524  0.          1.        ]\n",
      " [ 0.18410757  0.81589246  0.          1.        ]]\n",
      "Minibatch total loss at step 9984: 0.189608\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.1011067   0.8988933   0.          1.        ]\n",
      " [ 0.13841891  0.86158115  0.          1.        ]]\n",
      "Minibatch total loss at step 9997: 0.156655\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.87606055  0.12393948  1.          0.        ]\n",
      " [ 0.10301311  0.8969869   0.          1.        ]]\n",
      "Validation accuracy: 52.0\n",
      "Model saved in file: ./checkpoints/model.ckpt\n",
      "Finished training\n"
     ]
    }
   ],
   "source": [
    "num_steps = 10001\n",
    "\n",
    "trace_file = open('./tracing/timeline.json', 'w')\n",
    "save_path = './checkpoints/model.ckpt'\n",
    "\n",
    "best_loss = 0\n",
    "val_accu = 0\n",
    "best_val_accu = 0\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    tf.initialize_local_variables().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_dataset.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels}\n",
    "        _, loss, trpred, traccu, summary = session.run(\n",
    "            [optimizer, loss, train_predictions, train_accuracy, merged], \n",
    "            feed_dict=feed_dict)\n",
    "        results_writer.add_summary(summary, step)\n",
    "        if (step % 13 == 0):\n",
    "            best_loss = loss if loss < best_loss else best_loss\n",
    "            print('Minibatch total loss at step %d: %f' % (step, loss), 'Best:', best_loss)\n",
    "            print('Minibatch accuracy:', traccu)\n",
    "            print('Predictions | Labels:\\n', np.concatenate((trpred[:2], batch_labels[:2]), axis=1))\n",
    "        if (step % 100 == 0):\n",
    "            val_accu = valid_accuracy.eval()\n",
    "            best_val_accu = val_accu if val_accu < best_val_accu else best_val_accu\n",
    "            print('###-> Validation accuracy:', val_accu, 'Best:', best_val_accu)\n",
    "            \n",
    "    # Save tracing into disl\n",
    "    #trace = timeline.Timeline(step_stats=run_metadata.step_stats)\n",
    "    #trace_file.write(trace.generate_chrome_trace_format(show_memory=True))\n",
    "            \n",
    "    # Save the variables to disk.\n",
    "    saver.save(session, save_path)\n",
    "    print(\"Model saved in file: %s\" % save_path)\n",
    "            \n",
    "    results_writer.flush()\n",
    "    results_writer.close()\n",
    "\n",
    "    print('Finished training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "valid_batch_size = 1\n",
    "\n",
    "def accuracy_notpercent(predictions, labels):\n",
    "  return np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    saver.restore(session, save_path)\n",
    "    print('Model Loaded')\n",
    "    data_split = np.array_split(valid_dataset, valid_dataset.shape[0]//valid_batch_size, axis=0)\n",
    "    labels_split = np.array_split(valid_labels, valid_labels.shape[0]//valid_batch_size, axis=0)\n",
    "    correct_predictions = 0\n",
    "    for idx, batch_data in enumerate(data_split):\n",
    "        correct_predictions += accuracy_notpercent(\n",
    "            train_prediction.eval(feed_dict={tf_train_dataset: batch_data}), \n",
    "            labels_split[idx])\n",
    "        print('accuracy:', (100.0*correct_predictions)/((idx+1)*valid_batch_size))\n",
    "        \n",
    "        \n",
    "    print('Finished validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
