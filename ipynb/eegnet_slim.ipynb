{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import done\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from six.moves import cPickle as pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import timeline\n",
    "\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"import done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load pickled dataset into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unpickling ./data/trainsh1.pickle\n",
      "dataset shape: (30, 240000, 16)\n",
      "labels shape: (30,)\n"
     ]
    }
   ],
   "source": [
    "name_pickle = './data/trainsh1.pickle'\n",
    "\n",
    "with open(name_pickle, 'rb') as f:\n",
    "    print('Unpickling ' + name_pickle)\n",
    "    load = pickle.load(f)\n",
    "    dataset = load['data']\n",
    "    labels = load['labels']\n",
    "    del load\n",
    "    print('dataset shape:', dataset.shape)\n",
    "    print('labels shape:', labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reformat data for training\n",
    "- Divide each file with 240000 samples into smaller batch_samples ~= size of receptive field of eegnet\n",
    "- Keep valid_dataset nr of samples intact for proper validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_array(array):\n",
    "    # Normalize mean=0 and sigma=0.25: axis=0 is along columns, vertical lines.\n",
    "    array -= np.mean(array, axis=0) \n",
    "    array /= 2*np.ptp(array, axis=0)\n",
    "    return array\n",
    "    \n",
    "def clean_normalize_data_labels(data, labels, sigma=0.5):\n",
    "    data_tmp = list()\n",
    "    labels_tmp = list()\n",
    "    for idx, d in enumerate(data):\n",
    "        if (np.count_nonzero(d) < 10) or (np.any(np.std(d, axis=0) < sigma)):\n",
    "            continue\n",
    "        d = normalize_array(d)\n",
    "        data_tmp.append(d)\n",
    "        labels_tmp.append(labels[idx])\n",
    "    return np.asarray(data_tmp), np.asarray(labels_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset shape: (5858, 1, 800, 16) train_labels shape: (5858, 2) mix: 0.537043359508\n",
      "valid_dataset shape: (200, 1, 800, 16) valid_labels shape: (200, 2) mix: 0.505\n"
     ]
    }
   ],
   "source": [
    "#Output size of the layer\n",
    "num_labels = 2\n",
    "\n",
    "#60% for train and 40% for validation\n",
    "split_idx = int(dataset.shape[0]*0.8)\n",
    "#nr of splits\n",
    "nrOfSplits = 300\n",
    "\n",
    "def format_data(data, labels, nr_splits):\n",
    "    shape = data.shape\n",
    "    # reshape [batch, samples, channels] into [batch * samples, channels]\n",
    "    data = np.reshape(data, (shape[0]*shape[1], shape[2]))\n",
    "    # Split 2D array into the desired smaller chuncks\n",
    "    data = np.asarray(np.split(data, shape[0]*nr_splits, axis=0))\n",
    "    # labels are obtained by repeating original labels nr_splits times\n",
    "    labels = np.repeat((np.arange(num_labels) == labels[:,None]).astype(np.float32), nr_splits, axis=0)\n",
    "    # normalize and eliminate batches that only contain drop-outs\n",
    "    data, labels = clean_normalize_data_labels(data, labels, 0.01)\n",
    "    # data has to be 4D for tensorflow (insert an empty dimension)\n",
    "    data = data[:,None,:,:]\n",
    "    # shuffle data and labels mantaining relation between them. Important after the small batches.\n",
    "    shuffle_idx = np.random.permutation(data.shape[0])\n",
    "    data = data[shuffle_idx,:,:,:]\n",
    "    labels = labels[shuffle_idx]\n",
    "    return data, labels\n",
    "\n",
    "# shuffle file data\n",
    "shuffle_idx = np.random.permutation(dataset.shape[0])\n",
    "dataset = dataset[shuffle_idx,:,:]\n",
    "labels = labels[shuffle_idx]\n",
    "# format and split data into smaller chunks\n",
    "train_dataset, train_labels = format_data(dataset[:split_idx], labels[:split_idx], nrOfSplits)\n",
    "valid_dataset, valid_labels = format_data(dataset[split_idx:-1], labels[split_idx:-1], nrOfSplits)\n",
    "del dataset, labels\n",
    "\n",
    "valid_dataset = valid_dataset[:200]\n",
    "valid_labels = valid_labels[:200]\n",
    "\n",
    "print('train_dataset shape:', train_dataset.shape, 'train_labels shape:', train_labels.shape, \n",
    "      'mix:', float(np.count_nonzero(train_labels[:,1], axis=0))/train_labels.shape[0])\n",
    "print('valid_dataset shape:', valid_dataset.shape, 'valid_labels shape:', valid_labels.shape, \n",
    "      'mix:', float(np.count_nonzero(valid_labels[:,1], axis=0))/valid_labels.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot some data to have an idea of how data looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fe70a2e86d0>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhkAAAC7CAYAAADIUYxHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhmEYqWBOhmEYhmEYqfAfrgngK0QdAPAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe70c38b790>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplot(211)\n",
    "plt.plot(train_dataset[1,0,:,0])\n",
    "plt.plot(train_dataset[3,0,:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quantize signal\n",
    "\n",
    "Based on wavenet paper, raw signal is first mu-law transformed and then quantized to 256 possible values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset shape: (5858, 1, 800, 16)\n",
      "valid_dataset shape: (200, 1, 800, 16)\n"
     ]
    }
   ],
   "source": [
    "quantization_levels = 256\n",
    "\n",
    "def quantize_data(data, levels):\n",
    "    levels -= 1\n",
    "    # mu law companding transformation\n",
    "    data = np.sign(data) * (np.log(1.0 + levels * np.absolute(data)) / np.log(1.0 + levels))\n",
    "    data = normalize_array(data)\n",
    "    ## Quantization to int: [(make data > 0) * levels] -> cast uin8\n",
    "    #data += np.absolute(np.amin(data, axis=0))\n",
    "    #data /= np.amax(data, axis=0)\n",
    "    #data = (data*levels).astype(np.int32)\n",
    "    return data\n",
    "\n",
    "train_dataset = quantize_data(train_dataset, quantization_levels)\n",
    "valid_dataset = quantize_data(valid_dataset, quantization_levels)\n",
    "\n",
    "print('train_dataset shape:', train_dataset.shape)\n",
    "print('valid_dataset shape:', valid_dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot data after quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fe70a26cfd0>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhkAAAC7CAYAAADIUYxHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe6dab99b10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplot(211)\n",
    "plt.plot(train_dataset[1,0,:,0])\n",
    "plt.plot(train_dataset[3,0,:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EEGNET implementation\n",
    "\n",
    "Ideas\n",
    "  - Condition classification based on sensor?\n",
    "  \n",
    "Part of https://arxiv.org/pdf/1609.03499.pdf that most concerns classification:\n",
    "\"As a last experiment we looked at speech recognition with WaveNets on the TIMIT (Garofolo et al., 1993) dataset. For this task we added a mean-pooling layer after the dilation convolutions that aggregated the activations to coarser frames spanning 10 milliseconds (160 x downsampling). The pooling layer was followed by a few non-causal convolutions. We trained WaveNet with two loss terms, one to predict the next sample and one to classify the frame, the model generalized better than with a single loss and achieved 18.8 PER on the test set, which is to our knowledge the best score obtained from a model trained directly on raw audio on TIMIT.\"\n",
    "\n",
    "Look into: http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43290.pdf\n",
    "\"Input: This layer extracts 275 ms waveform segments from each of M input microphones. Successive inputs are hopped by 10ms. At the 16kHz sampling rate used in our experiments each segment contains M X 4401 dimensions.\"\n",
    "...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computational graph created\n"
     ]
    }
   ],
   "source": [
    "#How many files are supplied per batch.\n",
    "batch_size=16\n",
    "#Number of samples in each batch entry\n",
    "batch_samples=train_dataset.shape[2]\n",
    "#How many filters to learn for the input.\n",
    "input_channels=16\n",
    "#How many filters to learn for the residual.\n",
    "residual_channels=2*input_channels\n",
    "# size after pooling layer\n",
    "pool_size = 600\n",
    "#number of steps after which learning rate is decayed\n",
    "decay_steps=500\n",
    "\n",
    "filter_width=3\n",
    "\n",
    "slim = tf.contrib.slim\n",
    "\n",
    "#Construct computation graph\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    \n",
    "    # Input data\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, 1, batch_samples, input_channels))\n",
    "    tf_train_labels = tf.placeholder(tf.uint8, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset, dtype=tf.float32)\n",
    "    tf_valid_labels = tf.constant(valid_labels, dtype=tf.float32)\n",
    "    \n",
    "    def accuracy(logits, labels):\n",
    "        return tf.div(\n",
    "            tf.mul(\n",
    "                tf.to_float(\n",
    "                tf.reduce_sum(tf.to_int32(tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))))\n",
    "                ), \n",
    "                100), \n",
    "                tf.to_float(tf.shape(logits)[0]))\n",
    "    \n",
    "    def network(batch_data, reuse=False, is_training=True):\n",
    "        with tf.variable_scope('eegnet_network', reuse=reuse):\n",
    "            with slim.arg_scope([slim.batch_norm], \n",
    "                                is_training=is_training):\n",
    "                with slim.arg_scope([slim.conv2d, slim.fully_connected], \n",
    "                                    weights_initializer=slim.xavier_initializer(), \n",
    "                                    normalizer_fn=slim.batch_norm):\n",
    "                    with tf.variable_scope('input_layer'):\n",
    "                        hidden = slim.conv2d(batch_data, residual_channels, [1, filter_width], stride=1, rate=1, \n",
    "                                             activation_fn=None, scope='conv1')\n",
    "\n",
    "                    with tf.variable_scope('hidden'):\n",
    "                        with tf.variable_scope('layer1'):\n",
    "                            layer_input = hidden\n",
    "                            hidden = slim.conv2d(hidden, 2*residual_channels, [1, filter_width], stride=1, rate=2, \n",
    "                                                 activation_fn=None, scope='dilconv')\n",
    "                            filtr, gate = tf.split(3, 2, hidden) # split features in half\n",
    "                            hidden = tf.mul(tf.tanh(filtr), tf.sigmoid(gate), name='filterXgate')\n",
    "                            hidden = slim.conv2d(hidden, residual_channels, 1, activation_fn=None, scope='1x1skip')\n",
    "                            skip = hidden # skip conn\n",
    "                            hidden = tf.add(hidden, layer_input) # residual conn\n",
    "                        with tf.variable_scope('layer2'):\n",
    "                            layer_input = hidden\n",
    "                            hidden = slim.conv2d(hidden, 2*residual_channels, [1, filter_width], stride=1, rate=4, \n",
    "                                                 activation_fn=None, scope='dilconv')\n",
    "                            filtr, gate = tf.split(3, 2, hidden) # split features in half\n",
    "                            hidden = tf.mul(tf.tanh(filtr), tf.sigmoid(gate), name='filterXgate')\n",
    "                            hidden = slim.conv2d(hidden, residual_channels, 1, activation_fn=None, scope='1x1skip')\n",
    "                            skip = tf.add(skip, hidden) # skip conn\n",
    "                            hidden = tf.add(hidden, layer_input) # residual conn\n",
    "                        with tf.variable_scope('layer3'):\n",
    "                            hidden = slim.conv2d(hidden, 2*residual_channels, [1, filter_width], stride=1, rate=4, \n",
    "                                                 activation_fn=None, scope='dilconv')\n",
    "                            filtr, gate = tf.split(3, 2, hidden) # split features in half\n",
    "                            hidden = tf.mul(tf.tanh(filtr), tf.sigmoid(gate), name='filterXgate')\n",
    "                            hidden = slim.conv2d(hidden, residual_channels, 1, activation_fn=None, scope='1x1skip')\n",
    "                            skip = tf.add(skip, hidden) # skip conn\n",
    "                        \n",
    "                    with tf.variable_scope('skip_processing'):\n",
    "                        hidden = tf.nn.relu(skip)\n",
    "                        hidden = slim.conv2d(skip, 4, 1, activation_fn=tf.nn.relu, scope='1x1conv1')\n",
    "\n",
    "                    with tf.variable_scope('classification'):                        \n",
    "                        hidden = slim.avg_pool2d(hidden, [1, batch_samples*2//pool_size], [1, batch_samples//pool_size])\n",
    "                        shape = hidden.get_shape().as_list()\n",
    "                        hidden = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "                        classf = slim.fully_connected(hidden, num_labels, activation_fn=None, scope='fc1')\n",
    "        return classf \n",
    "\n",
    "    with tf.name_scope('eegnet_handling'):\n",
    "        with tf.name_scope('network'):\n",
    "            classification = network(tf_train_dataset)\n",
    "        with tf.name_scope('classification_loss'):\n",
    "            loss_class = slim.losses.softmax_cross_entropy(classification, tf_train_labels, scope='classification_loss')\n",
    "            tf.scalar_summary('classification_loss', loss_class)\n",
    "        with tf.name_scope('total_loss'):\n",
    "            total_loss = slim.losses.get_total_loss(add_regularization_losses=False)\n",
    "            tf.scalar_summary('total_loss', total_loss)\n",
    "        with tf.name_scope('optimizer'):\n",
    "            #global_step = tf.Variable(0)\n",
    "            #learning_rate = tf.train.exponential_decay(0.3, global_step, decay_steps, 0.96, staircase=True)\n",
    "            #optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(total_loss, global_step=global_step)\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=1e-3, epsilon=1e-4).minimize(\n",
    "                total_loss, var_list=tf.trainable_variables())\n",
    "        with tf.name_scope('accuracy'):\n",
    "            train_predictions = tf.nn.softmax(classification)\n",
    "            valid_predictions = tf.nn.softmax(network(tf_valid_dataset, True, True))\n",
    "            train_accuracy = accuracy(train_predictions, tf_train_labels)\n",
    "            valid_accuracy = accuracy(valid_predictions, tf_valid_labels)\n",
    "\n",
    "    # Add histograms for trainable variables.\n",
    "    for var in tf.trainable_variables():\n",
    "        tf.histogram_summary(var.op.name, var)\n",
    "        \n",
    "    # Add summaries for activations: NOT WORKING YET. TF ERROR.\n",
    "    #slim.summarize_activations()\n",
    "    \n",
    "    #Merge all summaries and write to a folder\n",
    "    merged = tf.merge_all_summaries()\n",
    "    results_writer = tf.train.SummaryWriter('./results', graph)\n",
    "    \n",
    "    # Add ops to save and restore all the variables.\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    #tracing for timeline\n",
    "    run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "    run_metadata = tf.RunMetadata()    \n",
    "    \n",
    "print('computational graph created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch total loss at step 0: 0.584093\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.33099234  0.66900766  0.          1.        ]\n",
      " [ 0.41981995  0.58018005  0.          1.        ]]\n",
      "Validation accuracy: 49.0\n",
      "Minibatch total loss at step 13: 1.113494\n",
      "Minibatch accuracy: 37.5\n",
      "Predictions | Labels:\n",
      " [[ 0.22416088  0.77583909  1.          0.        ]\n",
      " [ 0.93093109  0.06906889  1.          0.        ]]\n",
      "Minibatch total loss at step 26: 1.195611\n",
      "Minibatch accuracy: 18.75\n",
      "Predictions | Labels:\n",
      " [[ 0.49082777  0.50917226  0.          1.        ]\n",
      " [ 0.30960801  0.69039202  0.          1.        ]]\n",
      "Minibatch total loss at step 39: 0.692591\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.53944188  0.46055809  0.          1.        ]\n",
      " [ 0.60131288  0.39868712  1.          0.        ]]\n",
      "Minibatch total loss at step 52: 0.547819\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.40680766  0.59319234  0.          1.        ]\n",
      " [ 0.74785125  0.25214875  1.          0.        ]]\n",
      "Minibatch total loss at step 65: 0.849392\n",
      "Minibatch accuracy: 43.75\n",
      "Predictions | Labels:\n",
      " [[ 0.17208561  0.82791436  1.          0.        ]\n",
      " [ 0.28213191  0.71786815  1.          0.        ]]\n",
      "Minibatch total loss at step 78: 0.966188\n",
      "Minibatch accuracy: 37.5\n",
      "Predictions | Labels:\n",
      " [[ 0.59859991  0.40140009  0.          1.        ]\n",
      " [ 0.89077753  0.10922249  0.          1.        ]]\n",
      "Minibatch total loss at step 91: 0.665394\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.44798052  0.55201954  0.          1.        ]\n",
      " [ 0.58884257  0.41115743  1.          0.        ]]\n",
      "Validation accuracy: 62.5\n",
      "Minibatch total loss at step 104: 0.673604\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5034498   0.49655026  1.          0.        ]\n",
      " [ 0.60504317  0.3949568   0.          1.        ]]\n",
      "Minibatch total loss at step 117: 0.797145\n",
      "Minibatch accuracy: 37.5\n",
      "Predictions | Labels:\n",
      " [[ 0.66282237  0.33717769  0.          1.        ]\n",
      " [ 0.44845039  0.55154967  1.          0.        ]]\n",
      "Minibatch total loss at step 130: 0.658411\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.51908696  0.48091301  1.          0.        ]\n",
      " [ 0.67351586  0.32648405  1.          0.        ]]\n",
      "Minibatch total loss at step 143: 0.643679\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.45326665  0.54673338  0.          1.        ]\n",
      " [ 0.41406253  0.5859375   0.          1.        ]]\n",
      "Minibatch total loss at step 156: 0.636484\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.54761678  0.45238322  0.          1.        ]\n",
      " [ 0.4352496   0.56475037  0.          1.        ]]\n",
      "Minibatch total loss at step 169: 0.662442\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.56084746  0.4391526   0.          1.        ]\n",
      " [ 0.58464801  0.41535199  0.          1.        ]]\n",
      "Minibatch total loss at step 182: 0.693989\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.7387436   0.2612564   1.          0.        ]\n",
      " [ 0.56498402  0.43501598  1.          0.        ]]\n",
      "Minibatch total loss at step 195: 0.682262\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.41847888  0.58152115  0.          1.        ]\n",
      " [ 0.6808998   0.31910023  1.          0.        ]]\n",
      "Validation accuracy: 63.0\n",
      "Minibatch total loss at step 208: 0.638558\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.41742837  0.58257169  0.          1.        ]\n",
      " [ 0.68494666  0.31505334  1.          0.        ]]\n",
      "Minibatch total loss at step 221: 0.771338\n",
      "Minibatch accuracy: 37.5\n",
      "Predictions | Labels:\n",
      " [[ 0.37786588  0.62213409  1.          0.        ]\n",
      " [ 0.62565649  0.37434354  0.          1.        ]]\n",
      "Minibatch total loss at step 234: 0.667387\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.53843987  0.46156016  1.          0.        ]\n",
      " [ 0.68606544  0.31393453  1.          0.        ]]\n",
      "Minibatch total loss at step 247: 0.619802\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.26012278  0.73987722  0.          1.        ]\n",
      " [ 0.46320435  0.53679568  1.          0.        ]]\n",
      "Minibatch total loss at step 260: 0.565876\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.55523622  0.44476381  0.          1.        ]\n",
      " [ 0.24506363  0.75493634  1.          0.        ]]\n",
      "Minibatch total loss at step 273: 0.706588\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.49892604  0.50107402  0.          1.        ]\n",
      " [ 0.46183774  0.53816229  0.          1.        ]]\n",
      "Minibatch total loss at step 286: 0.661307\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.32225928  0.67774069  1.          0.        ]\n",
      " [ 0.34094715  0.65905285  0.          1.        ]]\n",
      "Minibatch total loss at step 299: 0.588979\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.45883429  0.54116577  0.          1.        ]\n",
      " [ 0.57102984  0.42897019  1.          0.        ]]\n",
      "Validation accuracy: 73.0\n",
      "Minibatch total loss at step 312: 0.733761\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.39900956  0.60099047  0.          1.        ]\n",
      " [ 0.22248968  0.77751034  1.          0.        ]]\n",
      "Minibatch total loss at step 325: 0.715393\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.40078568  0.59921438  0.          1.        ]\n",
      " [ 0.55696356  0.44303641  1.          0.        ]]\n",
      "Minibatch total loss at step 338: 0.573513\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.51282406  0.48717594  1.          0.        ]\n",
      " [ 0.6398384   0.36016163  1.          0.        ]]\n",
      "Minibatch total loss at step 351: 0.648653\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.69017106  0.30982897  1.          0.        ]\n",
      " [ 0.57855356  0.42144647  1.          0.        ]]\n",
      "Minibatch total loss at step 364: 0.593761\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.63146186  0.36853817  1.          0.        ]\n",
      " [ 0.5449385   0.45506147  0.          1.        ]]\n",
      "Minibatch total loss at step 377: 0.698183\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.52966285  0.47033721  1.          0.        ]\n",
      " [ 0.34007877  0.65992117  1.          0.        ]]\n",
      "Minibatch total loss at step 390: 0.827194\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.42102605  0.57897395  1.          0.        ]\n",
      " [ 0.10696904  0.893031    1.          0.        ]]\n",
      "Validation accuracy: 73.5\n",
      "Minibatch total loss at step 403: 0.761301\n",
      "Minibatch accuracy: 43.75\n",
      "Predictions | Labels:\n",
      " [[ 0.33411941  0.66588062  0.          1.        ]\n",
      " [ 0.24671036  0.75328964  0.          1.        ]]\n",
      "Minibatch total loss at step 416: 0.598126\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.44061056  0.55938941  0.          1.        ]\n",
      " [ 0.47631192  0.52368814  0.          1.        ]]\n",
      "Minibatch total loss at step 429: 0.660610\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.17327568  0.82672435  0.          1.        ]\n",
      " [ 0.48954248  0.51045752  0.          1.        ]]\n",
      "Minibatch total loss at step 442: 0.506335\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.78875011  0.21124987  1.          0.        ]\n",
      " [ 0.47741246  0.52258754  0.          1.        ]]\n",
      "Minibatch total loss at step 455: 0.562299\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.46239582  0.53760409  0.          1.        ]\n",
      " [ 0.55628806  0.443712    1.          0.        ]]\n",
      "Minibatch total loss at step 468: 0.606626\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.55189896  0.44810104  1.          0.        ]\n",
      " [ 0.76262778  0.23737222  1.          0.        ]]\n",
      "Minibatch total loss at step 481: 0.515360\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.55392724  0.44607279  0.          1.        ]\n",
      " [ 0.16728637  0.8327136   0.          1.        ]]\n",
      "Minibatch total loss at step 494: 0.714922\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.26313791  0.73686212  1.          0.        ]\n",
      " [ 0.67639863  0.32360139  0.          1.        ]]\n",
      "Validation accuracy: 79.5\n",
      "Minibatch total loss at step 507: 0.701912\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.21788535  0.78211462  0.          1.        ]\n",
      " [ 0.68257278  0.31742719  1.          0.        ]]\n",
      "Minibatch total loss at step 520: 0.595496\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.2938489   0.70615107  1.          0.        ]\n",
      " [ 0.44644096  0.55355901  0.          1.        ]]\n",
      "Minibatch total loss at step 533: 0.549597\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.44119868  0.55880129  1.          0.        ]\n",
      " [ 0.09587598  0.90412402  0.          1.        ]]\n",
      "Minibatch total loss at step 546: 0.661830\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.29979786  0.70020217  0.          1.        ]\n",
      " [ 0.28380445  0.71619552  1.          0.        ]]\n",
      "Minibatch total loss at step 559: 0.772215\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.67842609  0.32157394  1.          0.        ]\n",
      " [ 0.26391935  0.73608059  1.          0.        ]]\n",
      "Minibatch total loss at step 572: 0.578156\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.30326998  0.69672996  0.          1.        ]\n",
      " [ 0.9005999   0.09940013  1.          0.        ]]\n",
      "Minibatch total loss at step 585: 0.565015\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.50338304  0.49661693  1.          0.        ]\n",
      " [ 0.82687205  0.17312789  1.          0.        ]]\n",
      "Minibatch total loss at step 598: 0.608377\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5637306   0.43626937  0.          1.        ]\n",
      " [ 0.19313791  0.80686212  0.          1.        ]]\n",
      "Validation accuracy: 84.5\n",
      "Minibatch total loss at step 611: 0.486763\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.63429189  0.36570805  1.          0.        ]\n",
      " [ 0.82795525  0.17204475  1.          0.        ]]\n",
      "Minibatch total loss at step 624: 0.637194\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.65564233  0.34435761  0.          1.        ]\n",
      " [ 0.23530534  0.76469469  0.          1.        ]]\n",
      "Minibatch total loss at step 637: 0.621112\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.40474445  0.59525555  1.          0.        ]\n",
      " [ 0.68489462  0.31510535  1.          0.        ]]\n",
      "Minibatch total loss at step 650: 0.735107\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.4848246   0.51517546  1.          0.        ]\n",
      " [ 0.48812491  0.51187509  1.          0.        ]]\n",
      "Minibatch total loss at step 663: 0.612783\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.41814837  0.58185166  0.          1.        ]\n",
      " [ 0.57523626  0.42476371  1.          0.        ]]\n",
      "Minibatch total loss at step 676: 0.763064\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.54854703  0.45145297  1.          0.        ]\n",
      " [ 0.34094504  0.65905493  0.          1.        ]]\n",
      "Minibatch total loss at step 689: 0.777595\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.11036202  0.88963795  0.          1.        ]\n",
      " [ 0.15178005  0.84821987  1.          0.        ]]\n",
      "Validation accuracy: 84.5\n",
      "Minibatch total loss at step 702: 0.549430\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.58394831  0.41605169  0.          1.        ]\n",
      " [ 0.72849476  0.27150521  1.          0.        ]]\n",
      "Minibatch total loss at step 715: 0.675270\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.41319948  0.58680052  0.          1.        ]\n",
      " [ 0.62991935  0.37008062  1.          0.        ]]\n",
      "Minibatch total loss at step 728: 0.546180\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.40285429  0.59714568  0.          1.        ]\n",
      " [ 0.62891012  0.37108985  1.          0.        ]]\n",
      "Minibatch total loss at step 741: 0.475306\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.80822611  0.19177385  1.          0.        ]\n",
      " [ 0.4347859   0.5652141   1.          0.        ]]\n",
      "Minibatch total loss at step 754: 0.525578\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.13522437  0.8647756   0.          1.        ]\n",
      " [ 0.12551154  0.87448853  1.          0.        ]]\n",
      "Minibatch total loss at step 767: 0.511282\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.30006585  0.69993418  0.          1.        ]\n",
      " [ 0.64079309  0.35920694  0.          1.        ]]\n",
      "Minibatch total loss at step 780: 0.387144\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.78736597  0.21263409  1.          0.        ]\n",
      " [ 0.23191431  0.76808572  0.          1.        ]]\n",
      "Minibatch total loss at step 793: 0.801053\n",
      "Minibatch accuracy: 43.75\n",
      "Predictions | Labels:\n",
      " [[ 0.79331595  0.20668404  0.          1.        ]\n",
      " [ 0.21980847  0.78019148  1.          0.        ]]\n",
      "Validation accuracy: 82.5\n",
      "Minibatch total loss at step 806: 0.610753\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.569686    0.430314    0.          1.        ]\n",
      " [ 0.11168236  0.88831764  0.          1.        ]]\n",
      "Minibatch total loss at step 819: 0.543698\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.06156521  0.93843484  0.          1.        ]\n",
      " [ 0.62284315  0.37715682  1.          0.        ]]\n",
      "Minibatch total loss at step 832: 0.471825\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.18040729  0.81959265  0.          1.        ]\n",
      " [ 0.35879701  0.64120299  0.          1.        ]]\n",
      "Minibatch total loss at step 845: 0.596868\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5734309   0.42656913  1.          0.        ]\n",
      " [ 0.45660034  0.54339969  0.          1.        ]]\n",
      "Minibatch total loss at step 858: 0.646005\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.631392    0.36860797  1.          0.        ]\n",
      " [ 0.69100535  0.30899465  1.          0.        ]]\n",
      "Minibatch total loss at step 871: 0.685551\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.44324359  0.55675644  0.          1.        ]\n",
      " [ 0.40505198  0.59494793  1.          0.        ]]\n",
      "Minibatch total loss at step 884: 0.687016\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.43488264  0.56511736  0.          1.        ]\n",
      " [ 0.45725656  0.54274338  1.          0.        ]]\n",
      "Minibatch total loss at step 897: 0.385193\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.70741946  0.29258057  1.          0.        ]\n",
      " [ 0.5401333   0.45986673  0.          1.        ]]\n",
      "Validation accuracy: 81.5\n",
      "Minibatch total loss at step 910: 0.664639\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.33041674  0.66958326  1.          0.        ]\n",
      " [ 0.84634185  0.15365812  1.          0.        ]]\n",
      "Minibatch total loss at step 923: 0.620742\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.74490452  0.25509551  0.          1.        ]\n",
      " [ 0.87157321  0.12842679  1.          0.        ]]\n",
      "Minibatch total loss at step 936: 0.816463\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.06781092  0.93218905  1.          0.        ]\n",
      " [ 0.69617987  0.30382016  1.          0.        ]]\n",
      "Minibatch total loss at step 949: 0.719741\n",
      "Minibatch accuracy: 43.75\n",
      "Predictions | Labels:\n",
      " [[ 0.48695466  0.51304537  0.          1.        ]\n",
      " [ 0.51438093  0.48561901  0.          1.        ]]\n",
      "Minibatch total loss at step 962: 0.576890\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.17437556  0.82562441  0.          1.        ]\n",
      " [ 0.40294957  0.59705037  0.          1.        ]]\n",
      "Minibatch total loss at step 975: 0.632216\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.11624759  0.88375241  0.          1.        ]\n",
      " [ 0.36772698  0.63227302  0.          1.        ]]\n",
      "Minibatch total loss at step 988: 0.456588\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.8868283   0.11317176  1.          0.        ]\n",
      " [ 0.61812741  0.38187259  1.          0.        ]]\n",
      "Validation accuracy: 84.0\n",
      "Minibatch total loss at step 1001: 0.405656\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.69243109  0.30756894  1.          0.        ]\n",
      " [ 0.40351319  0.59648681  0.          1.        ]]\n",
      "Minibatch total loss at step 1014: 0.749072\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.08587325  0.91412669  1.          0.        ]\n",
      " [ 0.37601405  0.62398595  0.          1.        ]]\n",
      "Minibatch total loss at step 1027: 0.499975\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.91121334  0.08878671  1.          0.        ]\n",
      " [ 0.14270382  0.85729617  0.          1.        ]]\n",
      "Minibatch total loss at step 1040: 0.538792\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.68852645  0.31147358  0.          1.        ]\n",
      " [ 0.39702255  0.60297745  0.          1.        ]]\n",
      "Minibatch total loss at step 1053: 0.586438\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.81818014  0.18181989  1.          0.        ]\n",
      " [ 0.71814984  0.28185013  1.          0.        ]]\n",
      "Minibatch total loss at step 1066: 0.733615\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.11459334  0.88540661  0.          1.        ]\n",
      " [ 0.27517846  0.72482157  0.          1.        ]]\n",
      "Minibatch total loss at step 1079: 0.593712\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.45587999  0.54411995  1.          0.        ]\n",
      " [ 0.36572495  0.63427502  1.          0.        ]]\n",
      "Minibatch total loss at step 1092: 0.779079\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.94668895  0.05331107  0.          1.        ]\n",
      " [ 0.2340415   0.76595855  0.          1.        ]]\n",
      "Validation accuracy: 77.5\n",
      "Minibatch total loss at step 1105: 0.471936\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.2172268   0.7827732   0.          1.        ]\n",
      " [ 0.37087968  0.62912035  0.          1.        ]]\n",
      "Minibatch total loss at step 1118: 0.504983\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.61859024  0.38140979  0.          1.        ]\n",
      " [ 0.38888374  0.61111629  0.          1.        ]]\n",
      "Minibatch total loss at step 1131: 0.610374\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.15190235  0.84809762  1.          0.        ]\n",
      " [ 0.37757954  0.62242043  0.          1.        ]]\n",
      "Minibatch total loss at step 1144: 0.721569\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.83956349  0.16043653  1.          0.        ]\n",
      " [ 0.33648983  0.6635102   0.          1.        ]]\n",
      "Minibatch total loss at step 1157: 0.482090\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.9769569   0.02304315  1.          0.        ]\n",
      " [ 0.10417445  0.89582556  0.          1.        ]]\n",
      "Minibatch total loss at step 1170: 0.476109\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.22153847  0.77846152  0.          1.        ]\n",
      " [ 0.65698028  0.34301975  1.          0.        ]]\n",
      "Minibatch total loss at step 1183: 0.572704\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.61206859  0.38793147  1.          0.        ]\n",
      " [ 0.793037    0.20696302  1.          0.        ]]\n",
      "Minibatch total loss at step 1196: 0.398586\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.78881598  0.21118404  1.          0.        ]\n",
      " [ 0.28095227  0.71904773  0.          1.        ]]\n",
      "Validation accuracy: 85.0\n",
      "Minibatch total loss at step 1209: 0.558711\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.84349477  0.15650524  1.          0.        ]\n",
      " [ 0.02544127  0.97455877  0.          1.        ]]\n",
      "Minibatch total loss at step 1222: 0.523969\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.55296952  0.44703051  0.          1.        ]\n",
      " [ 0.32764298  0.67235702  0.          1.        ]]\n",
      "Minibatch total loss at step 1235: 0.965933\n",
      "Minibatch accuracy: 43.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5996148   0.4003852   0.          1.        ]\n",
      " [ 0.47822511  0.52177489  1.          0.        ]]\n",
      "Minibatch total loss at step 1248: 0.813867\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.48754719  0.51245284  0.          1.        ]\n",
      " [ 0.44853267  0.55146736  1.          0.        ]]\n",
      "Minibatch total loss at step 1261: 0.422310\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.94346207  0.05653794  1.          0.        ]\n",
      " [ 0.41017324  0.58982676  0.          1.        ]]\n",
      "Minibatch total loss at step 1274: 0.458937\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.78111207  0.21888791  1.          0.        ]\n",
      " [ 0.68587422  0.31412575  0.          1.        ]]\n",
      "Minibatch total loss at step 1287: 0.493406\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.74796474  0.25203523  1.          0.        ]\n",
      " [ 0.39540461  0.60459536  0.          1.        ]]\n",
      "Minibatch total loss at step 1300: 0.591617\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.48320827  0.5167917   1.          0.        ]\n",
      " [ 0.86799741  0.13200259  0.          1.        ]]\n",
      "Validation accuracy: 82.5\n",
      "Minibatch total loss at step 1313: 0.800916\n",
      "Minibatch accuracy: 37.5\n",
      "Predictions | Labels:\n",
      " [[ 0.3968153   0.60318464  1.          0.        ]\n",
      " [ 0.09243894  0.90756106  0.          1.        ]]\n",
      "Minibatch total loss at step 1326: 0.813579\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.71525657  0.28474343  1.          0.        ]\n",
      " [ 0.34767553  0.65232444  1.          0.        ]]\n",
      "Minibatch total loss at step 1339: 0.545655\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.23216343  0.76783651  1.          0.        ]\n",
      " [ 0.0142282   0.98577183  0.          1.        ]]\n",
      "Minibatch total loss at step 1352: 0.413654\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.42028138  0.57971865  0.          1.        ]\n",
      " [ 0.20426251  0.7957375   0.          1.        ]]\n",
      "Minibatch total loss at step 1365: 0.533359\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.53383505  0.46616498  0.          1.        ]\n",
      " [ 0.20016791  0.79983211  1.          0.        ]]\n",
      "Minibatch total loss at step 1378: 0.623408\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.68507576  0.31492427  0.          1.        ]\n",
      " [ 0.3182753   0.68172461  0.          1.        ]]\n",
      "Minibatch total loss at step 1391: 0.494133\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.69593996  0.30406007  1.          0.        ]\n",
      " [ 0.94357914  0.05642086  1.          0.        ]]\n",
      "Validation accuracy: 84.5\n",
      "Minibatch total loss at step 1404: 0.366145\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.45999518  0.54000479  1.          0.        ]\n",
      " [ 0.06121567  0.93878436  0.          1.        ]]\n",
      "Minibatch total loss at step 1417: 0.619259\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.05893676  0.94106328  0.          1.        ]\n",
      " [ 0.09569359  0.90430647  0.          1.        ]]\n",
      "Minibatch total loss at step 1430: 0.351237\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.13907458  0.86092544  0.          1.        ]\n",
      " [ 0.51731855  0.48268142  1.          0.        ]]\n",
      "Minibatch total loss at step 1443: 0.557077\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5361324   0.46386766  1.          0.        ]\n",
      " [ 0.67273873  0.32726124  1.          0.        ]]\n",
      "Minibatch total loss at step 1456: 0.481757\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.45649561  0.54350442  1.          0.        ]\n",
      " [ 0.27916121  0.72083873  1.          0.        ]]\n",
      "Minibatch total loss at step 1469: 0.706928\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.6850943   0.31490576  0.          1.        ]\n",
      " [ 0.14182548  0.85817456  1.          0.        ]]\n",
      "Minibatch total loss at step 1482: 0.557877\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.07088213  0.92911792  1.          0.        ]\n",
      " [ 0.49794978  0.50205022  1.          0.        ]]\n",
      "Minibatch total loss at step 1495: 0.567427\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.60569882  0.39430121  1.          0.        ]\n",
      " [ 0.38254109  0.61745894  0.          1.        ]]\n",
      "Validation accuracy: 77.0\n",
      "Minibatch total loss at step 1508: 0.601265\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.19596003  0.8040399   0.          1.        ]\n",
      " [ 0.20831007  0.79168993  0.          1.        ]]\n",
      "Minibatch total loss at step 1521: 0.530734\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.29341757  0.70658243  0.          1.        ]\n",
      " [ 0.96778727  0.03221266  1.          0.        ]]\n",
      "Minibatch total loss at step 1534: 0.582223\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.66285557  0.3371444   0.          1.        ]\n",
      " [ 0.30911043  0.69088954  1.          0.        ]]\n",
      "Minibatch total loss at step 1547: 0.571443\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.90941572  0.0905843   1.          0.        ]\n",
      " [ 0.57332623  0.42667374  1.          0.        ]]\n",
      "Minibatch total loss at step 1560: 0.558052\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.79101706  0.20898291  1.          0.        ]\n",
      " [ 0.34277919  0.65722078  0.          1.        ]]\n",
      "Minibatch total loss at step 1573: 0.445693\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.68132168  0.31867838  1.          0.        ]\n",
      " [ 0.37707639  0.62292361  0.          1.        ]]\n",
      "Minibatch total loss at step 1586: 0.675086\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.80697316  0.1930268   0.          1.        ]\n",
      " [ 0.35972756  0.64027244  1.          0.        ]]\n",
      "Minibatch total loss at step 1599: 0.380369\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.2216343   0.77836573  0.          1.        ]\n",
      " [ 0.23377171  0.76622832  0.          1.        ]]\n",
      "Validation accuracy: 78.5\n",
      "Minibatch total loss at step 1612: 0.546552\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.84785569  0.15214425  1.          0.        ]\n",
      " [ 0.68780887  0.3121911   1.          0.        ]]\n",
      "Minibatch total loss at step 1625: 0.627872\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.25978345  0.74021655  1.          0.        ]\n",
      " [ 0.35432491  0.64567506  1.          0.        ]]\n",
      "Minibatch total loss at step 1638: 0.494511\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.45531633  0.54468364  0.          1.        ]\n",
      " [ 0.46063945  0.53936058  0.          1.        ]]\n",
      "Minibatch total loss at step 1651: 0.417441\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.01428436  0.98571569  0.          1.        ]\n",
      " [ 0.23043704  0.76956296  0.          1.        ]]\n",
      "Minibatch total loss at step 1664: 0.472588\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.36935484  0.63064516  0.          1.        ]\n",
      " [ 0.04946166  0.95053834  0.          1.        ]]\n",
      "Minibatch total loss at step 1677: 0.625019\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.91004628  0.08995379  1.          0.        ]\n",
      " [ 0.09228442  0.90771556  0.          1.        ]]\n",
      "Minibatch total loss at step 1690: 0.589792\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.6383304   0.3616696   0.          1.        ]\n",
      " [ 0.07177962  0.92822039  0.          1.        ]]\n",
      "Validation accuracy: 79.0\n",
      "Minibatch total loss at step 1703: 0.484627\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.96582264  0.03417737  1.          0.        ]\n",
      " [ 0.95097905  0.04902092  1.          0.        ]]\n",
      "Minibatch total loss at step 1716: 0.488992\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.51470917  0.48529083  0.          1.        ]\n",
      " [ 0.72007364  0.27992639  1.          0.        ]]\n",
      "Minibatch total loss at step 1729: 0.576947\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.95303375  0.04696621  1.          0.        ]\n",
      " [ 0.23032598  0.769674    0.          1.        ]]\n",
      "Minibatch total loss at step 1742: 0.507166\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.85821712  0.14178291  1.          0.        ]\n",
      " [ 0.56131226  0.43868774  1.          0.        ]]\n",
      "Minibatch total loss at step 1755: 0.811362\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.38888392  0.61111617  1.          0.        ]\n",
      " [ 0.58358389  0.41641614  0.          1.        ]]\n",
      "Minibatch total loss at step 1768: 0.539303\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.92063308  0.07936694  1.          0.        ]\n",
      " [ 0.06388555  0.93611449  0.          1.        ]]\n",
      "Minibatch total loss at step 1781: 0.758056\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.31830516  0.68169481  0.          1.        ]\n",
      " [ 0.6833089   0.31669113  0.          1.        ]]\n",
      "Minibatch total loss at step 1794: 0.604702\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.07460995  0.92539006  0.          1.        ]\n",
      " [ 0.72089744  0.27910247  0.          1.        ]]\n",
      "Validation accuracy: 77.0\n",
      "Minibatch total loss at step 1807: 0.443590\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.2295028   0.7704972   0.          1.        ]\n",
      " [ 0.53979641  0.46020362  1.          0.        ]]\n",
      "Minibatch total loss at step 1820: 0.560826\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.07719667  0.92280334  0.          1.        ]\n",
      " [ 0.37233147  0.62766856  1.          0.        ]]\n",
      "Minibatch total loss at step 1833: 0.362277\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.3862026   0.61379737  0.          1.        ]\n",
      " [ 0.76255518  0.23744482  0.          1.        ]]\n",
      "Minibatch total loss at step 1846: 0.361526\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.35376093  0.64623904  1.          0.        ]\n",
      " [ 0.56101674  0.43898323  1.          0.        ]]\n",
      "Minibatch total loss at step 1859: 0.489154\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.49489018  0.50510985  1.          0.        ]\n",
      " [ 0.32994509  0.67005485  1.          0.        ]]\n",
      "Minibatch total loss at step 1872: 0.520800\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.48803174  0.51196826  1.          0.        ]\n",
      " [ 0.67875385  0.32124615  1.          0.        ]]\n",
      "Minibatch total loss at step 1885: 0.410324\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.85795373  0.14204626  1.          0.        ]\n",
      " [ 0.90344572  0.09655432  1.          0.        ]]\n",
      "Minibatch total loss at step 1898: 0.493990\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.36579061  0.63420939  0.          1.        ]\n",
      " [ 0.87788868  0.12211137  0.          1.        ]]\n",
      "Validation accuracy: 77.5\n",
      "Minibatch total loss at step 1911: 0.416628\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.78959227  0.21040767  1.          0.        ]\n",
      " [ 0.21592174  0.78407818  0.          1.        ]]\n",
      "Minibatch total loss at step 1924: 0.577173\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5761416   0.42385837  1.          0.        ]\n",
      " [ 0.03407048  0.96592951  0.          1.        ]]\n",
      "Minibatch total loss at step 1937: 0.376001\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.31284019  0.68715984  0.          1.        ]\n",
      " [ 0.6961028   0.30389723  1.          0.        ]]\n",
      "Minibatch total loss at step 1950: 0.572407\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.16344418  0.83655584  0.          1.        ]\n",
      " [ 0.48746318  0.51253688  0.          1.        ]]\n",
      "Minibatch total loss at step 1963: 0.591778\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.13729441  0.86270565  0.          1.        ]\n",
      " [ 0.39096928  0.60903072  1.          0.        ]]\n",
      "Minibatch total loss at step 1976: 0.518978\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.92980057  0.07019939  1.          0.        ]\n",
      " [ 0.09755572  0.9024443   0.          1.        ]]\n",
      "Minibatch total loss at step 1989: 0.508774\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.26715845  0.73284155  1.          0.        ]\n",
      " [ 0.17513475  0.82486522  0.          1.        ]]\n",
      "Validation accuracy: 75.5\n",
      "Minibatch total loss at step 2002: 0.332449\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.04759001  0.95240998  0.          1.        ]\n",
      " [ 0.44258156  0.55741847  0.          1.        ]]\n",
      "Minibatch total loss at step 2015: 0.221026\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.79951066  0.20048939  1.          0.        ]\n",
      " [ 0.32772925  0.67227077  0.          1.        ]]\n",
      "Minibatch total loss at step 2028: 0.715799\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.48269328  0.51730669  0.          1.        ]\n",
      " [ 0.98721212  0.01278783  1.          0.        ]]\n",
      "Minibatch total loss at step 2041: 0.388175\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.48824847  0.51175153  1.          0.        ]\n",
      " [ 0.24713787  0.7528621   0.          1.        ]]\n",
      "Minibatch total loss at step 2054: 0.648230\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.87364763  0.12635238  1.          0.        ]\n",
      " [ 0.18536246  0.81463754  1.          0.        ]]\n",
      "Minibatch total loss at step 2067: 0.640356\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.02190875  0.97809118  0.          1.        ]\n",
      " [ 0.21637665  0.78362334  0.          1.        ]]\n",
      "Minibatch total loss at step 2080: 0.358751\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.27416629  0.72583371  0.          1.        ]\n",
      " [ 0.18324937  0.81675065  0.          1.        ]]\n",
      "Minibatch total loss at step 2093: 0.504732\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.78899366  0.21100636  1.          0.        ]\n",
      " [ 0.67680651  0.32319346  0.          1.        ]]\n",
      "Validation accuracy: 80.0\n",
      "Minibatch total loss at step 2106: 0.340016\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.73810124  0.26189876  1.          0.        ]\n",
      " [ 0.1888492   0.81115085  0.          1.        ]]\n",
      "Minibatch total loss at step 2119: 0.375354\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.78248167  0.21751833  1.          0.        ]\n",
      " [ 0.34363371  0.65636629  0.          1.        ]]\n",
      "Minibatch total loss at step 2132: 0.547491\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.97312701  0.026873    1.          0.        ]\n",
      " [ 0.88376164  0.11623836  1.          0.        ]]\n",
      "Minibatch total loss at step 2145: 0.556182\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.08497749  0.91502249  1.          0.        ]\n",
      " [ 0.59575802  0.40424195  1.          0.        ]]\n",
      "Minibatch total loss at step 2158: 0.790382\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.6994912   0.30050874  1.          0.        ]\n",
      " [ 0.84643     0.15356997  1.          0.        ]]\n",
      "Minibatch total loss at step 2171: 0.491794\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.19461626  0.80538374  0.          1.        ]\n",
      " [ 0.47515905  0.52484089  0.          1.        ]]\n",
      "Minibatch total loss at step 2184: 0.448513\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.12663358  0.87336648  0.          1.        ]\n",
      " [ 0.77180189  0.22819816  1.          0.        ]]\n",
      "Minibatch total loss at step 2197: 0.365033\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.3769795   0.62302053  0.          1.        ]\n",
      " [ 0.93137306  0.06862698  1.          0.        ]]\n",
      "Validation accuracy: 75.5\n",
      "Minibatch total loss at step 2210: 0.313304\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.30645236  0.69354767  0.          1.        ]\n",
      " [ 0.64582998  0.35417008  1.          0.        ]]\n",
      "Minibatch total loss at step 2223: 0.638178\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.77005309  0.22994691  0.          1.        ]\n",
      " [ 0.02343523  0.97656471  0.          1.        ]]\n",
      "Minibatch total loss at step 2236: 0.616381\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.18459535  0.81540471  0.          1.        ]\n",
      " [ 0.84557474  0.15442525  1.          0.        ]]\n",
      "Minibatch total loss at step 2249: 0.407852\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.64631498  0.35368502  1.          0.        ]\n",
      " [ 0.19365883  0.80634123  0.          1.        ]]\n",
      "Minibatch total loss at step 2262: 0.603784\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.12122624  0.87877375  0.          1.        ]\n",
      " [ 0.1168875   0.88311249  1.          0.        ]]\n",
      "Minibatch total loss at step 2275: 0.375621\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.23302445  0.76697558  0.          1.        ]\n",
      " [ 0.41839713  0.58160287  1.          0.        ]]\n",
      "Minibatch total loss at step 2288: 0.367507\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.07621139  0.92378867  0.          1.        ]\n",
      " [ 0.46230447  0.53769553  1.          0.        ]]\n",
      "Validation accuracy: 77.0\n",
      "Minibatch total loss at step 2301: 0.359933\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.69956136  0.30043867  1.          0.        ]\n",
      " [ 0.13259569  0.86740428  0.          1.        ]]\n",
      "Minibatch total loss at step 2314: 0.339884\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.8603186   0.13968137  1.          0.        ]\n",
      " [ 0.86115968  0.13884038  1.          0.        ]]\n",
      "Minibatch total loss at step 2327: 0.337816\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.05403928  0.94596064  0.          1.        ]\n",
      " [ 0.87483388  0.12516615  1.          0.        ]]\n",
      "Minibatch total loss at step 2340: 0.611314\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.92803729  0.07196268  0.          1.        ]\n",
      " [ 0.03224094  0.96775907  0.          1.        ]]\n",
      "Minibatch total loss at step 2353: 0.209420\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.12202401  0.87797594  0.          1.        ]\n",
      " [ 0.69472057  0.3052794   1.          0.        ]]\n",
      "Minibatch total loss at step 2366: 0.549576\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.58915108  0.41084895  1.          0.        ]\n",
      " [ 0.13807878  0.86192125  1.          0.        ]]\n",
      "Minibatch total loss at step 2379: 0.659062\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.06562808  0.93437195  0.          1.        ]\n",
      " [ 0.07676174  0.92323822  0.          1.        ]]\n",
      "Minibatch total loss at step 2392: 0.498118\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.30160093  0.69839907  0.          1.        ]\n",
      " [ 0.4370259   0.56297404  1.          0.        ]]\n",
      "Validation accuracy: 76.0\n",
      "Minibatch total loss at step 2405: 0.646720\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.1071138   0.89288622  0.          1.        ]\n",
      " [ 0.1042062   0.8957938   1.          0.        ]]\n",
      "Minibatch total loss at step 2418: 0.567910\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.81231093  0.18768911  1.          0.        ]\n",
      " [ 0.96352291  0.036477    0.          1.        ]]\n",
      "Minibatch total loss at step 2431: 0.330572\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.66351336  0.33648664  1.          0.        ]\n",
      " [ 0.19259109  0.80740893  0.          1.        ]]\n",
      "Minibatch total loss at step 2444: 0.466451\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.20405942  0.79594058  0.          1.        ]\n",
      " [ 0.87731397  0.122686    1.          0.        ]]\n",
      "Minibatch total loss at step 2457: 0.474786\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.59981436  0.40018564  0.          1.        ]\n",
      " [ 0.17749016  0.82250988  0.          1.        ]]\n",
      "Minibatch total loss at step 2470: 0.345265\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.08796097  0.91203898  0.          1.        ]\n",
      " [ 0.18637691  0.81362301  0.          1.        ]]\n",
      "Minibatch total loss at step 2483: 0.407195\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.36299649  0.63700348  0.          1.        ]\n",
      " [ 0.17698786  0.82301211  0.          1.        ]]\n",
      "Minibatch total loss at step 2496: 0.317233\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.35418096  0.64581901  0.          1.        ]\n",
      " [ 0.43067092  0.56932914  0.          1.        ]]\n",
      "Validation accuracy: 78.5\n",
      "Minibatch total loss at step 2509: 0.554704\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.27098569  0.72901434  0.          1.        ]\n",
      " [ 0.71518779  0.28481221  1.          0.        ]]\n",
      "Minibatch total loss at step 2522: 0.471278\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.26696867  0.73303133  0.          1.        ]\n",
      " [ 0.8609069   0.1390931   0.          1.        ]]\n",
      "Minibatch total loss at step 2535: 0.460040\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.92909557  0.0709044   0.          1.        ]\n",
      " [ 0.33277085  0.66722912  0.          1.        ]]\n",
      "Minibatch total loss at step 2548: 0.472898\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.65731013  0.3426899   0.          1.        ]\n",
      " [ 0.24025089  0.75974911  0.          1.        ]]\n",
      "Minibatch total loss at step 2561: 0.375552\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.24644613  0.75355393  1.          0.        ]\n",
      " [ 0.16267788  0.83732212  0.          1.        ]]\n",
      "Minibatch total loss at step 2574: 0.368657\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.87174791  0.12825209  1.          0.        ]\n",
      " [ 0.64684582  0.35315427  1.          0.        ]]\n",
      "Minibatch total loss at step 2587: 0.385163\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.91427654  0.08572342  1.          0.        ]\n",
      " [ 0.84804291  0.15195709  0.          1.        ]]\n",
      "Minibatch total loss at step 2600: 0.556806\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.57471013  0.42528981  1.          0.        ]\n",
      " [ 0.35808161  0.64191836  0.          1.        ]]\n",
      "Validation accuracy: 74.0\n",
      "Minibatch total loss at step 2613: 0.348839\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.91328645  0.08671355  1.          0.        ]\n",
      " [ 0.06034766  0.93965232  0.          1.        ]]\n",
      "Minibatch total loss at step 2626: 0.445409\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.62167233  0.3783277   1.          0.        ]\n",
      " [ 0.05670596  0.94329399  0.          1.        ]]\n",
      "Minibatch total loss at step 2639: 0.411633\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.30686325  0.69313669  0.          1.        ]\n",
      " [ 0.30074644  0.69925356  0.          1.        ]]\n",
      "Minibatch total loss at step 2652: 0.258738\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.42005101  0.57994902  1.          0.        ]\n",
      " [ 0.63931274  0.36068726  1.          0.        ]]\n",
      "Minibatch total loss at step 2665: 0.255183\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.86441785  0.13558222  1.          0.        ]\n",
      " [ 0.89241326  0.10758667  1.          0.        ]]\n",
      "Minibatch total loss at step 2678: 0.361599\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.03301021  0.96698976  0.          1.        ]\n",
      " [ 0.94246697  0.05753303  1.          0.        ]]\n",
      "Minibatch total loss at step 2691: 0.448591\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.53949976  0.46050033  1.          0.        ]\n",
      " [ 0.91082591  0.08917409  1.          0.        ]]\n",
      "Validation accuracy: 72.0\n",
      "Minibatch total loss at step 2704: 0.237774\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.9521274   0.04787259  1.          0.        ]\n",
      " [ 0.91254497  0.08745497  1.          0.        ]]\n",
      "Minibatch total loss at step 2717: 0.377827\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.08994559  0.91005433  0.          1.        ]\n",
      " [ 0.92568219  0.07431779  1.          0.        ]]\n",
      "Minibatch total loss at step 2730: 0.411218\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.07673126  0.92326868  0.          1.        ]\n",
      " [ 0.14691715  0.8530829   0.          1.        ]]\n",
      "Minibatch total loss at step 2743: 0.378890\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.16938064  0.83061939  0.          1.        ]\n",
      " [ 0.85714322  0.14285673  1.          0.        ]]\n",
      "Minibatch total loss at step 2756: 0.374315\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.03439913  0.96560085  0.          1.        ]\n",
      " [ 0.7414133   0.25858676  1.          0.        ]]\n",
      "Minibatch total loss at step 2769: 0.616190\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.60414755  0.39585242  1.          0.        ]\n",
      " [ 0.95766205  0.04233792  0.          1.        ]]\n",
      "Minibatch total loss at step 2782: 0.440769\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.59392858  0.40607142  0.          1.        ]\n",
      " [ 0.13680495  0.86319506  0.          1.        ]]\n",
      "Minibatch total loss at step 2795: 0.518770\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.93390673  0.06609333  1.          0.        ]\n",
      " [ 0.44983459  0.55016536  0.          1.        ]]\n",
      "Validation accuracy: 73.5\n",
      "Minibatch total loss at step 2808: 0.438653\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.65490067  0.34509927  1.          0.        ]\n",
      " [ 0.93705058  0.06294941  1.          0.        ]]\n",
      "Minibatch total loss at step 2821: 0.321715\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.58967555  0.41032448  1.          0.        ]\n",
      " [ 0.60553455  0.39446548  1.          0.        ]]\n",
      "Minibatch total loss at step 2834: 0.316874\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.16307771  0.83692235  0.          1.        ]\n",
      " [ 0.4612644   0.53873563  0.          1.        ]]\n",
      "Minibatch total loss at step 2847: 0.323472\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.59890187  0.4010981   1.          0.        ]\n",
      " [ 0.52446842  0.47553161  1.          0.        ]]\n",
      "Minibatch total loss at step 2860: 0.451175\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.61332685  0.38667315  0.          1.        ]\n",
      " [ 0.8310138   0.16898619  1.          0.        ]]\n",
      "Minibatch total loss at step 2873: 0.432316\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.43089649  0.56910354  0.          1.        ]\n",
      " [ 0.17919709  0.82080299  1.          0.        ]]\n",
      "Minibatch total loss at step 2886: 0.328425\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.57705754  0.42294249  1.          0.        ]\n",
      " [ 0.60084194  0.39915803  1.          0.        ]]\n",
      "Minibatch total loss at step 2899: 0.751356\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.04765043  0.95234954  0.          1.        ]\n",
      " [ 0.55699193  0.44300807  1.          0.        ]]\n",
      "Validation accuracy: 72.0\n",
      "Minibatch total loss at step 2912: 0.348030\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.33154663  0.6684534   0.          1.        ]\n",
      " [ 0.04809878  0.95190126  0.          1.        ]]\n",
      "Minibatch total loss at step 2925: 0.415830\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.90196806  0.09803197  1.          0.        ]\n",
      " [ 0.04979948  0.95020056  0.          1.        ]]\n",
      "Minibatch total loss at step 2938: 0.269057\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.0148241   0.98517585  0.          1.        ]\n",
      " [ 0.85132688  0.14867316  1.          0.        ]]\n",
      "Minibatch total loss at step 2951: 0.340534\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.39041629  0.60958368  1.          0.        ]\n",
      " [ 0.36339375  0.63660622  1.          0.        ]]\n",
      "Minibatch total loss at step 2964: 0.481860\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.68022561  0.31977442  0.          1.        ]\n",
      " [ 0.04540976  0.95459026  0.          1.        ]]\n",
      "Minibatch total loss at step 2977: 0.324637\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.53005958  0.46994042  1.          0.        ]\n",
      " [ 0.83977026  0.16022976  1.          0.        ]]\n",
      "Minibatch total loss at step 2990: 0.273466\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.10693113  0.89306885  0.          1.        ]\n",
      " [ 0.74388862  0.25611135  1.          0.        ]]\n",
      "Validation accuracy: 75.5\n",
      "Minibatch total loss at step 3003: 0.286295\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.94496018  0.05503979  1.          0.        ]\n",
      " [ 0.09228512  0.9077149   0.          1.        ]]\n",
      "Minibatch total loss at step 3016: 0.310700\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.22167152  0.77832848  1.          0.        ]\n",
      " [ 0.1192725   0.88072753  0.          1.        ]]\n",
      "Minibatch total loss at step 3029: 0.324415\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.70221341  0.29778656  1.          0.        ]\n",
      " [ 0.22212279  0.77787727  0.          1.        ]]\n",
      "Minibatch total loss at step 3042: 0.324660\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.32477033  0.67522961  1.          0.        ]\n",
      " [ 0.01480318  0.98519683  0.          1.        ]]\n",
      "Minibatch total loss at step 3055: 0.321022\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.05081353  0.94918644  0.          1.        ]\n",
      " [ 0.77888048  0.22111949  1.          0.        ]]\n",
      "Minibatch total loss at step 3068: 0.381798\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.32468465  0.67531538  0.          1.        ]\n",
      " [ 0.76886493  0.23113504  1.          0.        ]]\n",
      "Minibatch total loss at step 3081: 0.388563\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.51875544  0.48124459  0.          1.        ]\n",
      " [ 0.58399129  0.41600871  1.          0.        ]]\n",
      "Minibatch total loss at step 3094: 0.288673\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.77255166  0.22744833  1.          0.        ]\n",
      " [ 0.61775637  0.38224354  1.          0.        ]]\n",
      "Validation accuracy: 73.5\n",
      "Minibatch total loss at step 3107: 0.472313\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.34120113  0.65879881  1.          0.        ]\n",
      " [ 0.65845823  0.34154174  1.          0.        ]]\n",
      "Minibatch total loss at step 3120: 0.304451\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.82403159  0.17596842  1.          0.        ]\n",
      " [ 0.43059927  0.56940073  0.          1.        ]]\n",
      "Minibatch total loss at step 3133: 0.305581\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.09731271  0.90268731  0.          1.        ]\n",
      " [ 0.21655571  0.78344434  0.          1.        ]]\n",
      "Minibatch total loss at step 3146: 0.283397\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.62290436  0.3770957   1.          0.        ]\n",
      " [ 0.03169129  0.96830869  0.          1.        ]]\n",
      "Minibatch total loss at step 3159: 0.265704\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.33309174  0.66690826  0.          1.        ]\n",
      " [ 0.4726643   0.5273357   1.          0.        ]]\n",
      "Minibatch total loss at step 3172: 0.253214\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.37178245  0.62821758  1.          0.        ]\n",
      " [ 0.85880351  0.14119652  1.          0.        ]]\n",
      "Minibatch total loss at step 3185: 0.445026\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.07255893  0.92744106  0.          1.        ]\n",
      " [ 0.16417292  0.83582705  0.          1.        ]]\n",
      "Minibatch total loss at step 3198: 0.238486\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.06855242  0.93144763  0.          1.        ]\n",
      " [ 0.41454309  0.58545691  1.          0.        ]]\n",
      "Validation accuracy: 75.0\n",
      "Minibatch total loss at step 3211: 0.484158\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.78212112  0.21787891  0.          1.        ]\n",
      " [ 0.86886567  0.13113435  1.          0.        ]]\n",
      "Minibatch total loss at step 3224: 0.287597\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.79130703  0.20869295  1.          0.        ]\n",
      " [ 0.0949846   0.90501541  0.          1.        ]]\n",
      "Minibatch total loss at step 3237: 0.440486\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.41520274  0.58479726  0.          1.        ]\n",
      " [ 0.87976146  0.12023856  1.          0.        ]]\n",
      "Minibatch total loss at step 3250: 0.288476\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.18359779  0.81640226  0.          1.        ]\n",
      " [ 0.90951067  0.09048928  1.          0.        ]]\n",
      "Minibatch total loss at step 3263: 0.261014\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.07649548  0.92350453  0.          1.        ]\n",
      " [ 0.11278486  0.88721508  0.          1.        ]]\n",
      "Minibatch total loss at step 3276: 0.396053\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.65588164  0.34411833  0.          1.        ]\n",
      " [ 0.616745    0.38325495  1.          0.        ]]\n",
      "Minibatch total loss at step 3289: 0.229683\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.7145462   0.28545383  1.          0.        ]\n",
      " [ 0.58393979  0.41606021  1.          0.        ]]\n",
      "Validation accuracy: 69.0\n",
      "Minibatch total loss at step 3302: 0.328800\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.72984004  0.27015996  1.          0.        ]\n",
      " [ 0.93560582  0.06439424  1.          0.        ]]\n",
      "Minibatch total loss at step 3315: 0.341704\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.17306086  0.82693917  0.          1.        ]\n",
      " [ 0.74471009  0.25528988  1.          0.        ]]\n",
      "Minibatch total loss at step 3328: 0.456523\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.6797359   0.3202641   0.          1.        ]\n",
      " [ 0.73855674  0.26144326  0.          1.        ]]\n",
      "Minibatch total loss at step 3341: 0.298248\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.34954423  0.65045577  1.          0.        ]\n",
      " [ 0.7762714   0.22372861  1.          0.        ]]\n",
      "Minibatch total loss at step 3354: 0.379399\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.91642702  0.08357301  1.          0.        ]\n",
      " [ 0.88372856  0.11627138  1.          0.        ]]\n",
      "Minibatch total loss at step 3367: 0.510638\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.0428609   0.95713907  0.          1.        ]\n",
      " [ 0.70535594  0.294644    1.          0.        ]]\n",
      "Minibatch total loss at step 3380: 0.330596\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.16000246  0.83999759  0.          1.        ]\n",
      " [ 0.62091869  0.37908128  0.          1.        ]]\n",
      "Minibatch total loss at step 3393: 0.320593\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.23257014  0.76742983  0.          1.        ]\n",
      " [ 0.00543783  0.99456221  0.          1.        ]]\n",
      "Validation accuracy: 69.0\n",
      "Minibatch total loss at step 3406: 0.301329\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.82264775  0.17735228  1.          0.        ]\n",
      " [ 0.13487756  0.86512244  0.          1.        ]]\n",
      "Minibatch total loss at step 3419: 0.278526\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.56962341  0.43037656  1.          0.        ]\n",
      " [ 0.10876364  0.89123636  0.          1.        ]]\n",
      "Minibatch total loss at step 3432: 0.424794\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.25588667  0.74411333  1.          0.        ]\n",
      " [ 0.46833676  0.53166324  1.          0.        ]]\n",
      "Minibatch total loss at step 3445: 0.267171\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.90952992  0.09047014  1.          0.        ]\n",
      " [ 0.08924631  0.91075367  0.          1.        ]]\n",
      "Minibatch total loss at step 3458: 0.367185\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.11954264  0.8804574   0.          1.        ]\n",
      " [ 0.31952268  0.68047738  0.          1.        ]]\n",
      "Minibatch total loss at step 3471: 0.193193\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.91368663  0.08631344  1.          0.        ]\n",
      " [ 0.76699126  0.23300868  1.          0.        ]]\n",
      "Minibatch total loss at step 3484: 0.274629\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.05589532  0.94410467  0.          1.        ]\n",
      " [ 0.0362548   0.96374524  0.          1.        ]]\n",
      "Minibatch total loss at step 3497: 0.267838\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.88723159  0.1127684   1.          0.        ]\n",
      " [ 0.96083069  0.03916934  1.          0.        ]]\n",
      "Validation accuracy: 73.5\n",
      "Minibatch total loss at step 3510: 0.501660\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.41572872  0.58427131  0.          1.        ]\n",
      " [ 0.20901617  0.7909838   1.          0.        ]]\n",
      "Minibatch total loss at step 3523: 0.411657\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.03168821  0.96831179  0.          1.        ]\n",
      " [ 0.38516566  0.61483437  0.          1.        ]]\n",
      "Minibatch total loss at step 3536: 0.327436\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.92540026  0.07459971  1.          0.        ]\n",
      " [ 0.55767614  0.44232389  1.          0.        ]]\n",
      "Minibatch total loss at step 3549: 0.251963\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.44983289  0.55016708  1.          0.        ]\n",
      " [ 0.53028762  0.46971241  1.          0.        ]]\n",
      "Minibatch total loss at step 3562: 0.337350\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.65002418  0.34997585  1.          0.        ]\n",
      " [ 0.86698741  0.13301261  1.          0.        ]]\n",
      "Minibatch total loss at step 3575: 0.378663\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.64869595  0.35130405  0.          1.        ]\n",
      " [ 0.96370381  0.03629615  1.          0.        ]]\n",
      "Minibatch total loss at step 3588: 0.364491\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.19384652  0.80615348  1.          0.        ]\n",
      " [ 0.61209679  0.38790315  1.          0.        ]]\n",
      "Validation accuracy: 75.5\n",
      "Minibatch total loss at step 3601: 0.215174\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.07124722  0.92875272  0.          1.        ]\n",
      " [ 0.75118017  0.2488198   1.          0.        ]]\n",
      "Minibatch total loss at step 3614: 0.315926\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.85206741  0.14793257  1.          0.        ]\n",
      " [ 0.02608184  0.97391814  0.          1.        ]]\n",
      "Minibatch total loss at step 3627: 0.421141\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.08534697  0.91465306  0.          1.        ]\n",
      " [ 0.76285803  0.23714201  0.          1.        ]]\n",
      "Minibatch total loss at step 3640: 0.218964\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.36107716  0.63892281  0.          1.        ]\n",
      " [ 0.92102629  0.0789737   1.          0.        ]]\n",
      "Minibatch total loss at step 3653: 0.249204\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.88832921  0.11167084  1.          0.        ]\n",
      " [ 0.54963911  0.45036089  1.          0.        ]]\n",
      "Minibatch total loss at step 3666: 0.248789\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.23087095  0.7691291   0.          1.        ]\n",
      " [ 0.15459974  0.84540033  0.          1.        ]]\n",
      "Minibatch total loss at step 3679: 0.219682\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.75095266  0.24904734  1.          0.        ]\n",
      " [ 0.94957262  0.05042738  1.          0.        ]]\n",
      "Minibatch total loss at step 3692: 0.309983\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.92052567  0.07947434  1.          0.        ]\n",
      " [ 0.90436924  0.0956308   1.          0.        ]]\n",
      "Validation accuracy: 66.5\n",
      "Minibatch total loss at step 3705: 0.288894\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.14188622  0.85811377  0.          1.        ]\n",
      " [ 0.7688114   0.23118863  1.          0.        ]]\n",
      "Minibatch total loss at step 3718: 0.293979\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.66048163  0.33951837  1.          0.        ]\n",
      " [ 0.11257404  0.88742596  0.          1.        ]]\n",
      "Minibatch total loss at step 3731: 0.432501\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.46171999  0.53828001  1.          0.        ]\n",
      " [ 0.03184317  0.96815675  0.          1.        ]]\n",
      "Minibatch total loss at step 3744: 0.283851\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.95340735  0.04659262  1.          0.        ]\n",
      " [ 0.03467315  0.96532685  0.          1.        ]]\n",
      "Minibatch total loss at step 3757: 0.220576\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.31888875  0.68111128  0.          1.        ]\n",
      " [ 0.13142782  0.86857224  0.          1.        ]]\n",
      "Minibatch total loss at step 3770: 0.392952\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.04534752  0.95465249  0.          1.        ]\n",
      " [ 0.70025951  0.29974055  1.          0.        ]]\n",
      "Minibatch total loss at step 3783: 0.307143\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.24792066  0.75207937  0.          1.        ]\n",
      " [ 0.7399953   0.26000473  1.          0.        ]]\n",
      "Minibatch total loss at step 3796: 0.362548\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.16314688  0.83685309  1.          0.        ]\n",
      " [ 0.23402265  0.76597738  0.          1.        ]]\n",
      "Validation accuracy: 68.5\n",
      "Minibatch total loss at step 3809: 0.381676\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.02321737  0.97678262  0.          1.        ]\n",
      " [ 0.71059173  0.28940821  1.          0.        ]]\n",
      "Minibatch total loss at step 3822: 0.300112\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.16432005  0.83567995  0.          1.        ]\n",
      " [ 0.01985084  0.98014921  0.          1.        ]]\n",
      "Minibatch total loss at step 3835: 0.260801\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.77977598  0.22022401  1.          0.        ]\n",
      " [ 0.06090911  0.93909091  0.          1.        ]]\n",
      "Minibatch total loss at step 3848: 0.262495\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.47372127  0.52627873  1.          0.        ]\n",
      " [ 0.17688312  0.8231169   0.          1.        ]]\n",
      "Minibatch total loss at step 3861: 0.265021\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.62981981  0.37018013  0.          1.        ]\n",
      " [ 0.16090262  0.83909732  0.          1.        ]]\n",
      "Minibatch total loss at step 3874: 0.362199\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.89038366  0.10961636  1.          0.        ]\n",
      " [ 0.30167335  0.69832665  0.          1.        ]]\n",
      "Minibatch total loss at step 3887: 0.268445\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.93301642  0.06698358  1.          0.        ]\n",
      " [ 0.35070324  0.64929676  0.          1.        ]]\n",
      "Minibatch total loss at step 3900: 0.316858\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.88039315  0.11960684  1.          0.        ]\n",
      " [ 0.90047807  0.09952191  1.          0.        ]]\n",
      "Validation accuracy: 72.5\n",
      "Minibatch total loss at step 3913: 0.342453\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.05483748  0.94516248  0.          1.        ]\n",
      " [ 0.65538454  0.34461549  0.          1.        ]]\n",
      "Minibatch total loss at step 3926: 0.244328\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.70740992  0.29259014  1.          0.        ]\n",
      " [ 0.44918129  0.55081868  0.          1.        ]]\n",
      "Minibatch total loss at step 3939: 0.420011\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.48253277  0.5174672   1.          0.        ]\n",
      " [ 0.66028023  0.33971974  1.          0.        ]]\n",
      "Minibatch total loss at step 3952: 0.217588\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.01784926  0.98215067  0.          1.        ]\n",
      " [ 0.84687907  0.15312095  1.          0.        ]]\n",
      "Minibatch total loss at step 3965: 0.315792\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.01982346  0.98017657  0.          1.        ]\n",
      " [ 0.03463887  0.96536112  0.          1.        ]]\n",
      "Minibatch total loss at step 3978: 0.206567\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.12514807  0.87485194  0.          1.        ]\n",
      " [ 0.96216661  0.03783346  1.          0.        ]]\n",
      "Minibatch total loss at step 3991: 0.388281\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.20332403  0.79667598  0.          1.        ]\n",
      " [ 0.41272306  0.58727694  0.          1.        ]]\n",
      "Validation accuracy: 71.5\n",
      "Minibatch total loss at step 4004: 0.345253\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.52634066  0.47365931  1.          0.        ]\n",
      " [ 0.48268458  0.51731545  0.          1.        ]]\n",
      "Minibatch total loss at step 4017: 0.238778\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.40325886  0.59674114  0.          1.        ]\n",
      " [ 0.93309957  0.06690047  1.          0.        ]]\n",
      "Minibatch total loss at step 4030: 0.326086\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.9146384   0.08536159  1.          0.        ]\n",
      " [ 0.0953774   0.90462261  0.          1.        ]]\n",
      "Minibatch total loss at step 4043: 0.370289\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.29912448  0.70087552  0.          1.        ]\n",
      " [ 0.50515556  0.49484447  1.          0.        ]]\n",
      "Minibatch total loss at step 4056: 0.353535\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.44887087  0.55112916  1.          0.        ]\n",
      " [ 0.8696394   0.13036062  1.          0.        ]]\n",
      "Minibatch total loss at step 4069: 0.297789\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.44450173  0.5554983   0.          1.        ]\n",
      " [ 0.13521586  0.86478418  0.          1.        ]]\n",
      "Minibatch total loss at step 4082: 0.383194\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.48374569  0.51625431  1.          0.        ]\n",
      " [ 0.80569583  0.19430417  1.          0.        ]]\n",
      "Minibatch total loss at step 4095: 0.246827\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.1951129   0.80488718  0.          1.        ]\n",
      " [ 0.40460178  0.59539819  1.          0.        ]]\n",
      "Validation accuracy: 71.5\n",
      "Minibatch total loss at step 4108: 0.233543\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.69454181  0.30545822  1.          0.        ]\n",
      " [ 0.72899836  0.27100167  1.          0.        ]]\n",
      "Minibatch total loss at step 4121: 0.289949\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.67709649  0.32290354  0.          1.        ]\n",
      " [ 0.77392942  0.22607054  1.          0.        ]]\n",
      "Minibatch total loss at step 4134: 0.218406\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.22532366  0.77467632  0.          1.        ]\n",
      " [ 0.67382228  0.32617769  1.          0.        ]]\n",
      "Minibatch total loss at step 4147: 0.293650\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.80339909  0.19660096  1.          0.        ]\n",
      " [ 0.21811837  0.78188163  0.          1.        ]]\n",
      "Minibatch total loss at step 4160: 0.303326\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.50944227  0.49055767  0.          1.        ]\n",
      " [ 0.70888132  0.29111865  1.          0.        ]]\n",
      "Minibatch total loss at step 4173: 0.230074\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.20670788  0.79329211  0.          1.        ]\n",
      " [ 0.04392794  0.95607209  0.          1.        ]]\n",
      "Minibatch total loss at step 4186: 0.235877\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.05765321  0.94234675  0.          1.        ]\n",
      " [ 0.29548615  0.70451385  0.          1.        ]]\n",
      "Minibatch total loss at step 4199: 0.262199\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.09440584  0.90559423  0.          1.        ]\n",
      " [ 0.92897314  0.07102685  1.          0.        ]]\n",
      "Validation accuracy: 67.0\n",
      "Minibatch total loss at step 4212: 0.274755\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.3393071   0.66069293  0.          1.        ]\n",
      " [ 0.07909775  0.92090225  0.          1.        ]]\n",
      "Minibatch total loss at step 4225: 0.419205\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.41483319  0.58516681  1.          0.        ]\n",
      " [ 0.48987523  0.51012474  0.          1.        ]]\n",
      "Minibatch total loss at step 4238: 0.319561\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.31863698  0.68136305  1.          0.        ]\n",
      " [ 0.77691388  0.22308616  1.          0.        ]]\n",
      "Minibatch total loss at step 4251: 0.246945\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.17493051  0.82506943  0.          1.        ]\n",
      " [ 0.67189109  0.32810894  0.          1.        ]]\n",
      "Minibatch total loss at step 4264: 0.290992\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.87120563  0.12879434  1.          0.        ]\n",
      " [ 0.09575381  0.90424621  0.          1.        ]]\n",
      "Minibatch total loss at step 4277: 0.352758\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.73743629  0.26256368  1.          0.        ]\n",
      " [ 0.05755175  0.94244832  0.          1.        ]]\n",
      "Minibatch total loss at step 4290: 0.238771\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.36376518  0.63623476  0.          1.        ]\n",
      " [ 0.94274902  0.05725101  1.          0.        ]]\n",
      "Validation accuracy: 73.0\n",
      "Minibatch total loss at step 4303: 0.459072\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.94593924  0.05406074  1.          0.        ]\n",
      " [ 0.97153765  0.02846234  1.          0.        ]]\n",
      "Minibatch total loss at step 4316: 0.342920\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.56184953  0.43815047  1.          0.        ]\n",
      " [ 0.10197507  0.89802492  0.          1.        ]]\n",
      "Minibatch total loss at step 4329: 0.251218\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.02473677  0.97526324  0.          1.        ]\n",
      " [ 0.07420676  0.92579323  0.          1.        ]]\n",
      "Minibatch total loss at step 4342: 0.215102\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.83836716  0.16163287  1.          0.        ]\n",
      " [ 0.11544321  0.88455683  0.          1.        ]]\n",
      "Minibatch total loss at step 4355: 0.174148\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.91866463  0.08133537  1.          0.        ]\n",
      " [ 0.30078214  0.69921792  0.          1.        ]]\n",
      "Minibatch total loss at step 4368: 0.214964\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.07727676  0.92272323  0.          1.        ]\n",
      " [ 0.74647534  0.2535246   1.          0.        ]]\n",
      "Minibatch total loss at step 4381: 0.325515\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.1182574   0.8817426   0.          1.        ]\n",
      " [ 0.28192228  0.71807772  0.          1.        ]]\n",
      "Minibatch total loss at step 4394: 0.253162\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.39641657  0.60358346  1.          0.        ]\n",
      " [ 0.7243253   0.27567476  1.          0.        ]]\n",
      "Validation accuracy: 67.0\n",
      "Minibatch total loss at step 4407: 0.309221\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.15723254  0.84276748  0.          1.        ]\n",
      " [ 0.82612085  0.17387916  1.          0.        ]]\n",
      "Minibatch total loss at step 4420: 0.205948\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.71490204  0.28509793  1.          0.        ]\n",
      " [ 0.23521277  0.76478726  0.          1.        ]]\n",
      "Minibatch total loss at step 4433: 0.221929\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.08481892  0.91518104  0.          1.        ]\n",
      " [ 0.29439172  0.70560825  0.          1.        ]]\n",
      "Minibatch total loss at step 4446: 0.250238\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.10644262  0.89355743  0.          1.        ]\n",
      " [ 0.02799844  0.97200161  0.          1.        ]]\n",
      "Minibatch total loss at step 4459: 0.247850\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.93056232  0.06943763  1.          0.        ]\n",
      " [ 0.53809178  0.46190825  1.          0.        ]]\n",
      "Minibatch total loss at step 4472: 0.269092\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.60082638  0.39917365  1.          0.        ]\n",
      " [ 0.77258497  0.227415    1.          0.        ]]\n",
      "Minibatch total loss at step 4485: 0.306500\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.26446491  0.73553503  0.          1.        ]\n",
      " [ 0.08189997  0.9181      0.          1.        ]]\n",
      "Minibatch total loss at step 4498: 0.303502\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.91706026  0.08293971  1.          0.        ]\n",
      " [ 0.9439038   0.05609626  1.          0.        ]]\n",
      "Validation accuracy: 69.0\n",
      "Minibatch total loss at step 4511: 0.326497\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.94321221  0.05678773  1.          0.        ]\n",
      " [ 0.23066966  0.76933032  0.          1.        ]]\n",
      "Minibatch total loss at step 4524: 0.215542\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.61288297  0.38711697  1.          0.        ]\n",
      " [ 0.04135285  0.95864713  0.          1.        ]]\n",
      "Minibatch total loss at step 4537: 0.241586\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.66024303  0.339757    1.          0.        ]\n",
      " [ 0.68572742  0.31427258  1.          0.        ]]\n",
      "Minibatch total loss at step 4550: 0.286676\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.92863041  0.07136957  1.          0.        ]\n",
      " [ 0.79153043  0.20846952  1.          0.        ]]\n",
      "Minibatch total loss at step 4563: 0.372486\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.84271032  0.15728973  1.          0.        ]\n",
      " [ 0.65484256  0.34515738  1.          0.        ]]\n",
      "Minibatch total loss at step 4576: 0.172674\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.13831137  0.86168867  0.          1.        ]\n",
      " [ 0.97438443  0.02561558  1.          0.        ]]\n",
      "Minibatch total loss at step 4589: 0.206070\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.48175228  0.51824772  1.          0.        ]\n",
      " [ 0.80083489  0.19916509  1.          0.        ]]\n",
      "Validation accuracy: 73.5\n",
      "Minibatch total loss at step 4602: 0.301457\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.62410212  0.37589782  1.          0.        ]\n",
      " [ 0.58732724  0.41267273  1.          0.        ]]\n",
      "Minibatch total loss at step 4615: 0.276581\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.8054893   0.19451067  1.          0.        ]\n",
      " [ 0.22558622  0.77441376  0.          1.        ]]\n",
      "Minibatch total loss at step 4628: 0.315410\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.15557389  0.8444261   0.          1.        ]\n",
      " [ 0.51552463  0.48447537  1.          0.        ]]\n",
      "Minibatch total loss at step 4641: 0.185444\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.81757122  0.18242879  1.          0.        ]\n",
      " [ 0.09251115  0.90748882  0.          1.        ]]\n",
      "Minibatch total loss at step 4654: 0.261521\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.0198522   0.98014784  0.          1.        ]\n",
      " [ 0.33555827  0.6644417   0.          1.        ]]\n",
      "Minibatch total loss at step 4667: 0.284497\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.01379507  0.98620492  0.          1.        ]\n",
      " [ 0.78584242  0.21415752  1.          0.        ]]\n",
      "Minibatch total loss at step 4680: 0.339630\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.65569913  0.34430084  1.          0.        ]\n",
      " [ 0.05760897  0.94239104  0.          1.        ]]\n",
      "Minibatch total loss at step 4693: 0.247671\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.27951494  0.72048503  0.          1.        ]\n",
      " [ 0.83484596  0.16515404  1.          0.        ]]\n",
      "Validation accuracy: 74.0\n",
      "Minibatch total loss at step 4706: 0.244574\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.87613273  0.1238673   1.          0.        ]\n",
      " [ 0.08317378  0.91682625  0.          1.        ]]\n",
      "Minibatch total loss at step 4719: 0.220628\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.78383207  0.2161679   1.          0.        ]\n",
      " [ 0.31105715  0.68894285  0.          1.        ]]\n",
      "Minibatch total loss at step 4732: 0.262217\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.69244039  0.30755964  1.          0.        ]\n",
      " [ 0.55830497  0.44169503  1.          0.        ]]\n",
      "Minibatch total loss at step 4745: 0.293913\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.86409169  0.13590835  1.          0.        ]\n",
      " [ 0.87886405  0.12113598  1.          0.        ]]\n",
      "Minibatch total loss at step 4758: 0.287663\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.04488719  0.95511281  0.          1.        ]\n",
      " [ 0.95395291  0.04604711  1.          0.        ]]\n",
      "Minibatch total loss at step 4771: 0.423913\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.03426329  0.96573669  0.          1.        ]\n",
      " [ 0.48419917  0.51580089  1.          0.        ]]\n",
      "Minibatch total loss at step 4784: 0.182245\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.12165814  0.87834185  0.          1.        ]\n",
      " [ 0.2957969   0.70420307  0.          1.        ]]\n",
      "Minibatch total loss at step 4797: 0.326760\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.54213178  0.45786825  0.          1.        ]\n",
      " [ 0.40849486  0.59150511  0.          1.        ]]\n",
      "Validation accuracy: 66.0\n",
      "Minibatch total loss at step 4810: 0.234865\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.23017938  0.76982069  0.          1.        ]\n",
      " [ 0.03344284  0.96655709  0.          1.        ]]\n",
      "Minibatch total loss at step 4823: 0.218725\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.87327093  0.1267291   1.          0.        ]\n",
      " [ 0.00575459  0.99424547  0.          1.        ]]\n",
      "Minibatch total loss at step 4836: 0.292094\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.01187037  0.98812962  0.          1.        ]\n",
      " [ 0.89572293  0.10427714  1.          0.        ]]\n",
      "Minibatch total loss at step 4849: 0.245784\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.34202108  0.65797895  0.          1.        ]\n",
      " [ 0.06055883  0.93944114  0.          1.        ]]\n",
      "Minibatch total loss at step 4862: 0.234771\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.06645068  0.93354928  0.          1.        ]\n",
      " [ 0.76092243  0.23907755  1.          0.        ]]\n",
      "Minibatch total loss at step 4875: 0.259419\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.19799729  0.80200279  0.          1.        ]\n",
      " [ 0.72969514  0.27030486  1.          0.        ]]\n",
      "Minibatch total loss at step 4888: 0.202663\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.92744344  0.07255659  1.          0.        ]\n",
      " [ 0.93901062  0.06098941  1.          0.        ]]\n",
      "Validation accuracy: 67.5\n",
      "Minibatch total loss at step 4901: 0.293622\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.97617221  0.02382776  1.          0.        ]\n",
      " [ 0.20160951  0.79839045  0.          1.        ]]\n",
      "Minibatch total loss at step 4914: 0.219535\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.90726477  0.09273528  1.          0.        ]\n",
      " [ 0.97590077  0.02409922  1.          0.        ]]\n",
      "Minibatch total loss at step 4927: 0.195336\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.97481358  0.02518648  1.          0.        ]\n",
      " [ 0.15700607  0.84299392  0.          1.        ]]\n",
      "Minibatch total loss at step 4940: 0.215201\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.12389466  0.87610537  0.          1.        ]\n",
      " [ 0.88161713  0.11838292  1.          0.        ]]\n",
      "Minibatch total loss at step 4953: 0.227370\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.1217879   0.87821209  0.          1.        ]\n",
      " [ 0.42226946  0.57773054  0.          1.        ]]\n",
      "Minibatch total loss at step 4966: 0.221231\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.86096418  0.13903582  1.          0.        ]\n",
      " [ 0.70528293  0.29471716  1.          0.        ]]\n",
      "Minibatch total loss at step 4979: 0.278544\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.15340704  0.84659296  0.          1.        ]\n",
      " [ 0.21590319  0.78409684  0.          1.        ]]\n",
      "Minibatch total loss at step 4992: 0.241010\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.17891294  0.82108706  0.          1.        ]\n",
      " [ 0.94596827  0.0540317   1.          0.        ]]\n",
      "Validation accuracy: 71.5\n",
      "Minibatch total loss at step 5005: 0.319494\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.69342083  0.30657914  1.          0.        ]\n",
      " [ 0.1793555   0.82064444  0.          1.        ]]\n",
      "Minibatch total loss at step 5018: 0.203014\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.10349378  0.89650625  0.          1.        ]\n",
      " [ 0.06177351  0.93822652  0.          1.        ]]\n",
      "Minibatch total loss at step 5031: 0.402101\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.57133746  0.42866254  1.          0.        ]\n",
      " [ 0.58944637  0.41055369  1.          0.        ]]\n",
      "Minibatch total loss at step 5044: 0.154672\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.94668823  0.05331179  1.          0.        ]\n",
      " [ 0.27271211  0.72728789  0.          1.        ]]\n",
      "Minibatch total loss at step 5057: 0.299575\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.0496792   0.95032078  0.          1.        ]\n",
      " [ 0.01606011  0.98393989  0.          1.        ]]\n",
      "Minibatch total loss at step 5070: 0.332010\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.21918051  0.78081954  0.          1.        ]\n",
      " [ 0.85419059  0.14580938  1.          0.        ]]\n",
      "Minibatch total loss at step 5083: 0.278609\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.76553845  0.23446161  0.          1.        ]\n",
      " [ 0.18525761  0.81474239  0.          1.        ]]\n",
      "Minibatch total loss at step 5096: 0.233261\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.88427615  0.11572388  1.          0.        ]\n",
      " [ 0.62308753  0.37691247  1.          0.        ]]\n",
      "Validation accuracy: 69.0\n",
      "Minibatch total loss at step 5109: 0.185538\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.92187411  0.07812596  1.          0.        ]\n",
      " [ 0.85974801  0.14025198  1.          0.        ]]\n",
      "Minibatch total loss at step 5122: 0.225492\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.11679177  0.88320816  0.          1.        ]\n",
      " [ 0.98440069  0.01559924  1.          0.        ]]\n",
      "Minibatch total loss at step 5135: 0.311213\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.94645697  0.05354302  1.          0.        ]\n",
      " [ 0.8546266   0.14537333  1.          0.        ]]\n",
      "Minibatch total loss at step 5148: 0.261783\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.7378571   0.26214293  1.          0.        ]\n",
      " [ 0.71048099  0.28951895  1.          0.        ]]\n",
      "Minibatch total loss at step 5161: 0.236679\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.74589074  0.25410926  1.          0.        ]\n",
      " [ 0.06384453  0.93615544  0.          1.        ]]\n",
      "Minibatch total loss at step 5174: 0.230310\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.3309409   0.6690591   0.          1.        ]\n",
      " [ 0.76062131  0.23937865  1.          0.        ]]\n",
      "Minibatch total loss at step 5187: 0.273081\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.46764606  0.53235388  0.          1.        ]\n",
      " [ 0.4790448   0.5209552   0.          1.        ]]\n",
      "Minibatch total loss at step 5200: 0.325485\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.76746666  0.23253332  0.          1.        ]\n",
      " [ 0.10943601  0.89056402  0.          1.        ]]\n",
      "Validation accuracy: 65.5\n",
      "Minibatch total loss at step 5213: 0.214202\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.98289514  0.01710488  1.          0.        ]\n",
      " [ 0.08497444  0.91502553  0.          1.        ]]\n",
      "Minibatch total loss at step 5226: 0.214523\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.16767506  0.83232498  0.          1.        ]\n",
      " [ 0.08967157  0.91032839  0.          1.        ]]\n",
      "Minibatch total loss at step 5239: 0.218642\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.8349883   0.16501166  1.          0.        ]\n",
      " [ 0.94486374  0.0551362   1.          0.        ]]\n",
      "Minibatch total loss at step 5252: 0.188707\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.53410619  0.46589383  1.          0.        ]\n",
      " [ 0.78982478  0.21017526  1.          0.        ]]\n",
      "Minibatch total loss at step 5265: 0.219650\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.57440138  0.42559859  1.          0.        ]\n",
      " [ 0.10934368  0.89065629  0.          1.        ]]\n",
      "Minibatch total loss at step 5278: 0.225983\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.36986428  0.63013577  0.          1.        ]\n",
      " [ 0.11533243  0.88466758  0.          1.        ]]\n",
      "Minibatch total loss at step 5291: 0.233496\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.59215426  0.40784568  1.          0.        ]\n",
      " [ 0.6615994   0.3384006   1.          0.        ]]\n",
      "Validation accuracy: 70.0\n",
      "Minibatch total loss at step 5304: 0.266991\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.08924223  0.91075772  0.          1.        ]\n",
      " [ 0.10111275  0.89888722  0.          1.        ]]\n",
      "Minibatch total loss at step 5317: 0.219671\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.14095065  0.85904932  0.          1.        ]\n",
      " [ 0.17215298  0.82784706  0.          1.        ]]\n",
      "Minibatch total loss at step 5330: 0.320523\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.11436606  0.88563395  0.          1.        ]\n",
      " [ 0.15575077  0.84424925  0.          1.        ]]\n",
      "Minibatch total loss at step 5343: 0.175887\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.18353352  0.81646651  0.          1.        ]\n",
      " [ 0.25774801  0.74225205  0.          1.        ]]\n",
      "Minibatch total loss at step 5356: 0.341109\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.32003137  0.67996866  0.          1.        ]\n",
      " [ 0.35856503  0.64143497  0.          1.        ]]\n",
      "Minibatch total loss at step 5369: 0.309996\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.53332651  0.46667346  1.          0.        ]\n",
      " [ 0.11736337  0.88263655  0.          1.        ]]\n",
      "Minibatch total loss at step 5382: 0.278489\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.15202057  0.84797949  0.          1.        ]\n",
      " [ 0.25324732  0.74675268  0.          1.        ]]\n",
      "Minibatch total loss at step 5395: 0.271407\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.17132701  0.82867301  0.          1.        ]\n",
      " [ 0.15211205  0.84788793  0.          1.        ]]\n",
      "Validation accuracy: 68.5\n",
      "Minibatch total loss at step 5408: 0.274868\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.62706012  0.37293994  0.          1.        ]\n",
      " [ 0.48137048  0.51862955  0.          1.        ]]\n",
      "Minibatch total loss at step 5421: 0.199059\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.43752125  0.56247878  0.          1.        ]\n",
      " [ 0.73705739  0.26294264  1.          0.        ]]\n",
      "Minibatch total loss at step 5434: 0.193202\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.7355265   0.2644735   1.          0.        ]\n",
      " [ 0.84584206  0.15415797  1.          0.        ]]\n",
      "Minibatch total loss at step 5447: 0.212211\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.32067448  0.67932552  0.          1.        ]\n",
      " [ 0.52409852  0.4759014   1.          0.        ]]\n",
      "Minibatch total loss at step 5460: 0.176922\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.91309565  0.0869043   1.          0.        ]\n",
      " [ 0.21870655  0.78129345  0.          1.        ]]\n",
      "Minibatch total loss at step 5473: 0.243083\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.90580201  0.09419798  1.          0.        ]\n",
      " [ 0.01614103  0.983859    0.          1.        ]]\n",
      "Minibatch total loss at step 5486: 0.215218\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.16321652  0.83678347  0.          1.        ]\n",
      " [ 0.88325357  0.11674646  1.          0.        ]]\n",
      "Minibatch total loss at step 5499: 0.188755\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.66427124  0.33572873  1.          0.        ]\n",
      " [ 0.01178532  0.98821473  0.          1.        ]]\n",
      "Validation accuracy: 67.0\n",
      "Minibatch total loss at step 5512: 0.208311\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.78477371  0.21522628  1.          0.        ]\n",
      " [ 0.8619734   0.13802657  1.          0.        ]]\n",
      "Minibatch total loss at step 5525: 0.194959\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.11580224  0.88419777  0.          1.        ]\n",
      " [ 0.17605808  0.82394195  0.          1.        ]]\n",
      "Minibatch total loss at step 5538: 0.210806\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.1868742   0.81312579  0.          1.        ]\n",
      " [ 0.10675144  0.89324856  0.          1.        ]]\n",
      "Minibatch total loss at step 5551: 0.210621\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.08284634  0.9171536   0.          1.        ]\n",
      " [ 0.373422    0.62657797  0.          1.        ]]\n",
      "Minibatch total loss at step 5564: 0.188080\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.09265137  0.90734863  0.          1.        ]\n",
      " [ 0.82047528  0.17952475  1.          0.        ]]\n",
      "Minibatch total loss at step 5577: 0.481674\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.07014108  0.92985892  0.          1.        ]\n",
      " [ 0.43832305  0.56167692  0.          1.        ]]\n",
      "Minibatch total loss at step 5590: 0.274292\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.37789509  0.62210488  0.          1.        ]\n",
      " [ 0.57174391  0.42825609  0.          1.        ]]\n",
      "Validation accuracy: 65.0\n",
      "Minibatch total loss at step 5603: 0.319886\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.70354468  0.29645535  0.          1.        ]\n",
      " [ 0.73161155  0.26838848  0.          1.        ]]\n",
      "Minibatch total loss at step 5616: 0.201788\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.90386564  0.09613432  1.          0.        ]\n",
      " [ 0.21306624  0.78693372  0.          1.        ]]\n",
      "Minibatch total loss at step 5629: 0.291820\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.09723843  0.90276158  0.          1.        ]\n",
      " [ 0.09883146  0.90116858  0.          1.        ]]\n",
      "Minibatch total loss at step 5642: 0.239120\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.39858603  0.60141397  1.          0.        ]\n",
      " [ 0.33044496  0.66955501  0.          1.        ]]\n",
      "Minibatch total loss at step 5655: 0.193884\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.94834709  0.05165295  1.          0.        ]\n",
      " [ 0.08645644  0.91354352  0.          1.        ]]\n",
      "Minibatch total loss at step 5668: 0.211120\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.02434276  0.97565722  0.          1.        ]\n",
      " [ 0.11859105  0.88140899  0.          1.        ]]\n",
      "Minibatch total loss at step 5681: 0.189865\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.22984815  0.77015185  0.          1.        ]\n",
      " [ 0.91163737  0.08836259  1.          0.        ]]\n",
      "Minibatch total loss at step 5694: 0.209246\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.85505313  0.14494684  1.          0.        ]\n",
      " [ 0.54516816  0.45483184  0.          1.        ]]\n",
      "Validation accuracy: 69.5\n",
      "Minibatch total loss at step 5707: 0.189371\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.16523352  0.83476645  0.          1.        ]\n",
      " [ 0.05114764  0.94885236  0.          1.        ]]\n",
      "Minibatch total loss at step 5720: 0.208594\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.20966265  0.79033738  0.          1.        ]\n",
      " [ 0.1211755   0.87882447  0.          1.        ]]\n",
      "Minibatch total loss at step 5733: 0.159940\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.81895405  0.1810459   1.          0.        ]\n",
      " [ 0.19269021  0.80730975  0.          1.        ]]\n",
      "Minibatch total loss at step 5746: 0.170733\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.13683005  0.86316991  0.          1.        ]\n",
      " [ 0.64468044  0.35531947  1.          0.        ]]\n",
      "Minibatch total loss at step 5759: 0.293991\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.82689136  0.17310861  1.          0.        ]\n",
      " [ 0.97930866  0.02069136  1.          0.        ]]\n",
      "Minibatch total loss at step 5772: 0.359446\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.0865611   0.91343886  0.          1.        ]\n",
      " [ 0.10697955  0.89302045  0.          1.        ]]\n",
      "Minibatch total loss at step 5785: 0.248607\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.03860629  0.96139371  0.          1.        ]\n",
      " [ 0.77931798  0.22068208  1.          0.        ]]\n",
      "Minibatch total loss at step 5798: 0.385926\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.25899571  0.74100429  0.          1.        ]\n",
      " [ 0.50167918  0.49832085  0.          1.        ]]\n",
      "Validation accuracy: 72.5\n",
      "Minibatch total loss at step 5811: 0.190788\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.147682    0.85231805  0.          1.        ]\n",
      " [ 0.15161847  0.84838152  0.          1.        ]]\n",
      "Minibatch total loss at step 5824: 0.250515\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.02202442  0.97797555  0.          1.        ]\n",
      " [ 0.32133061  0.67866945  0.          1.        ]]\n",
      "Minibatch total loss at step 5837: 0.288145\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.03692711  0.96307296  0.          1.        ]\n",
      " [ 0.50559598  0.49440408  0.          1.        ]]\n",
      "Minibatch total loss at step 5850: 0.213154\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.15970433  0.84029567  0.          1.        ]\n",
      " [ 0.34360746  0.65639251  0.          1.        ]]\n",
      "Minibatch total loss at step 5863: 0.247887\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.7873596   0.2126404   1.          0.        ]\n",
      " [ 0.01195439  0.98804563  0.          1.        ]]\n",
      "Minibatch total loss at step 5876: 0.207046\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.06937597  0.93062407  0.          1.        ]\n",
      " [ 0.10787269  0.89212734  0.          1.        ]]\n",
      "Minibatch total loss at step 5889: 0.199504\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.92512012  0.07487986  1.          0.        ]\n",
      " [ 0.8694548   0.1305452   1.          0.        ]]\n",
      "Validation accuracy: 65.0\n",
      "Minibatch total loss at step 5902: 0.191886\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.01268714  0.98731285  0.          1.        ]\n",
      " [ 0.73901796  0.26098201  1.          0.        ]]\n",
      "Minibatch total loss at step 5915: 0.184824\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.33185449  0.66814554  0.          1.        ]\n",
      " [ 0.90904737  0.09095269  1.          0.        ]]\n",
      "Minibatch total loss at step 5928: 0.299588\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.90361291  0.09638707  1.          0.        ]\n",
      " [ 0.67598569  0.32401431  1.          0.        ]]\n",
      "Minibatch total loss at step 5941: 0.176161\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.90851253  0.09148747  1.          0.        ]\n",
      " [ 0.10027264  0.89972734  0.          1.        ]]\n",
      "Minibatch total loss at step 5954: 0.217885\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.94340158  0.05659841  1.          0.        ]\n",
      " [ 0.73818529  0.26181468  1.          0.        ]]\n",
      "Minibatch total loss at step 5967: 0.237956\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.02316293  0.9768371   0.          1.        ]\n",
      " [ 0.03286413  0.96713591  0.          1.        ]]\n",
      "Minibatch total loss at step 5980: 0.291790\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.80141294  0.19858706  1.          0.        ]\n",
      " [ 0.29148471  0.70851523  0.          1.        ]]\n",
      "Minibatch total loss at step 5993: 0.213060\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.00642285  0.99357718  0.          1.        ]\n",
      " [ 0.65337354  0.34662646  1.          0.        ]]\n",
      "Validation accuracy: 68.5\n",
      "Minibatch total loss at step 6006: 0.204437\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.72549385  0.27450612  1.          0.        ]\n",
      " [ 0.19239272  0.80760729  0.          1.        ]]\n",
      "Minibatch total loss at step 6019: 0.256308\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.87984174  0.12015823  1.          0.        ]\n",
      " [ 0.88301623  0.11698382  1.          0.        ]]\n",
      "Minibatch total loss at step 6032: 0.267917\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.18143781  0.81856215  0.          1.        ]\n",
      " [ 0.97557425  0.02442575  1.          0.        ]]\n",
      "Minibatch total loss at step 6045: 0.211104\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.30289289  0.69710708  0.          1.        ]\n",
      " [ 0.57369959  0.42630041  1.          0.        ]]\n",
      "Minibatch total loss at step 6058: 0.184386\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.43552577  0.56447423  0.          1.        ]\n",
      " [ 0.95239913  0.04760088  1.          0.        ]]\n",
      "Minibatch total loss at step 6071: 0.287951\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.96323818  0.03676185  1.          0.        ]\n",
      " [ 0.1723005   0.82769954  0.          1.        ]]\n",
      "Minibatch total loss at step 6084: 0.201371\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.87515873  0.12484127  1.          0.        ]\n",
      " [ 0.15001568  0.84998429  0.          1.        ]]\n",
      "Minibatch total loss at step 6097: 0.194346\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.16773202  0.832268    0.          1.        ]\n",
      " [ 0.81929898  0.18070103  1.          0.        ]]\n",
      "Validation accuracy: 68.0\n",
      "Minibatch total loss at step 6110: 0.217544\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.19378963  0.80621034  0.          1.        ]\n",
      " [ 0.22695492  0.77304506  0.          1.        ]]\n",
      "Minibatch total loss at step 6123: 0.219435\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.07296092  0.92703909  0.          1.        ]\n",
      " [ 0.26370463  0.73629546  0.          1.        ]]\n",
      "Minibatch total loss at step 6136: 0.181446\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.68923467  0.31076536  1.          0.        ]\n",
      " [ 0.74645597  0.25354403  1.          0.        ]]\n",
      "Minibatch total loss at step 6149: 0.249400\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.10486672  0.89513326  0.          1.        ]\n",
      " [ 0.94725412  0.05274584  1.          0.        ]]\n",
      "Minibatch total loss at step 6162: 0.183881\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.89277381  0.1072262   1.          0.        ]\n",
      " [ 0.09853736  0.90146261  0.          1.        ]]\n",
      "Minibatch total loss at step 6175: 0.270002\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.47851062  0.52148938  0.          1.        ]\n",
      " [ 0.71541238  0.28458759  1.          0.        ]]\n",
      "Minibatch total loss at step 6188: 0.209701\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.09624148  0.90375853  0.          1.        ]\n",
      " [ 0.69553292  0.30446717  1.          0.        ]]\n",
      "Validation accuracy: 65.0\n",
      "Minibatch total loss at step 6201: 0.200272\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.04301369  0.95698631  0.          1.        ]\n",
      " [ 0.17514452  0.82485539  0.          1.        ]]\n",
      "Minibatch total loss at step 6214: 0.268809\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.18616216  0.81383789  0.          1.        ]\n",
      " [ 0.98727798  0.012722    1.          0.        ]]\n",
      "Minibatch total loss at step 6227: 0.225155\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.37311473  0.62688529  0.          1.        ]\n",
      " [ 0.18568893  0.81431109  0.          1.        ]]\n",
      "Minibatch total loss at step 6240: 0.147716\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.29682493  0.70317501  0.          1.        ]\n",
      " [ 0.1039215   0.89607847  0.          1.        ]]\n",
      "Minibatch total loss at step 6253: 0.196777\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.30315295  0.69684702  0.          1.        ]\n",
      " [ 0.79277182  0.20722815  1.          0.        ]]\n",
      "Minibatch total loss at step 6266: 0.374537\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.00409205  0.9959079   0.          1.        ]\n",
      " [ 0.39226025  0.60773981  1.          0.        ]]\n",
      "Minibatch total loss at step 6279: 0.173283\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.92590779  0.07409221  1.          0.        ]\n",
      " [ 0.10125869  0.8987413   0.          1.        ]]\n",
      "Minibatch total loss at step 6292: 0.176103\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.02327302  0.97672701  0.          1.        ]\n",
      " [ 0.95195007  0.04804995  1.          0.        ]]\n",
      "Validation accuracy: 66.0\n",
      "Minibatch total loss at step 6305: 0.221027\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.96361715  0.03638282  1.          0.        ]\n",
      " [ 0.94970572  0.05029424  1.          0.        ]]\n",
      "Minibatch total loss at step 6318: 0.176221\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.8600412   0.13995883  1.          0.        ]\n",
      " [ 0.13098647  0.86901349  0.          1.        ]]\n",
      "Minibatch total loss at step 6331: 0.207744\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.08557878  0.91442126  0.          1.        ]\n",
      " [ 0.89072013  0.1092799   1.          0.        ]]\n",
      "Minibatch total loss at step 6344: 0.218994\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.07287192  0.92712814  0.          1.        ]\n",
      " [ 0.06701814  0.93298185  0.          1.        ]]\n",
      "Minibatch total loss at step 6357: 0.240502\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.15464216  0.84535789  0.          1.        ]\n",
      " [ 0.54934478  0.45065522  1.          0.        ]]\n",
      "Minibatch total loss at step 6370: 0.222134\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.06003123  0.93996871  0.          1.        ]\n",
      " [ 0.02703012  0.97296989  0.          1.        ]]\n",
      "Minibatch total loss at step 6383: 0.180027\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.82128388  0.17871615  1.          0.        ]\n",
      " [ 0.21824419  0.78175581  0.          1.        ]]\n",
      "Minibatch total loss at step 6396: 0.182825\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.0506712   0.94932878  0.          1.        ]\n",
      " [ 0.07806097  0.92193902  0.          1.        ]]\n",
      "Validation accuracy: 68.5\n",
      "Minibatch total loss at step 6409: 0.225966\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.15971793  0.84028208  0.          1.        ]\n",
      " [ 0.49243435  0.50756562  0.          1.        ]]\n",
      "Minibatch total loss at step 6422: 0.275577\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.48647186  0.51352811  1.          0.        ]\n",
      " [ 0.02840249  0.97159755  0.          1.        ]]\n",
      "Minibatch total loss at step 6435: 0.160218\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.0494572   0.95054281  0.          1.        ]\n",
      " [ 0.15199929  0.84800071  0.          1.        ]]\n",
      "Minibatch total loss at step 6448: 0.210043\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.11524224  0.88475782  0.          1.        ]\n",
      " [ 0.33506709  0.66493285  0.          1.        ]]\n",
      "Minibatch total loss at step 6461: 0.202814\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.04353756  0.95646244  0.          1.        ]\n",
      " [ 0.37387419  0.62612581  0.          1.        ]]\n",
      "Minibatch total loss at step 6474: 0.211887\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.11392614  0.88607383  0.          1.        ]\n",
      " [ 0.8633281   0.13667184  1.          0.        ]]\n",
      "Minibatch total loss at step 6487: 0.237377\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.12912346  0.87087655  0.          1.        ]\n",
      " [ 0.89715475  0.10284519  1.          0.        ]]\n",
      "Minibatch total loss at step 6500: 0.202760\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.67409194  0.32590804  1.          0.        ]\n",
      " [ 0.88286769  0.11713231  1.          0.        ]]\n",
      "Validation accuracy: 68.5\n",
      "Minibatch total loss at step 6513: 0.163512\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.92527264  0.07472739  1.          0.        ]\n",
      " [ 0.03868018  0.96131974  0.          1.        ]]\n",
      "Minibatch total loss at step 6526: 0.193643\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.69469011  0.30530989  1.          0.        ]\n",
      " [ 0.07333847  0.92666149  0.          1.        ]]\n",
      "Minibatch total loss at step 6539: 0.211231\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.04694538  0.95305467  0.          1.        ]\n",
      " [ 0.17011617  0.82988387  0.          1.        ]]\n",
      "Minibatch total loss at step 6552: 0.172442\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.1200148   0.87998515  0.          1.        ]\n",
      " [ 0.06214316  0.93785685  0.          1.        ]]\n",
      "Minibatch total loss at step 6565: 0.265990\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.0772313   0.92276871  0.          1.        ]\n",
      " [ 0.59767002  0.40233004  0.          1.        ]]\n",
      "Minibatch total loss at step 6578: 0.176485\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.05643728  0.94356269  0.          1.        ]\n",
      " [ 0.71396291  0.28603709  1.          0.        ]]\n",
      "Minibatch total loss at step 6591: 0.290631\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.05187549  0.94812447  0.          1.        ]\n",
      " [ 0.23714942  0.76285058  0.          1.        ]]\n",
      "Validation accuracy: 64.0\n",
      "Minibatch total loss at step 6604: 0.203275\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.8476249   0.15237512  1.          0.        ]\n",
      " [ 0.75994575  0.24005425  1.          0.        ]]\n",
      "Minibatch total loss at step 6617: 0.215959\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.13170192  0.86829811  0.          1.        ]\n",
      " [ 0.76969439  0.23030561  1.          0.        ]]\n",
      "Minibatch total loss at step 6630: 0.263269\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.08141373  0.91858625  0.          1.        ]\n",
      " [ 0.74351025  0.25648975  1.          0.        ]]\n",
      "Minibatch total loss at step 6643: 0.345376\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.84189969  0.15810029  1.          0.        ]\n",
      " [ 0.02536973  0.9746303   0.          1.        ]]\n",
      "Minibatch total loss at step 6656: 0.201291\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.84461731  0.15538271  1.          0.        ]\n",
      " [ 0.15835302  0.84164697  0.          1.        ]]\n",
      "Minibatch total loss at step 6669: 0.158321\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.84624165  0.15375836  1.          0.        ]\n",
      " [ 0.13788679  0.86211318  0.          1.        ]]\n",
      "Minibatch total loss at step 6682: 0.199694\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.26625577  0.73374414  0.          1.        ]\n",
      " [ 0.21462342  0.78537655  0.          1.        ]]\n",
      "Minibatch total loss at step 6695: 0.204093\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.98903251  0.01096741  1.          0.        ]\n",
      " [ 0.69435108  0.30564895  1.          0.        ]]\n",
      "Validation accuracy: 69.0\n",
      "Minibatch total loss at step 6708: 0.230126\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.29423937  0.7057606   0.          1.        ]\n",
      " [ 0.11559696  0.88440305  0.          1.        ]]\n",
      "Minibatch total loss at step 6721: 0.154938\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.12504603  0.87495399  0.          1.        ]\n",
      " [ 0.08412776  0.91587228  0.          1.        ]]\n",
      "Minibatch total loss at step 6734: 0.188995\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.24900927  0.75099069  0.          1.        ]\n",
      " [ 0.47433859  0.52566141  0.          1.        ]]\n",
      "Minibatch total loss at step 6747: 0.191920\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.13928019  0.8607198   0.          1.        ]\n",
      " [ 0.12647343  0.87352651  0.          1.        ]]\n",
      "Minibatch total loss at step 6760: 0.297580\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.2614736   0.7385264   0.          1.        ]\n",
      " [ 0.97597688  0.02402314  1.          0.        ]]\n",
      "Minibatch total loss at step 6773: 0.222566\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.76589745  0.23410262  1.          0.        ]\n",
      " [ 0.03449736  0.96550268  0.          1.        ]]\n",
      "Minibatch total loss at step 6786: 0.204950\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.16258936  0.83741063  0.          1.        ]\n",
      " [ 0.15342526  0.84657478  0.          1.        ]]\n",
      "Minibatch total loss at step 6799: 0.169225\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.91973323  0.08026674  1.          0.        ]\n",
      " [ 0.76587713  0.23412283  1.          0.        ]]\n",
      "Validation accuracy: 71.0\n",
      "Minibatch total loss at step 6812: 0.179618\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.97326195  0.02673804  1.          0.        ]\n",
      " [ 0.02956919  0.97043085  0.          1.        ]]\n",
      "Minibatch total loss at step 6825: 0.175711\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.07548337  0.92451662  0.          1.        ]\n",
      " [ 0.11423052  0.88576949  0.          1.        ]]\n",
      "Minibatch total loss at step 6838: 0.214915\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.23187749  0.76812255  0.          1.        ]\n",
      " [ 0.212865    0.787135    0.          1.        ]]\n",
      "Minibatch total loss at step 6851: 0.212935\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.12684605  0.87315392  0.          1.        ]\n",
      " [ 0.9298082   0.07019173  1.          0.        ]]\n",
      "Minibatch total loss at step 6864: 0.311912\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.31751823  0.68248171  0.          1.        ]\n",
      " [ 0.27137733  0.72862267  0.          1.        ]]\n",
      "Minibatch total loss at step 6877: 0.190017\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.83095765  0.16904238  1.          0.        ]\n",
      " [ 0.50837296  0.49162701  1.          0.        ]]\n",
      "Minibatch total loss at step 6890: 0.184931\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.06470607  0.93529391  0.          1.        ]\n",
      " [ 0.77009273  0.22990721  1.          0.        ]]\n",
      "Validation accuracy: 66.0\n",
      "Minibatch total loss at step 6903: 0.222474\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.33728591  0.66271412  0.          1.        ]\n",
      " [ 0.79788113  0.20211887  1.          0.        ]]\n",
      "Minibatch total loss at step 6916: 0.221578\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.07211965  0.92788035  0.          1.        ]\n",
      " [ 0.93454254  0.0654575   1.          0.        ]]\n",
      "Minibatch total loss at step 6929: 0.220777\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.21608171  0.78391832  0.          1.        ]\n",
      " [ 0.70262408  0.29737586  1.          0.        ]]\n",
      "Minibatch total loss at step 6942: 0.232648\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.0619631   0.93803692  0.          1.        ]\n",
      " [ 0.95283288  0.04716714  1.          0.        ]]\n",
      "Minibatch total loss at step 6955: 0.207844\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.87746447  0.12253546  1.          0.        ]\n",
      " [ 0.16037142  0.83962858  0.          1.        ]]\n",
      "Minibatch total loss at step 6968: 0.172088\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.2528345   0.7471655   0.          1.        ]\n",
      " [ 0.21082664  0.78917336  0.          1.        ]]\n",
      "Minibatch total loss at step 6981: 0.170789\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.82243127  0.17756878  1.          0.        ]\n",
      " [ 0.04953577  0.95046419  0.          1.        ]]\n",
      "Minibatch total loss at step 6994: 0.270230\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.3913472   0.60865283  1.          0.        ]\n",
      " [ 0.14165111  0.85834891  0.          1.        ]]\n",
      "Validation accuracy: 64.5\n",
      "Minibatch total loss at step 7007: 0.200778\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.69144315  0.30855691  1.          0.        ]\n",
      " [ 0.76102573  0.23897424  1.          0.        ]]\n",
      "Minibatch total loss at step 7020: 0.237970\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.95195979  0.04804018  1.          0.        ]\n",
      " [ 0.30770528  0.69229466  0.          1.        ]]\n",
      "Minibatch total loss at step 7033: 0.236341\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.86929995  0.1307001   1.          0.        ]\n",
      " [ 0.01911459  0.98088545  0.          1.        ]]\n",
      "Minibatch total loss at step 7046: 0.211888\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.10995094  0.89004904  0.          1.        ]\n",
      " [ 0.163164    0.83683598  0.          1.        ]]\n",
      "Minibatch total loss at step 7059: 0.180850\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.11053359  0.88946646  0.          1.        ]\n",
      " [ 0.33310485  0.66689521  0.          1.        ]]\n",
      "Minibatch total loss at step 7072: 0.199893\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.97604412  0.02395595  1.          0.        ]\n",
      " [ 0.85008466  0.14991537  1.          0.        ]]\n",
      "Minibatch total loss at step 7085: 0.177884\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.83709288  0.16290717  1.          0.        ]\n",
      " [ 0.81359643  0.18640359  1.          0.        ]]\n",
      "Minibatch total loss at step 7098: 0.181307\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.02663997  0.97336     0.          1.        ]\n",
      " [ 0.85081267  0.14918734  1.          0.        ]]\n",
      "Validation accuracy: 61.0\n",
      "Minibatch total loss at step 7111: 0.186054\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.09750888  0.90249109  0.          1.        ]\n",
      " [ 0.95272064  0.04727942  1.          0.        ]]\n",
      "Minibatch total loss at step 7124: 0.192616\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.03328244  0.9667176   0.          1.        ]\n",
      " [ 0.84201801  0.15798196  1.          0.        ]]\n",
      "Minibatch total loss at step 7137: 0.202920\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.83657527  0.16342469  1.          0.        ]\n",
      " [ 0.1201179   0.87988216  0.          1.        ]]\n",
      "Minibatch total loss at step 7150: 0.209944\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.9638955   0.03610454  1.          0.        ]\n",
      " [ 0.81405026  0.18594971  1.          0.        ]]\n",
      "Minibatch total loss at step 7163: 0.183421\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.11075453  0.88924545  0.          1.        ]\n",
      " [ 0.87243205  0.12756795  1.          0.        ]]\n",
      "Minibatch total loss at step 7176: 0.163942\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.15340021  0.84659976  0.          1.        ]\n",
      " [ 0.89472425  0.10527574  1.          0.        ]]\n",
      "Minibatch total loss at step 7189: 0.215958\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.10057067  0.89942938  0.          1.        ]\n",
      " [ 0.45192197  0.54807806  1.          0.        ]]\n",
      "Validation accuracy: 68.0\n",
      "Minibatch total loss at step 7202: 0.161695\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.25409466  0.74590534  0.          1.        ]\n",
      " [ 0.07117769  0.92882234  0.          1.        ]]\n",
      "Minibatch total loss at step 7215: 0.154024\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.10912133  0.89087868  0.          1.        ]\n",
      " [ 0.84961456  0.15038544  1.          0.        ]]\n",
      "Minibatch total loss at step 7228: 0.420409\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.182942    0.81705791  0.          1.        ]\n",
      " [ 0.96997917  0.03002075  1.          0.        ]]\n",
      "Minibatch total loss at step 7241: 0.168804\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.20372     0.79628003  0.          1.        ]\n",
      " [ 0.85012937  0.14987068  1.          0.        ]]\n",
      "Minibatch total loss at step 7254: 0.188378\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.25567997  0.74432003  0.          1.        ]\n",
      " [ 0.28985834  0.71014166  0.          1.        ]]\n",
      "Minibatch total loss at step 7267: 0.222992\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.07918868  0.92081136  0.          1.        ]\n",
      " [ 0.89518285  0.10481714  1.          0.        ]]\n",
      "Minibatch total loss at step 7280: 0.188470\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.20934165  0.79065841  0.          1.        ]\n",
      " [ 0.66478795  0.33521202  1.          0.        ]]\n",
      "Minibatch total loss at step 7293: 0.172251\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.93400955  0.0659904   1.          0.        ]\n",
      " [ 0.26099694  0.73900306  0.          1.        ]]\n",
      "Validation accuracy: 65.0\n",
      "Minibatch total loss at step 7306: 0.163519\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.15159182  0.84840816  0.          1.        ]\n",
      " [ 0.78005183  0.21994819  1.          0.        ]]\n",
      "Minibatch total loss at step 7319: 0.158189\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.06135207  0.93864787  0.          1.        ]\n",
      " [ 0.92591834  0.07408164  1.          0.        ]]\n",
      "Minibatch total loss at step 7332: 0.191241\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.11386223  0.88613778  0.          1.        ]\n",
      " [ 0.11885805  0.88114196  0.          1.        ]]\n",
      "Minibatch total loss at step 7345: 0.243386\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.81152678  0.18847315  1.          0.        ]\n",
      " [ 0.54207468  0.45792538  1.          0.        ]]\n",
      "Minibatch total loss at step 7358: 0.171863\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.78681606  0.21318394  1.          0.        ]\n",
      " [ 0.87482601  0.12517399  1.          0.        ]]\n",
      "Minibatch total loss at step 7371: 0.188420\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.95150924  0.04849084  1.          0.        ]\n",
      " [ 0.44847104  0.55152899  1.          0.        ]]\n",
      "Minibatch total loss at step 7384: 0.206043\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.11491989  0.8850801   0.          1.        ]\n",
      " [ 0.2780751   0.7219249   0.          1.        ]]\n",
      "Minibatch total loss at step 7397: 0.157885\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.11787008  0.88212991  0.          1.        ]\n",
      " [ 0.84152496  0.15847506  1.          0.        ]]\n",
      "Validation accuracy: 69.5\n",
      "Minibatch total loss at step 7410: 0.199407\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.75511372  0.24488625  1.          0.        ]\n",
      " [ 0.53705853  0.46294144  1.          0.        ]]\n",
      "Minibatch total loss at step 7423: 0.214400\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.1452076   0.85479242  0.          1.        ]\n",
      " [ 0.80983424  0.19016577  1.          0.        ]]\n",
      "Minibatch total loss at step 7436: 0.227742\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.33119079  0.66880918  0.          1.        ]\n",
      " [ 0.09391745  0.90608251  0.          1.        ]]\n",
      "Minibatch total loss at step 7449: 0.181598\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.31589416  0.68410587  0.          1.        ]\n",
      " [ 0.03539052  0.9646095   0.          1.        ]]\n",
      "Minibatch total loss at step 7462: 0.181739\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.8732878   0.12671219  1.          0.        ]\n",
      " [ 0.7678175   0.23218252  1.          0.        ]]\n",
      "Minibatch total loss at step 7475: 0.311722\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.02994207  0.9700579   0.          1.        ]\n",
      " [ 0.52723372  0.47276628  1.          0.        ]]\n",
      "Minibatch total loss at step 7488: 0.236556\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.02374516  0.97625482  0.          1.        ]\n",
      " [ 0.84636867  0.15363134  1.          0.        ]]\n",
      "Validation accuracy: 66.5\n",
      "Minibatch total loss at step 7501: 0.231006\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.56681633  0.43318367  1.          0.        ]\n",
      " [ 0.37907287  0.62092716  1.          0.        ]]\n",
      "Minibatch total loss at step 7514: 0.153424\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.84196639  0.15803361  1.          0.        ]\n",
      " [ 0.08113001  0.91886991  0.          1.        ]]\n",
      "Minibatch total loss at step 7527: 0.301909\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.31157896  0.68842107  0.          1.        ]\n",
      " [ 0.42614803  0.57385194  0.          1.        ]]\n",
      "Minibatch total loss at step 7540: 0.240983\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.6562795  0.3437205  1.         0.       ]\n",
      " [ 0.0336923  0.9663077  0.         1.       ]]\n",
      "Minibatch total loss at step 7553: 0.192402\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.82724953  0.17275052  1.          0.        ]\n",
      " [ 0.17493176  0.82506824  0.          1.        ]]\n",
      "Minibatch total loss at step 7566: 0.235807\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.7582022   0.24179786  1.          0.        ]\n",
      " [ 0.04134577  0.95865422  0.          1.        ]]\n",
      "Minibatch total loss at step 7579: 0.187694\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.29536653  0.70463347  0.          1.        ]\n",
      " [ 0.72149634  0.27850366  1.          0.        ]]\n",
      "Minibatch total loss at step 7592: 0.250677\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.73025429  0.26974568  1.          0.        ]\n",
      " [ 0.23513582  0.76486415  0.          1.        ]]\n",
      "Validation accuracy: 68.0\n",
      "Minibatch total loss at step 7605: 0.160943\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.92767119  0.07232883  1.          0.        ]\n",
      " [ 0.81418544  0.18581459  1.          0.        ]]\n",
      "Minibatch total loss at step 7618: 0.196843\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.95643598  0.04356405  1.          0.        ]\n",
      " [ 0.58432561  0.41567439  1.          0.        ]]\n",
      "Minibatch total loss at step 7631: 0.175741\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.07508522  0.92491478  0.          1.        ]\n",
      " [ 0.70203888  0.29796106  1.          0.        ]]\n",
      "Minibatch total loss at step 7644: 0.152831\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.02667878  0.97332126  0.          1.        ]\n",
      " [ 0.86785871  0.13214125  1.          0.        ]]\n",
      "Minibatch total loss at step 7657: 0.240199\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.9284426   0.07155741  1.          0.        ]\n",
      " [ 0.99183559  0.00816449  1.          0.        ]]\n",
      "Minibatch total loss at step 7670: 0.184835\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.09520619  0.90479386  0.          1.        ]\n",
      " [ 0.77114731  0.22885278  1.          0.        ]]\n",
      "Minibatch total loss at step 7683: 0.168866\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.6897369   0.31026307  1.          0.        ]\n",
      " [ 0.72610587  0.27389416  1.          0.        ]]\n",
      "Minibatch total loss at step 7696: 0.184750\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.59750372  0.40249634  1.          0.        ]\n",
      " [ 0.05475978  0.9452402   0.          1.        ]]\n",
      "Validation accuracy: 65.0\n",
      "Minibatch total loss at step 7709: 0.203472\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.78138435  0.21861564  1.          0.        ]\n",
      " [ 0.15508169  0.84491837  0.          1.        ]]\n",
      "Minibatch total loss at step 7722: 0.246708\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.7775045   0.22249556  1.          0.        ]\n",
      " [ 0.67013812  0.32986191  1.          0.        ]]\n",
      "Minibatch total loss at step 7735: 0.181483\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.8288759   0.17112412  1.          0.        ]\n",
      " [ 0.80809176  0.19190823  1.          0.        ]]\n",
      "Minibatch total loss at step 7748: 0.217689\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.71863562  0.28136429  1.          0.        ]\n",
      " [ 0.78870654  0.21129346  1.          0.        ]]\n",
      "Minibatch total loss at step 7761: 0.235427\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.16481039  0.83518964  0.          1.        ]\n",
      " [ 0.52594584  0.47405422  0.          1.        ]]\n",
      "Minibatch total loss at step 7774: 0.197800\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.30493721  0.69506282  0.          1.        ]\n",
      " [ 0.2090061   0.79099399  0.          1.        ]]\n",
      "Minibatch total loss at step 7787: 0.187886\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.74870807  0.25129196  1.          0.        ]\n",
      " [ 0.13137402  0.868626    0.          1.        ]]\n",
      "Minibatch total loss at step 7800: 0.187013\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.2895928   0.71040726  0.          1.        ]\n",
      " [ 0.89863515  0.10136486  1.          0.        ]]\n",
      "Validation accuracy: 65.5\n",
      "Minibatch total loss at step 7813: 0.497813\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.00539757  0.99460238  0.          1.        ]\n",
      " [ 0.47567603  0.52432394  1.          0.        ]]\n",
      "Minibatch total loss at step 7826: 0.182008\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.2837252   0.7162748   0.          1.        ]\n",
      " [ 0.1850926   0.81490743  0.          1.        ]]\n",
      "Minibatch total loss at step 7839: 0.163418\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.13057846  0.8694216   0.          1.        ]\n",
      " [ 0.08277683  0.91722322  0.          1.        ]]\n",
      "Minibatch total loss at step 7852: 0.213847\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.37995175  0.62004822  0.          1.        ]\n",
      " [ 0.29093656  0.70906341  0.          1.        ]]\n",
      "Minibatch total loss at step 7865: 0.197142\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.88826811  0.11173184  1.          0.        ]\n",
      " [ 0.38021457  0.61978543  0.          1.        ]]\n",
      "Minibatch total loss at step 7878: 0.181511\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.24538761  0.75461239  0.          1.        ]\n",
      " [ 0.86971843  0.1302816   1.          0.        ]]\n",
      "Minibatch total loss at step 7891: 0.202890\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.15367226  0.84632778  0.          1.        ]\n",
      " [ 0.07065654  0.92934346  0.          1.        ]]\n",
      "Validation accuracy: 66.5\n",
      "Minibatch total loss at step 7904: 0.152399\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.08341251  0.91658753  0.          1.        ]\n",
      " [ 0.84773016  0.15226983  1.          0.        ]]\n",
      "Minibatch total loss at step 7917: 0.200386\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.58560127  0.41439867  1.          0.        ]\n",
      " [ 0.05465501  0.94534504  0.          1.        ]]\n",
      "Minibatch total loss at step 7930: 0.165471\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.64010328  0.35989672  1.          0.        ]\n",
      " [ 0.19330871  0.80669123  0.          1.        ]]\n",
      "Minibatch total loss at step 7943: 0.152468\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.85796899  0.14203101  1.          0.        ]\n",
      " [ 0.28339648  0.71660352  0.          1.        ]]\n",
      "Minibatch total loss at step 7956: 0.210415\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.01062516  0.98937476  0.          1.        ]\n",
      " [ 0.73295403  0.267046    1.          0.        ]]\n",
      "Minibatch total loss at step 7969: 0.182201\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.92853564  0.0714644   1.          0.        ]\n",
      " [ 0.13771296  0.86228704  0.          1.        ]]\n",
      "Minibatch total loss at step 7982: 0.243074\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.91098434  0.08901562  1.          0.        ]\n",
      " [ 0.74826199  0.25173801  1.          0.        ]]\n",
      "Minibatch total loss at step 7995: 0.210370\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.64392537  0.35607466  1.          0.        ]\n",
      " [ 0.48193598  0.51806396  1.          0.        ]]\n",
      "Validation accuracy: 64.5\n",
      "Minibatch total loss at step 8008: 0.183469\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.91710657  0.08289336  1.          0.        ]\n",
      " [ 0.85563266  0.14436735  1.          0.        ]]\n",
      "Minibatch total loss at step 8021: 0.165638\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.11805226  0.88194776  0.          1.        ]\n",
      " [ 0.1055512   0.89444882  0.          1.        ]]\n",
      "Minibatch total loss at step 8034: 0.157272\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.76597059  0.23402938  1.          0.        ]\n",
      " [ 0.86802685  0.13197313  1.          0.        ]]\n",
      "Minibatch total loss at step 8047: 0.255585\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.47522727  0.5247727   0.          1.        ]\n",
      " [ 0.91938555  0.08061445  1.          0.        ]]\n",
      "Minibatch total loss at step 8060: 0.174931\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.91638166  0.08361834  1.          0.        ]\n",
      " [ 0.06320117  0.93679881  0.          1.        ]]\n",
      "Minibatch total loss at step 8073: 0.178696\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.02765476  0.97234523  0.          1.        ]\n",
      " [ 0.25977722  0.74022281  0.          1.        ]]\n",
      "Minibatch total loss at step 8086: 0.203158\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.74882025  0.2511797   1.          0.        ]\n",
      " [ 0.86298865  0.1370113   1.          0.        ]]\n",
      "Minibatch total loss at step 8099: 0.184339\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.80175394  0.19824606  1.          0.        ]\n",
      " [ 0.68490934  0.31509063  1.          0.        ]]\n",
      "Validation accuracy: 65.5\n",
      "Minibatch total loss at step 8112: 0.199308\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.10523887  0.8947612   0.          1.        ]\n",
      " [ 0.76108819  0.23891184  1.          0.        ]]\n",
      "Minibatch total loss at step 8125: 0.179821\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.26135391  0.73864603  0.          1.        ]\n",
      " [ 0.86253548  0.13746449  1.          0.        ]]\n",
      "Minibatch total loss at step 8138: 0.227712\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.4290038   0.57099622  0.          1.        ]\n",
      " [ 0.96429348  0.03570651  1.          0.        ]]\n",
      "Minibatch total loss at step 8151: 0.157232\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.28602269  0.71397734  0.          1.        ]\n",
      " [ 0.09517956  0.90482038  0.          1.        ]]\n",
      "Minibatch total loss at step 8164: 0.182345\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.09060542  0.90939462  0.          1.        ]\n",
      " [ 0.78475463  0.21524537  1.          0.        ]]\n",
      "Minibatch total loss at step 8177: 0.167652\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.90559697  0.09440303  1.          0.        ]\n",
      " [ 0.16707191  0.83292812  0.          1.        ]]\n",
      "Minibatch total loss at step 8190: 0.177582\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.07041747  0.92958248  0.          1.        ]\n",
      " [ 0.07246079  0.92753923  0.          1.        ]]\n",
      "Validation accuracy: 67.0\n",
      "Minibatch total loss at step 8203: 0.238207\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.02871232  0.97128773  0.          1.        ]\n",
      " [ 0.64762723  0.35237274  0.          1.        ]]\n",
      "Minibatch total loss at step 8216: 0.205004\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.68570936  0.31429064  1.          0.        ]\n",
      " [ 0.22387233  0.77612764  0.          1.        ]]\n",
      "Minibatch total loss at step 8229: 0.169649\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.91576153  0.08423855  1.          0.        ]\n",
      " [ 0.12938611  0.87061393  0.          1.        ]]\n",
      "Minibatch total loss at step 8242: 0.310193\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.29373997  0.70625997  0.          1.        ]\n",
      " [ 0.37564796  0.62435204  0.          1.        ]]\n",
      "Minibatch total loss at step 8255: 0.175838\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.04644483  0.95355517  0.          1.        ]\n",
      " [ 0.0372228   0.96277714  0.          1.        ]]\n",
      "Minibatch total loss at step 8268: 0.162764\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.25964183  0.74035823  0.          1.        ]\n",
      " [ 0.20856793  0.79143208  0.          1.        ]]\n",
      "Minibatch total loss at step 8281: 0.173708\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.04685673  0.95314324  0.          1.        ]\n",
      " [ 0.86772114  0.13227892  1.          0.        ]]\n",
      "Minibatch total loss at step 8294: 0.187238\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.93133223  0.06866774  1.          0.        ]\n",
      " [ 0.82874656  0.17125347  1.          0.        ]]\n",
      "Validation accuracy: 64.5\n",
      "Minibatch total loss at step 8307: 0.249259\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.02118005  0.97881991  0.          1.        ]\n",
      " [ 0.06452756  0.93547249  0.          1.        ]]\n",
      "Minibatch total loss at step 8320: 0.167505\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.11832504  0.88167495  0.          1.        ]\n",
      " [ 0.76660156  0.23339842  1.          0.        ]]\n",
      "Minibatch total loss at step 8333: 0.242003\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.15992515  0.8400749   0.          1.        ]\n",
      " [ 0.03798707  0.96201289  0.          1.        ]]\n",
      "Minibatch total loss at step 8346: 0.182247\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.2940802   0.70591986  0.          1.        ]\n",
      " [ 0.06496907  0.935031    0.          1.        ]]\n",
      "Minibatch total loss at step 8359: 0.176759\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.18883042  0.81116956  0.          1.        ]\n",
      " [ 0.08772715  0.91227287  0.          1.        ]]\n",
      "Minibatch total loss at step 8372: 0.197119\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.45128873  0.54871118  0.          1.        ]\n",
      " [ 0.10013857  0.8998614   0.          1.        ]]\n",
      "Minibatch total loss at step 8385: 0.160402\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.04658676  0.95341319  0.          1.        ]\n",
      " [ 0.07131911  0.92868084  0.          1.        ]]\n",
      "Minibatch total loss at step 8398: 0.211398\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.98554033  0.01445965  1.          0.        ]\n",
      " [ 0.01751436  0.98248565  0.          1.        ]]\n",
      "Validation accuracy: 63.5\n",
      "Minibatch total loss at step 8411: 0.155695\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.06650355  0.93349642  0.          1.        ]\n",
      " [ 0.04626109  0.95373893  0.          1.        ]]\n",
      "Minibatch total loss at step 8424: 0.179868\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.21438366  0.78561628  0.          1.        ]\n",
      " [ 0.79798174  0.20201828  1.          0.        ]]\n",
      "Minibatch total loss at step 8437: 0.191539\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.05959385  0.94040614  0.          1.        ]\n",
      " [ 0.97135633  0.02864362  1.          0.        ]]\n",
      "Minibatch total loss at step 8450: 0.233188\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.08317972  0.91682023  0.          1.        ]\n",
      " [ 0.97655815  0.02344186  1.          0.        ]]\n",
      "Minibatch total loss at step 8463: 0.184860\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.14121513  0.85878485  0.          1.        ]\n",
      " [ 0.27965268  0.72034729  0.          1.        ]]\n",
      "Minibatch total loss at step 8476: 0.151273\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.18330733  0.81669265  0.          1.        ]\n",
      " [ 0.22476563  0.77523434  0.          1.        ]]\n",
      "Minibatch total loss at step 8489: 0.237665\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.03405562  0.96594441  0.          1.        ]\n",
      " [ 0.59593183  0.40406823  1.          0.        ]]\n",
      "Validation accuracy: 71.5\n",
      "Minibatch total loss at step 8502: 0.201272\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.77482176  0.22517821  1.          0.        ]\n",
      " [ 0.20245931  0.79754072  0.          1.        ]]\n",
      "Minibatch total loss at step 8515: 0.169898\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.66164553  0.33835447  1.          0.        ]\n",
      " [ 0.21161394  0.78838611  0.          1.        ]]\n",
      "Minibatch total loss at step 8528: 0.207170\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.8661955   0.13380453  1.          0.        ]\n",
      " [ 0.17848764  0.82151234  0.          1.        ]]\n",
      "Minibatch total loss at step 8541: 0.165527\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.28055531  0.71944469  0.          1.        ]\n",
      " [ 0.92940873  0.07059125  1.          0.        ]]\n",
      "Minibatch total loss at step 8554: 0.165511\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.88641566  0.11358429  1.          0.        ]\n",
      " [ 0.82629555  0.1737044   1.          0.        ]]\n",
      "Minibatch total loss at step 8567: 0.169203\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.14448652  0.85551345  0.          1.        ]\n",
      " [ 0.88016582  0.11983412  1.          0.        ]]\n",
      "Minibatch total loss at step 8580: 0.146453\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.87088788  0.12911215  1.          0.        ]\n",
      " [ 0.16386607  0.8361339   0.          1.        ]]\n",
      "Minibatch total loss at step 8593: 0.180446\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.04574614  0.95425385  0.          1.        ]\n",
      " [ 0.74734962  0.25265035  1.          0.        ]]\n",
      "Validation accuracy: 68.0\n",
      "Minibatch total loss at step 8606: 0.171519\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.88827342  0.11172657  1.          0.        ]\n",
      " [ 0.08176649  0.91823357  0.          1.        ]]\n",
      "Minibatch total loss at step 8619: 0.318562\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.06203308  0.93796694  0.          1.        ]\n",
      " [ 0.01566643  0.98433357  0.          1.        ]]\n",
      "Minibatch total loss at step 8632: 0.149783\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.83567238  0.16432765  1.          0.        ]\n",
      " [ 0.21169242  0.78830761  0.          1.        ]]\n",
      "Minibatch total loss at step 8645: 0.182448\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.70191211  0.29808787  1.          0.        ]\n",
      " [ 0.13201296  0.86798704  0.          1.        ]]\n",
      "Minibatch total loss at step 8658: 0.174604\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.87492812  0.12507185  1.          0.        ]\n",
      " [ 0.07712279  0.92287725  0.          1.        ]]\n",
      "Minibatch total loss at step 8671: 0.169614\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.95678896  0.04321101  1.          0.        ]\n",
      " [ 0.80385214  0.19614781  1.          0.        ]]\n",
      "Minibatch total loss at step 8684: 0.151225\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.13621604  0.86378402  0.          1.        ]\n",
      " [ 0.76106262  0.23893741  1.          0.        ]]\n",
      "Minibatch total loss at step 8697: 0.190188\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.81939495  0.1806051   1.          0.        ]\n",
      " [ 0.15671335  0.84328663  0.          1.        ]]\n",
      "Validation accuracy: 70.0\n",
      "Minibatch total loss at step 8710: 0.152742\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.81287467  0.1871253   1.          0.        ]\n",
      " [ 0.17601559  0.82398444  0.          1.        ]]\n",
      "Minibatch total loss at step 8723: 0.151475\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.07016759  0.92983246  0.          1.        ]\n",
      " [ 0.11141391  0.8885861   0.          1.        ]]\n",
      "Minibatch total loss at step 8736: 0.171299\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.28430423  0.71569574  0.          1.        ]\n",
      " [ 0.25756863  0.74243134  0.          1.        ]]\n",
      "Minibatch total loss at step 8749: 0.259729\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.81255144  0.18744853  1.          0.        ]\n",
      " [ 0.90015978  0.09984025  1.          0.        ]]\n",
      "Minibatch total loss at step 8762: 0.228574\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.1262387   0.8737613   0.          1.        ]\n",
      " [ 0.7180773   0.28192264  1.          0.        ]]\n",
      "Minibatch total loss at step 8775: 0.302733\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.87516099  0.12483899  1.          0.        ]\n",
      " [ 0.01513289  0.98486716  0.          1.        ]]\n",
      "Minibatch total loss at step 8788: 0.266297\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.837497    0.16250297  1.          0.        ]\n",
      " [ 0.02552839  0.97447157  0.          1.        ]]\n",
      "Validation accuracy: 65.5\n",
      "Minibatch total loss at step 8801: 0.179221\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.06401373  0.93598622  0.          1.        ]\n",
      " [ 0.8434332   0.15656677  1.          0.        ]]\n",
      "Minibatch total loss at step 8814: 0.209263\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.39035755  0.60964245  0.          1.        ]\n",
      " [ 0.04181762  0.95818239  0.          1.        ]]\n",
      "Minibatch total loss at step 8827: 0.150887\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.0434939   0.95650607  0.          1.        ]\n",
      " [ 0.12296174  0.8770383   0.          1.        ]]\n",
      "Minibatch total loss at step 8840: 0.186289\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.76666558  0.23333441  1.          0.        ]\n",
      " [ 0.15440576  0.84559429  0.          1.        ]]\n",
      "Minibatch total loss at step 8853: 0.178873\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.32482535  0.67517471  0.          1.        ]\n",
      " [ 0.19070868  0.8092913   0.          1.        ]]\n",
      "Minibatch total loss at step 8866: 0.201110\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.04806528  0.95193475  0.          1.        ]\n",
      " [ 0.62349039  0.37650961  1.          0.        ]]\n",
      "Minibatch total loss at step 8879: 0.168399\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.75307614  0.24692383  1.          0.        ]\n",
      " [ 0.94206518  0.05793485  1.          0.        ]]\n",
      "Minibatch total loss at step 8892: 0.200811\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.21593782  0.78406221  0.          1.        ]\n",
      " [ 0.96814764  0.03185242  1.          0.        ]]\n",
      "Validation accuracy: 67.0\n",
      "Minibatch total loss at step 8905: 0.168755\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.81802738  0.18197264  1.          0.        ]\n",
      " [ 0.75126159  0.24873841  1.          0.        ]]\n",
      "Minibatch total loss at step 8918: 0.217764\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.2677274   0.73227257  0.          1.        ]\n",
      " [ 0.07031734  0.92968273  0.          1.        ]]\n",
      "Minibatch total loss at step 8931: 0.191062\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.04090692  0.95909309  0.          1.        ]\n",
      " [ 0.08217977  0.91782022  0.          1.        ]]\n",
      "Minibatch total loss at step 8944: 0.179905\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.82894564  0.17105439  1.          0.        ]\n",
      " [ 0.20407213  0.79592782  0.          1.        ]]\n",
      "Minibatch total loss at step 8957: 0.235009\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.88751262  0.11248737  1.          0.        ]\n",
      " [ 0.71875399  0.28124598  1.          0.        ]]\n",
      "Minibatch total loss at step 8970: 0.177670\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.14370136  0.85629863  0.          1.        ]\n",
      " [ 0.15440136  0.84559864  0.          1.        ]]\n",
      "Minibatch total loss at step 8983: 0.320779\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.87283242  0.12716761  1.          0.        ]\n",
      " [ 0.78649378  0.21350627  1.          0.        ]]\n",
      "Minibatch total loss at step 8996: 0.194245\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.22644992  0.77355003  0.          1.        ]\n",
      " [ 0.11276666  0.88723332  0.          1.        ]]\n",
      "Validation accuracy: 68.0\n",
      "Minibatch total loss at step 9009: 0.165275\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.92267424  0.07732578  1.          0.        ]\n",
      " [ 0.0596035   0.94039655  0.          1.        ]]\n",
      "Minibatch total loss at step 9022: 0.210264\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.84456539  0.15543459  1.          0.        ]\n",
      " [ 0.89634854  0.10365143  1.          0.        ]]\n",
      "Minibatch total loss at step 9035: 0.176288\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.87869138  0.12130868  1.          0.        ]\n",
      " [ 0.17888334  0.82111663  0.          1.        ]]\n",
      "Minibatch total loss at step 9048: 0.189565\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.78858042  0.21141958  1.          0.        ]\n",
      " [ 0.83108491  0.16891505  1.          0.        ]]\n",
      "Minibatch total loss at step 9061: 0.164348\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.05457522  0.94542485  0.          1.        ]\n",
      " [ 0.93380994  0.06619006  1.          0.        ]]\n",
      "Minibatch total loss at step 9074: 0.200735\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.72645271  0.27354729  1.          0.        ]\n",
      " [ 0.18482533  0.8151747   0.          1.        ]]\n",
      "Minibatch total loss at step 9087: 0.308547\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.50563329  0.49436668  1.          0.        ]\n",
      " [ 0.55494416  0.44505581  1.          0.        ]]\n",
      "Minibatch total loss at step 9100: 0.188106\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.1425959   0.85740405  0.          1.        ]\n",
      " [ 0.84124643  0.15875357  1.          0.        ]]\n",
      "Validation accuracy: 67.0\n",
      "Minibatch total loss at step 9113: 0.175561\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.10861398  0.89138609  0.          1.        ]\n",
      " [ 0.0559628   0.9440372   0.          1.        ]]\n",
      "Minibatch total loss at step 9126: 0.164946\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.09076874  0.90923131  0.          1.        ]\n",
      " [ 0.7345112   0.2654888   1.          0.        ]]\n",
      "Minibatch total loss at step 9139: 0.175134\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.81020957  0.18979035  1.          0.        ]\n",
      " [ 0.04200087  0.95799911  0.          1.        ]]\n",
      "Minibatch total loss at step 9152: 0.183677\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.55846786  0.44153211  1.          0.        ]\n",
      " [ 0.09880908  0.90119094  0.          1.        ]]\n",
      "Minibatch total loss at step 9165: 0.170744\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.92619407  0.07380591  1.          0.        ]\n",
      " [ 0.37882081  0.6211791   0.          1.        ]]\n",
      "Minibatch total loss at step 9178: 0.154340\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.16865166  0.8313483   0.          1.        ]\n",
      " [ 0.90695804  0.09304195  1.          0.        ]]\n",
      "Minibatch total loss at step 9191: 0.185510\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.94240445  0.05759549  1.          0.        ]\n",
      " [ 0.45240742  0.54759258  0.          1.        ]]\n",
      "Validation accuracy: 66.0\n",
      "Minibatch total loss at step 9204: 0.185906\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.12803258  0.87196738  0.          1.        ]\n",
      " [ 0.25041997  0.74957997  0.          1.        ]]\n",
      "Minibatch total loss at step 9217: 0.193254\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.66206682  0.33793321  1.          0.        ]\n",
      " [ 0.72154564  0.27845433  1.          0.        ]]\n",
      "Minibatch total loss at step 9230: 0.161701\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.13759981  0.86240017  0.          1.        ]\n",
      " [ 0.15933263  0.84066737  0.          1.        ]]\n",
      "Minibatch total loss at step 9243: 0.162788\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.82399178  0.17600818  1.          0.        ]\n",
      " [ 0.74785912  0.25214088  1.          0.        ]]\n",
      "Minibatch total loss at step 9256: 0.196335\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.2844016   0.7155984   0.          1.        ]\n",
      " [ 0.19302352  0.8069765   0.          1.        ]]\n",
      "Minibatch total loss at step 9269: 0.203754\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.90098655  0.09901348  1.          0.        ]\n",
      " [ 0.77020639  0.22979361  1.          0.        ]]\n",
      "Minibatch total loss at step 9282: 0.184154\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.89785558  0.10214444  1.          0.        ]\n",
      " [ 0.23449731  0.76550269  0.          1.        ]]\n",
      "Minibatch total loss at step 9295: 0.139540\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.10422923  0.89577085  0.          1.        ]\n",
      " [ 0.13877097  0.86122906  0.          1.        ]]\n",
      "Validation accuracy: 65.0\n",
      "Minibatch total loss at step 9308: 0.220717\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.79882669  0.20117329  1.          0.        ]\n",
      " [ 0.54580623  0.45419377  1.          0.        ]]\n",
      "Minibatch total loss at step 9321: 0.201108\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.10324986  0.89675015  0.          1.        ]\n",
      " [ 0.84696525  0.15303479  1.          0.        ]]\n",
      "Minibatch total loss at step 9334: 0.161348\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.10576338  0.89423656  0.          1.        ]\n",
      " [ 0.32068068  0.67931932  0.          1.        ]]\n",
      "Minibatch total loss at step 9347: 0.159877\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.22879377  0.77120626  0.          1.        ]\n",
      " [ 0.30613676  0.69386321  0.          1.        ]]\n",
      "Minibatch total loss at step 9360: 0.181411\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.09966777  0.90033227  0.          1.        ]\n",
      " [ 0.24844326  0.75155675  0.          1.        ]]\n",
      "Minibatch total loss at step 9373: 0.248461\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.98442984  0.01557015  1.          0.        ]\n",
      " [ 0.14965916  0.85034084  0.          1.        ]]\n",
      "Minibatch total loss at step 9386: 0.200839\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.77441722  0.22558279  1.          0.        ]\n",
      " [ 0.7034871   0.29651296  1.          0.        ]]\n",
      "Minibatch total loss at step 9399: 0.208526\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.17251763  0.82748234  0.          1.        ]\n",
      " [ 0.09227682  0.90772325  0.          1.        ]]\n",
      "Validation accuracy: 66.0\n",
      "Minibatch total loss at step 9412: 0.233290\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.06504635  0.93495363  0.          1.        ]\n",
      " [ 0.62071782  0.37928221  1.          0.        ]]\n",
      "Minibatch total loss at step 9425: 0.198947\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.13315655  0.86684346  0.          1.        ]\n",
      " [ 0.18452881  0.81547111  0.          1.        ]]\n",
      "Minibatch total loss at step 9438: 0.209270\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.10651568  0.89348429  0.          1.        ]\n",
      " [ 0.78268886  0.21731116  1.          0.        ]]\n",
      "Minibatch total loss at step 9451: 0.203042\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.97637182  0.02362819  1.          0.        ]\n",
      " [ 0.18125385  0.81874615  0.          1.        ]]\n",
      "Minibatch total loss at step 9464: 0.291725\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.54886299  0.45113704  0.          1.        ]\n",
      " [ 0.25036246  0.74963754  0.          1.        ]]\n",
      "Minibatch total loss at step 9477: 0.268083\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.09172808  0.90827197  0.          1.        ]\n",
      " [ 0.67334414  0.32665589  1.          0.        ]]\n",
      "Minibatch total loss at step 9490: 0.151387\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.16830042  0.83169955  0.          1.        ]\n",
      " [ 0.23520795  0.76479208  0.          1.        ]]\n",
      "Validation accuracy: 64.5\n",
      "Minibatch total loss at step 9503: 0.173450\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.17167871  0.82832122  0.          1.        ]\n",
      " [ 0.851695    0.14830497  1.          0.        ]]\n",
      "Minibatch total loss at step 9516: 0.265616\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.76642889  0.23357107  1.          0.        ]\n",
      " [ 0.0316924   0.96830755  0.          1.        ]]\n",
      "Minibatch total loss at step 9529: 0.235203\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.76995414  0.23004584  1.          0.        ]\n",
      " [ 0.85945165  0.14054833  1.          0.        ]]\n",
      "Minibatch total loss at step 9542: 0.201864\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.07889099  0.92110902  0.          1.        ]\n",
      " [ 0.09903388  0.90096611  0.          1.        ]]\n",
      "Minibatch total loss at step 9555: 0.163700\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.73018497  0.26981506  1.          0.        ]\n",
      " [ 0.83709645  0.16290353  1.          0.        ]]\n",
      "Minibatch total loss at step 9568: 0.232449\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.18960081  0.81039917  0.          1.        ]\n",
      " [ 0.12760022  0.87239975  0.          1.        ]]\n",
      "Minibatch total loss at step 9581: 0.221895\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.96751183  0.03248814  1.          0.        ]\n",
      " [ 0.06878648  0.93121356  0.          1.        ]]\n",
      "Minibatch total loss at step 9594: 0.175209\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.15616153  0.84383845  0.          1.        ]\n",
      " [ 0.30268145  0.69731855  0.          1.        ]]\n",
      "Validation accuracy: 68.5\n",
      "Minibatch total loss at step 9607: 0.242725\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.53769666  0.46230328  0.          1.        ]\n",
      " [ 0.26119781  0.73880213  0.          1.        ]]\n",
      "Minibatch total loss at step 9620: 0.187115\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.2208878   0.77911222  0.          1.        ]\n",
      " [ 0.05599041  0.9440096   0.          1.        ]]\n",
      "Minibatch total loss at step 9633: 0.159137\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.86323935  0.13676065  1.          0.        ]\n",
      " [ 0.87144101  0.12855901  1.          0.        ]]\n",
      "Minibatch total loss at step 9646: 0.192452\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.05166446  0.94833553  0.          1.        ]\n",
      " [ 0.06029759  0.93970245  0.          1.        ]]\n",
      "Minibatch total loss at step 9659: 0.170249\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.2281266   0.77187347  0.          1.        ]\n",
      " [ 0.89826965  0.10173037  1.          0.        ]]\n",
      "Minibatch total loss at step 9672: 0.188617\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.20935045  0.79064953  0.          1.        ]\n",
      " [ 0.89275187  0.10724819  1.          0.        ]]\n",
      "Minibatch total loss at step 9685: 0.171571\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.43614796  0.56385207  0.          1.        ]\n",
      " [ 0.93187654  0.0681235   1.          0.        ]]\n",
      "Minibatch total loss at step 9698: 0.172973\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.88829136  0.11170866  1.          0.        ]\n",
      " [ 0.89294618  0.10705385  1.          0.        ]]\n",
      "Validation accuracy: 66.5\n",
      "Minibatch total loss at step 9711: 0.157890\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.78498656  0.2150135   1.          0.        ]\n",
      " [ 0.83810323  0.1618968   1.          0.        ]]\n",
      "Minibatch total loss at step 9724: 0.179236\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.15218771  0.84781224  0.          1.        ]\n",
      " [ 0.85263562  0.14736436  1.          0.        ]]\n",
      "Minibatch total loss at step 9737: 0.207246\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.08733607  0.91266394  0.          1.        ]\n",
      " [ 0.07142302  0.92857695  0.          1.        ]]\n",
      "Minibatch total loss at step 9750: 0.199201\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.93423146  0.06576861  1.          0.        ]\n",
      " [ 0.89694375  0.10305627  1.          0.        ]]\n",
      "Minibatch total loss at step 9763: 0.222302\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.60190797  0.39809209  1.          0.        ]\n",
      " [ 0.87103856  0.12896149  1.          0.        ]]\n",
      "Minibatch total loss at step 9776: 0.183353\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.2446439   0.75535613  0.          1.        ]\n",
      " [ 0.03397213  0.96602786  0.          1.        ]]\n",
      "Minibatch total loss at step 9789: 0.181288\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.14315292  0.85684705  0.          1.        ]\n",
      " [ 0.22446869  0.77553129  0.          1.        ]]\n",
      "Validation accuracy: 68.5\n",
      "Minibatch total loss at step 9802: 0.181647\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.14734834  0.85265166  0.          1.        ]\n",
      " [ 0.79139948  0.20860052  1.          0.        ]]\n",
      "Minibatch total loss at step 9815: 0.261365\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.27887055  0.72112948  0.          1.        ]\n",
      " [ 0.08517708  0.91482294  0.          1.        ]]\n",
      "Minibatch total loss at step 9828: 0.154395\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.07438364  0.92561644  0.          1.        ]\n",
      " [ 0.13011865  0.86988127  0.          1.        ]]\n",
      "Minibatch total loss at step 9841: 0.184963\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.74346966  0.25653034  1.          0.        ]\n",
      " [ 0.1150864   0.88491356  0.          1.        ]]\n",
      "Minibatch total loss at step 9854: 0.208050\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.10380371  0.89619631  0.          1.        ]\n",
      " [ 0.11642189  0.88357818  0.          1.        ]]\n",
      "Minibatch total loss at step 9867: 0.172763\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.04931908  0.95068091  0.          1.        ]\n",
      " [ 0.10005112  0.89994889  0.          1.        ]]\n",
      "Minibatch total loss at step 9880: 0.148054\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.10294769  0.89705229  0.          1.        ]\n",
      " [ 0.20426999  0.79572999  0.          1.        ]]\n",
      "Minibatch total loss at step 9893: 0.174447\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.69406688  0.30593315  1.          0.        ]\n",
      " [ 0.78235245  0.21764763  1.          0.        ]]\n",
      "Validation accuracy: 64.0\n",
      "Minibatch total loss at step 9906: 0.213726\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.2123533   0.78764677  0.          1.        ]\n",
      " [ 0.3527537   0.64724624  0.          1.        ]]\n",
      "Minibatch total loss at step 9919: 0.189352\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.02661811  0.97338188  0.          1.        ]\n",
      " [ 0.90285063  0.09714939  1.          0.        ]]\n",
      "Minibatch total loss at step 9932: 0.167929\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.91983467  0.08016531  1.          0.        ]\n",
      " [ 0.15769614  0.84230387  0.          1.        ]]\n",
      "Minibatch total loss at step 9945: 0.156227\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.85955608  0.14044392  1.          0.        ]\n",
      " [ 0.89552069  0.10447934  1.          0.        ]]\n",
      "Minibatch total loss at step 9958: 0.222018\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.25546217  0.74453783  0.          1.        ]\n",
      " [ 0.95248878  0.04751114  1.          0.        ]]\n",
      "Minibatch total loss at step 9971: 0.180803\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.10874293  0.89125711  0.          1.        ]\n",
      " [ 0.87574369  0.12425633  1.          0.        ]]\n",
      "Minibatch total loss at step 9984: 0.171781\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.3255209   0.67447913  0.          1.        ]\n",
      " [ 0.95511967  0.04488031  1.          0.        ]]\n",
      "Minibatch total loss at step 9997: 0.149259\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.11779454  0.88220549  0.          1.        ]\n",
      " [ 0.24189582  0.75810421  0.          1.        ]]\n",
      "Validation accuracy: 66.5\n",
      "Model saved in file: ./checkpoints/model.ckpt\n",
      "Finished training\n"
     ]
    }
   ],
   "source": [
    "num_steps = 10001\n",
    "\n",
    "trace_file = open('./tracing/timeline.json', 'w')\n",
    "save_path = './checkpoints/model.ckpt'\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    tf.initialize_local_variables().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_dataset.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels}\n",
    "        _, tloss, trpred, traccu, summary = session.run(\n",
    "            [optimizer, total_loss, train_predictions, train_accuracy, merged], \n",
    "            feed_dict=feed_dict)\n",
    "        results_writer.add_summary(summary, step)\n",
    "        if (step % 13 == 0):\n",
    "            print('Minibatch total loss at step %d: %f' % (step, tloss))\n",
    "            print('Minibatch accuracy:', traccu)\n",
    "            print('Predictions | Labels:\\n', np.concatenate((trpred[:2], batch_labels[:2]), axis=1))\n",
    "        if (step % 100 == 0):\n",
    "            print('Validation accuracy:', valid_accuracy.eval())\n",
    "            \n",
    "    # Save tracing into disl\n",
    "    #trace = timeline.Timeline(step_stats=run_metadata.step_stats)\n",
    "    #trace_file.write(trace.generate_chrome_trace_format(show_memory=True))\n",
    "            \n",
    "    # Save the variables to disk.\n",
    "    saver.save(session, save_path)\n",
    "    print(\"Model saved in file: %s\" % save_path)\n",
    "            \n",
    "    results_writer.flush()\n",
    "    results_writer.close()\n",
    "\n",
    "    print('Finished training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "valid_batch_size = 1\n",
    "\n",
    "def accuracy_notpercent(predictions, labels):\n",
    "  return np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    saver.restore(session, save_path)\n",
    "    print('Model Loaded')\n",
    "    data_split = np.array_split(valid_dataset, valid_dataset.shape[0]//valid_batch_size, axis=0)\n",
    "    labels_split = np.array_split(valid_labels, valid_labels.shape[0]//valid_batch_size, axis=0)\n",
    "    correct_predictions = 0\n",
    "    for idx, batch_data in enumerate(data_split):\n",
    "        correct_predictions += accuracy_notpercent(\n",
    "            train_prediction.eval(feed_dict={tf_train_dataset: batch_data}), \n",
    "            labels_split[idx])\n",
    "        print('accuracy:', (100.0*correct_predictions)/((idx+1)*valid_batch_size))\n",
    "        \n",
    "        \n",
    "    print('Finished validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
