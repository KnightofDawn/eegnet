{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import done\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from six.moves import cPickle as pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import timeline\n",
    "\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"import done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load pickled dataset into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unpickling ./data/trainsh1.pickle\n",
      "dataset shape: (30, 240000, 16)\n",
      "labels shape: (30,)\n"
     ]
    }
   ],
   "source": [
    "name_pickle = './data/trainsh1.pickle'\n",
    "\n",
    "with open(name_pickle, 'rb') as f:\n",
    "    print('Unpickling ' + name_pickle)\n",
    "    load = pickle.load(f)\n",
    "    dataset = load['data']\n",
    "    labels = load['labels']\n",
    "    del load\n",
    "    print('dataset shape:', dataset.shape)\n",
    "    print('labels shape:', labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reformat data for training\n",
    "- Divide each file with 240000 samples into smaller batch_samples ~= size of receptive field of eegnet\n",
    "- Keep valid_dataset nr of samples intact for proper validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_array(array):\n",
    "    # Normalize mean=0 and sigma=0.25: axis=0 is along columns, vertical lines.\n",
    "    array -= np.mean(array, axis=0) \n",
    "    array /= 2*np.ptp(array, axis=0)\n",
    "    return array\n",
    "    \n",
    "def clean_normalize_data_labels(data, labels, sigma=0.5):\n",
    "    data_tmp = list()\n",
    "    labels_tmp = list()\n",
    "    for idx, d in enumerate(data):\n",
    "        if (np.count_nonzero(d) < 10) or (np.any(np.std(d, axis=0) < sigma)):\n",
    "            continue\n",
    "        d = normalize_array(d)\n",
    "        data_tmp.append(d)\n",
    "        labels_tmp.append(labels[idx])\n",
    "    return np.asarray(data_tmp), np.asarray(labels_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset shape: (1920, 1, 2400, 16) train_labels shape: (1920, 2) mix: 0.5828125\n",
      "valid_dataset shape: (200, 1, 2400, 16) valid_labels shape: (200, 2) mix: 0.215\n"
     ]
    }
   ],
   "source": [
    "#Output size of the layer\n",
    "num_labels = 2\n",
    "\n",
    "#60% for train and 40% for validation\n",
    "split_idx = int(dataset.shape[0]*0.8)\n",
    "#nr of splits\n",
    "nrOfSplits = 100\n",
    "\n",
    "def format_data(data, labels, nr_splits):\n",
    "    shape = data.shape\n",
    "    # reshape [batch, samples, channels] into [batch * samples, channels]\n",
    "    data = np.reshape(data, (shape[0]*shape[1], shape[2]))\n",
    "    # Split 2D array into the desired smaller chuncks\n",
    "    data = np.asarray(np.split(data, shape[0]*nr_splits, axis=0))\n",
    "    # labels are obtained by repeating original labels nr_splits times\n",
    "    labels = np.repeat((np.arange(num_labels) == labels[:,None]).astype(np.float32), nr_splits, axis=0)\n",
    "    # normalize and eliminate batches that only contain drop-outs\n",
    "    data, labels = clean_normalize_data_labels(data, labels, 0.01)\n",
    "    # data has to be 4D for tensorflow (insert an empty dimension)\n",
    "    data = data[:,None,:,:]\n",
    "    # shuffle data and labels mantaining relation between them. Important after the small batches.\n",
    "    shuffle_idx = np.random.permutation(data.shape[0])\n",
    "    data = data[shuffle_idx,:,:,:]\n",
    "    labels = labels[shuffle_idx]\n",
    "    return data, labels\n",
    "\n",
    "# shuffle file data\n",
    "shuffle_idx = np.random.permutation(dataset.shape[0])\n",
    "dataset = dataset[shuffle_idx,:,:]\n",
    "labels = labels[shuffle_idx]\n",
    "# format and split data into smaller chunks\n",
    "train_dataset, train_labels = format_data(dataset[:split_idx], labels[:split_idx], nrOfSplits)\n",
    "valid_dataset, valid_labels = format_data(dataset[split_idx:-1], labels[split_idx:-1], nrOfSplits)\n",
    "del dataset, labels\n",
    "\n",
    "valid_dataset = valid_dataset[:200]\n",
    "valid_labels = valid_labels[:200]\n",
    "\n",
    "print('train_dataset shape:', train_dataset.shape, 'train_labels shape:', train_labels.shape, \n",
    "      'mix:', float(np.count_nonzero(train_labels[:,1], axis=0))/train_labels.shape[0])\n",
    "print('valid_dataset shape:', valid_dataset.shape, 'valid_labels shape:', valid_labels.shape, \n",
    "      'mix:', float(np.count_nonzero(valid_labels[:,1], axis=0))/valid_labels.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot some data to have an idea of how data looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f7968952ad0>]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh0AAAC7CAYAAADBuiw9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n+S9CytvWygAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f793c3e5b90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplot(211)\n",
    "plt.plot(train_dataset[1,0,:,0])\n",
    "plt.plot(train_dataset[3,0,:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EEGNET implementation\n",
    "\n",
    "Part of https://arxiv.org/pdf/1609.03499.pdf that most concerns classification:\n",
    "\"As a last experiment we looked at speech recognition with WaveNets on the TIMIT (Garofolo et al., 1993) dataset. For this task we added a mean-pooling layer after the dilation convolutions that aggregated the activations to coarser frames spanning 10 milliseconds (160 x downsampling). The pooling layer was followed by a few non-causal convolutions. We trained WaveNet with two loss terms, one to predict the next sample and one to classify the frame, the model generalized better than with a single loss and achieved 18.8 PER on the test set, which is to our knowledge the best score obtained from a model trained directly on raw audio on TIMIT.\"\n",
    "\n",
    "Look into: http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43290.pdf\n",
    "\"Input: This layer extracts 275 ms waveform segments from each of M input microphones. Successive inputs are hopped by 10ms. At the 16kHz sampling rate used in our experiments each segment contains M X 4401 dimensions.\"\n",
    "...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computational graph created\n"
     ]
    }
   ],
   "source": [
    "#How many files are supplied per batch.\n",
    "batch_size=16\n",
    "#Number of samples in each batch entry\n",
    "batch_samples=train_dataset.shape[2]\n",
    "#How many filters to learn for the input.\n",
    "input_channels=16\n",
    "#How many filters to learn for the residual.\n",
    "residual_channels=2*input_channels\n",
    "# size after pooling layer\n",
    "pool_size = 2400\n",
    "#number of steps after which learning rate is decayed\n",
    "decay_steps=500\n",
    "\n",
    "filter_width=3\n",
    "\n",
    "slim = tf.contrib.slim\n",
    "\n",
    "#Construct computation graph\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    \n",
    "    # Input data\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, 1, batch_samples, input_channels))\n",
    "    tf_train_labels = tf.placeholder(tf.int32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset, dtype=tf.float32)\n",
    "    tf_valid_labels = tf.constant(valid_labels, dtype=tf.int32)\n",
    "    \n",
    "    def network(batch_data, reuse=False, is_training=True):\n",
    "        with tf.variable_scope('eegnet_network', reuse=reuse):\n",
    "            with slim.arg_scope([slim.batch_norm], \n",
    "                                is_training=is_training):\n",
    "                with slim.arg_scope([slim.conv2d, slim.fully_connected], \n",
    "                                    weights_initializer=slim.xavier_initializer(), \n",
    "                                    normalizer_fn=slim.batch_norm):\n",
    "                    with tf.variable_scope('input_layer'):\n",
    "                        hidden = slim.conv2d(batch_data, residual_channels, [1, filter_width], stride=1, rate=1, \n",
    "                                             activation_fn=None, scope='conv1')\n",
    "\n",
    "                    with tf.variable_scope('hidden'):\n",
    "                        with tf.variable_scope('layer1'):\n",
    "                            layer_input = hidden\n",
    "                            hidden = slim.conv2d(hidden, 2*residual_channels, [1, filter_width], stride=1, rate=2, \n",
    "                                                 activation_fn=None, scope='dilconv')\n",
    "                            filtr, gate = tf.split(3, 2, hidden) # split features in half\n",
    "                            hidden = tf.mul(tf.tanh(filtr), tf.sigmoid(gate), name='filterXgate')\n",
    "                            hidden = slim.conv2d(hidden, residual_channels, 1, activation_fn=None, scope='1x1skip')\n",
    "                            skip = hidden # skip conn\n",
    "                            hidden = tf.add(hidden, layer_input) # residual conn\n",
    "                        with tf.variable_scope('layer2'):\n",
    "                            layer_input = hidden\n",
    "                            hidden = slim.conv2d(hidden, 2*residual_channels, [1, filter_width], stride=1, rate=4, \n",
    "                                                 activation_fn=None, scope='dilconv')\n",
    "                            filtr, gate = tf.split(3, 2, hidden) # split features in half\n",
    "                            hidden = tf.mul(tf.tanh(filtr), tf.sigmoid(gate), name='filterXgate')\n",
    "                            hidden = slim.conv2d(hidden, residual_channels, 1, activation_fn=None, scope='1x1skip')\n",
    "                            skip = tf.add(skip, hidden) # skip conn\n",
    "                            hidden = tf.add(hidden, layer_input) # residual conn\n",
    "                        with tf.variable_scope('layer3'):\n",
    "                            hidden = slim.conv2d(hidden, 2*residual_channels, [1, filter_width], stride=1, rate=8, \n",
    "                                                 activation_fn=None, scope='dilconv')\n",
    "                            filtr, gate = tf.split(3, 2, hidden) # split features in half\n",
    "                            hidden = tf.mul(tf.tanh(filtr), tf.sigmoid(gate), name='filterXgate')\n",
    "                            hidden = slim.conv2d(hidden, residual_channels, 1, activation_fn=None, scope='1x1skip')\n",
    "                            skip = tf.add(skip, hidden) # skip conn\n",
    "                        \n",
    "                    with tf.variable_scope('skip_processing'):\n",
    "                        hidden = tf.nn.relu(skip)\n",
    "                        hidden = slim.avg_pool2d(hidden, [1, batch_samples*2//pool_size], [1, batch_samples//pool_size])\n",
    "                        # 1 x 2400 x residual_channels\n",
    "                        hidden = slim.conv2d(hidden, 32, 1, activation_fn=tf.nn.relu, scope='1x1compress1')\n",
    "                        hidden = slim.conv2d(hidden, 16, [1, 8], stride=4, activation_fn=tf.nn.relu, scope='1x5reduce1')\n",
    "                        # 1 x 600 x 16\n",
    "                        hidden = slim.conv2d(hidden, 8, 1, activation_fn=tf.nn.relu, scope='1x1compress2')\n",
    "                        hidden = slim.conv2d(hidden, 4, [1, 8], stride=4, activation_fn=tf.nn.relu, scope='1x5reduce2')\n",
    "                        # 1 x 150 x 4\n",
    "                        hidden = slim.conv2d(hidden, 2, 1, activation_fn=tf.nn.relu, scope='1x1compress3')\n",
    "                        hidden = slim.conv2d(hidden, 2, [1, 6], stride=3, activation_fn=tf.nn.relu, scope='1x5reduce3')\n",
    "                        # 1 x 75 x 2\n",
    "\n",
    "                    with tf.variable_scope('logits'):\n",
    "                        hidden = slim.dropout(hidden, 0.7, is_training=is_training)\n",
    "                        hidden = slim.flatten(hidden)\n",
    "                        logits = slim.fully_connected(hidden, num_labels, activation_fn=None, \n",
    "                                                      normalizer_fn=None, scope='fc1')\n",
    "        return logits \n",
    "\n",
    "    with tf.name_scope('eegnet_handling'):\n",
    "        logits = network(tf_train_dataset)\n",
    "        loss = slim.losses.softmax_cross_entropy(logits, tf_train_labels, scope='loss')\n",
    "        tf.scalar_summary('loss', loss)\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=1e-3, epsilon=1e-4).minimize(loss, \n",
    "                                                                                      var_list=tf.trainable_variables())\n",
    "        train_probabilities = tf.nn.softmax(logits)\n",
    "        train_predictions = tf.one_hot(tf.argmax(train_probabilities, 1), num_labels, dtype=tf.int32)\n",
    "        train_accuracy = slim.metrics.accuracy(train_predictions, tf_train_labels, 100.0)\n",
    "        valid_probabilities = tf.nn.softmax(network(tf_valid_dataset, True, False))\n",
    "        valid_predictions = tf.one_hot(tf.argmax(valid_probabilities, 1), num_labels, dtype=tf.int32)\n",
    "        valid_accuracy = slim.metrics.accuracy(valid_predictions, tf_valid_labels, 100.0)\n",
    "\n",
    "    # Add histograms for trainable variables.\n",
    "    for var in tf.trainable_variables():\n",
    "        tf.histogram_summary(var.op.name, var)\n",
    "        \n",
    "    # Add summaries for activations: NOT WORKING YET. TF ERROR.\n",
    "    #slim.summarize_activations()\n",
    "    \n",
    "    #Merge all summaries and write to a folder\n",
    "    merged_summs = tf.merge_all_summaries()\n",
    "    results_writer = tf.train.SummaryWriter('./results', graph)\n",
    "    \n",
    "    # Add ops to save and restore all the variables.\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    #tracing for timeline\n",
    "    run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "    run_metadata = tf.RunMetadata()    \n",
    "    \n",
    "print('computational graph created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch total loss at step 0: 0.612041 | Best: 0.612041\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.37404463  0.62595534  0.          1.        ]\n",
      " [ 0.49554217  0.50445783  1.          0.        ]]\n",
      "###-> Validation accuracy: 21.5 | Best: 21.5\n",
      "Minibatch total loss at step 13: 0.664518 | Best: 0.612041\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.39868829  0.60131168  0.          1.        ]\n",
      " [ 0.7300756   0.26992446  0.          1.        ]]\n",
      "Minibatch total loss at step 26: 1.093468 | Best: 0.612041\n",
      "Minibatch accuracy: 37.5\n",
      "Predictions | Labels:\n",
      " [[ 0.41777068  0.58222938  1.          0.        ]\n",
      " [ 0.21997815  0.78002191  1.          0.        ]]\n",
      "Minibatch total loss at step 39: 0.652846 | Best: 0.612041\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.52835995  0.47164002  1.          0.        ]\n",
      " [ 0.51308364  0.4869163   0.          1.        ]]\n",
      "###-> Validation accuracy: 21.5 | Best: 21.5\n",
      "Minibatch total loss at step 52: 0.646210 | Best: 0.612041\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.14187397  0.85812604  1.          0.        ]\n",
      " [ 0.55871558  0.44128445  0.          1.        ]]\n",
      "Minibatch total loss at step 65: 0.891605 | Best: 0.612041\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.13271834  0.86728162  0.          1.        ]\n",
      " [ 0.59146577  0.40853417  1.          0.        ]]\n",
      "Minibatch total loss at step 78: 0.619555 | Best: 0.612041\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.11269768  0.88730234  0.          1.        ]\n",
      " [ 0.44664404  0.55335593  0.          1.        ]]\n",
      "Minibatch total loss at step 91: 0.648163 | Best: 0.612041\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.61290294  0.38709709  1.          0.        ]\n",
      " [ 0.44528681  0.55471319  1.          0.        ]]\n",
      "###-> Validation accuracy: 21.5 | Best: 21.5\n",
      "Minibatch total loss at step 104: 0.727662 | Best: 0.612041\n",
      "Minibatch accuracy: 43.75\n",
      "Predictions | Labels:\n",
      " [[ 0.83071333  0.1692867   1.          0.        ]\n",
      " [ 0.16690394  0.83309603  0.          1.        ]]\n",
      "Minibatch total loss at step 117: 0.852994 | Best: 0.612041\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.21651688  0.78348309  1.          0.        ]\n",
      " [ 0.08302718  0.91697288  1.          0.        ]]\n",
      "Minibatch total loss at step 130: 0.814258 | Best: 0.612041\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.64122462  0.35877538  1.          0.        ]\n",
      " [ 0.25046998  0.74953002  1.          0.        ]]\n",
      "Minibatch total loss at step 143: 0.848772 | Best: 0.612041\n",
      "Minibatch accuracy: 37.5\n",
      "Predictions | Labels:\n",
      " [[ 0.51156306  0.48843688  0.          1.        ]\n",
      " [ 0.65718281  0.34281719  0.          1.        ]]\n",
      "###-> Validation accuracy: 21.5 | Best: 21.5\n",
      "Minibatch total loss at step 156: 0.666248 | Best: 0.612041\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.2386172   0.76138282  0.          1.        ]\n",
      " [ 0.09983136  0.90016866  0.          1.        ]]\n",
      "Minibatch total loss at step 169: 0.930697 | Best: 0.612041\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.59153837  0.40846166  1.          0.        ]\n",
      " [ 0.32574162  0.67425835  0.          1.        ]]\n",
      "Minibatch total loss at step 182: 0.786829 | Best: 0.612041\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.19047175  0.80952829  0.          1.        ]\n",
      " [ 0.39712346  0.60287654  0.          1.        ]]\n",
      "Minibatch total loss at step 195: 0.537685 | Best: 0.537685\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.4475145   0.55248553  1.          0.        ]\n",
      " [ 0.28423572  0.71576428  0.          1.        ]]\n",
      "###-> Validation accuracy: 21.5 | Best: 21.5\n",
      "Minibatch total loss at step 208: 0.621273 | Best: 0.537685\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.21560472  0.78439528  1.          0.        ]\n",
      " [ 0.27820322  0.72179675  0.          1.        ]]\n",
      "Minibatch total loss at step 221: 0.713585 | Best: 0.537685\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.43343961  0.56656045  0.          1.        ]\n",
      " [ 0.5665341   0.43346596  1.          0.        ]]\n",
      "Minibatch total loss at step 234: 0.716215 | Best: 0.537685\n",
      "Minibatch accuracy: 43.75\n",
      "Predictions | Labels:\n",
      " [[ 0.36557239  0.63442767  0.          1.        ]\n",
      " [ 0.71764547  0.28235453  1.          0.        ]]\n",
      "Minibatch total loss at step 247: 0.957275 | Best: 0.537685\n",
      "Minibatch accuracy: 25.0\n",
      "Predictions | Labels:\n",
      " [[ 0.1354983   0.86450177  1.          0.        ]\n",
      " [ 0.33346322  0.66653681  1.          0.        ]]\n",
      "###-> Validation accuracy: 21.5 | Best: 21.5\n",
      "Minibatch total loss at step 260: 0.731174 | Best: 0.537685\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.48261058  0.51738948  1.          0.        ]\n",
      " [ 0.20163999  0.79835999  0.          1.        ]]\n",
      "Minibatch total loss at step 273: 0.936846 | Best: 0.537685\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.33219126  0.66780871  1.          0.        ]\n",
      " [ 0.33440629  0.66559368  0.          1.        ]]\n",
      "Minibatch total loss at step 286: 0.684203 | Best: 0.537685\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.63738394  0.36261606  1.          0.        ]\n",
      " [ 0.68668801  0.31331202  0.          1.        ]]\n",
      "Minibatch total loss at step 299: 0.654840 | Best: 0.537685\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.30740517  0.69259483  0.          1.        ]\n",
      " [ 0.28664339  0.71335655  0.          1.        ]]\n",
      "###-> Validation accuracy: 21.5 | Best: 21.5\n",
      "Minibatch total loss at step 312: 0.648481 | Best: 0.537685\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.50592923  0.49407074  1.          0.        ]\n",
      " [ 0.41117296  0.58882707  1.          0.        ]]\n",
      "Minibatch total loss at step 325: 0.515278 | Best: 0.515278\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5301187   0.46988133  0.          1.        ]\n",
      " [ 0.31256455  0.68743539  0.          1.        ]]\n",
      "Minibatch total loss at step 338: 0.641164 | Best: 0.515278\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.66662586  0.33337408  0.          1.        ]\n",
      " [ 0.32171556  0.67828441  0.          1.        ]]\n",
      "###-> Validation accuracy: 21.5 | Best: 21.5\n",
      "Minibatch total loss at step 351: 0.820814 | Best: 0.515278\n",
      "Minibatch accuracy: 43.75\n",
      "Predictions | Labels:\n",
      " [[ 0.31000832  0.68999165  1.          0.        ]\n",
      " [ 0.47283429  0.52716565  0.          1.        ]]\n",
      "Minibatch total loss at step 364: 0.699863 | Best: 0.515278\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.57824022  0.42175981  0.          1.        ]\n",
      " [ 0.66773885  0.3322612   0.          1.        ]]\n",
      "Minibatch total loss at step 377: 0.768167 | Best: 0.515278\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.31127343  0.68872654  1.          0.        ]\n",
      " [ 0.52670795  0.47329199  0.          1.        ]]\n",
      "Minibatch total loss at step 390: 0.694274 | Best: 0.515278\n",
      "Minibatch accuracy: 43.75\n",
      "Predictions | Labels:\n",
      " [[ 0.39909664  0.60090339  0.          1.        ]\n",
      " [ 0.35232881  0.64767122  1.          0.        ]]\n",
      "###-> Validation accuracy: 21.5 | Best: 21.5\n",
      "Minibatch total loss at step 403: 0.574816 | Best: 0.515278\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.68060005  0.31939995  1.          0.        ]\n",
      " [ 0.31330594  0.68669409  1.          0.        ]]\n",
      "Minibatch total loss at step 416: 0.498503 | Best: 0.498503\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.52935755  0.47064254  0.          1.        ]\n",
      " [ 0.46122113  0.53877884  0.          1.        ]]\n",
      "Minibatch total loss at step 429: 0.437466 | Best: 0.437466\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.76765239  0.23234762  1.          0.        ]\n",
      " [ 0.7437157   0.25628433  1.          0.        ]]\n",
      "Minibatch total loss at step 442: 0.665898 | Best: 0.437466\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.48972279  0.51027721  0.          1.        ]\n",
      " [ 0.19259371  0.80740625  0.          1.        ]]\n",
      "###-> Validation accuracy: 21.5 | Best: 21.5\n",
      "Minibatch total loss at step 455: 0.548092 | Best: 0.437466\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.47828007  0.52171987  1.          0.        ]\n",
      " [ 0.36300007  0.63699991  0.          1.        ]]\n",
      "Minibatch total loss at step 468: 0.688964 | Best: 0.437466\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.66162801  0.33837202  1.          0.        ]\n",
      " [ 0.39422059  0.60577941  0.          1.        ]]\n",
      "Minibatch total loss at step 481: 0.559673 | Best: 0.437466\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.46850538  0.53149462  0.          1.        ]\n",
      " [ 0.24089332  0.7591067   0.          1.        ]]\n",
      "Minibatch total loss at step 494: 0.650053 | Best: 0.437466\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.5683338   0.43166617  0.          1.        ]\n",
      " [ 0.16145033  0.83854967  0.          1.        ]]\n",
      "###-> Validation accuracy: 21.5 | Best: 21.5\n",
      "Minibatch total loss at step 507: 0.688845 | Best: 0.437466\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.05842244  0.94157761  0.          1.        ]\n",
      " [ 0.71953726  0.28046274  0.          1.        ]]\n",
      "Minibatch total loss at step 520: 0.741427 | Best: 0.437466\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.28266352  0.71733648  0.          1.        ]\n",
      " [ 0.34671319  0.65328681  0.          1.        ]]\n",
      "Minibatch total loss at step 533: 0.517318 | Best: 0.437466\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.22296003  0.77703995  0.          1.        ]\n",
      " [ 0.57904184  0.42095813  1.          0.        ]]\n",
      "Minibatch total loss at step 546: 0.570438 | Best: 0.437466\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.7644124   0.2355876   1.          0.        ]\n",
      " [ 0.29945213  0.70054787  1.          0.        ]]\n",
      "###-> Validation accuracy: 21.5 | Best: 21.5\n",
      "Minibatch total loss at step 559: 0.714330 | Best: 0.437466\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.33172426  0.66827565  0.          1.        ]\n",
      " [ 0.36535469  0.63464534  0.          1.        ]]\n",
      "Minibatch total loss at step 572: 0.624103 | Best: 0.437466\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.3422114   0.65778857  0.          1.        ]\n",
      " [ 0.18073411  0.81926584  1.          0.        ]]\n",
      "Minibatch total loss at step 585: 0.744478 | Best: 0.437466\n",
      "Minibatch accuracy: 43.75\n",
      "Predictions | Labels:\n",
      " [[ 0.46839598  0.53160399  1.          0.        ]\n",
      " [ 0.58419853  0.41580153  1.          0.        ]]\n",
      "Minibatch total loss at step 598: 0.449452 | Best: 0.437466\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.34262583  0.65737426  0.          1.        ]\n",
      " [ 0.55452693  0.4454731   1.          0.        ]]\n",
      "###-> Validation accuracy: 21.5 | Best: 21.5\n",
      "Minibatch total loss at step 611: 0.554730 | Best: 0.437466\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.43100935  0.56899059  0.          1.        ]\n",
      " [ 0.5534727   0.44652727  1.          0.        ]]\n",
      "Minibatch total loss at step 624: 0.485741 | Best: 0.437466\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.48626605  0.51373398  0.          1.        ]\n",
      " [ 0.31641626  0.68358374  0.          1.        ]]\n",
      "Minibatch total loss at step 637: 0.605042 | Best: 0.437466\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.45071191  0.54928809  1.          0.        ]\n",
      " [ 0.29257229  0.70742768  1.          0.        ]]\n",
      "Minibatch total loss at step 650: 0.709855 | Best: 0.437466\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.53313011  0.46686995  0.          1.        ]\n",
      " [ 0.5171752   0.48282474  0.          1.        ]]\n",
      "###-> Validation accuracy: 21.5 | Best: 21.5\n",
      "Minibatch total loss at step 663: 0.758590 | Best: 0.437466\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.29912275  0.70087725  1.          0.        ]\n",
      " [ 0.33752298  0.66247702  0.          1.        ]]\n",
      "Minibatch total loss at step 676: 0.621595 | Best: 0.437466\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.35338596  0.64661402  0.          1.        ]\n",
      " [ 0.4876267   0.51237333  0.          1.        ]]\n",
      "Minibatch total loss at step 689: 0.649199 | Best: 0.437466\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.55307907  0.44692093  0.          1.        ]\n",
      " [ 0.54318726  0.45681277  1.          0.        ]]\n",
      "###-> Validation accuracy: 21.5 | Best: 21.5\n",
      "Minibatch total loss at step 702: 0.579766 | Best: 0.437466\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.8599233   0.14007667  1.          0.        ]\n",
      " [ 0.39176419  0.60823578  1.          0.        ]]\n",
      "Minibatch total loss at step 715: 0.486384 | Best: 0.437466\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.39560902  0.60439098  0.          1.        ]\n",
      " [ 0.18669786  0.81330216  0.          1.        ]]\n",
      "Minibatch total loss at step 728: 0.696131 | Best: 0.437466\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.45547876  0.54452115  0.          1.        ]\n",
      " [ 0.4199093   0.58009076  0.          1.        ]]\n",
      "Minibatch total loss at step 741: 0.614437 | Best: 0.437466\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.69050765  0.30949232  1.          0.        ]\n",
      " [ 0.72797531  0.27202469  1.          0.        ]]\n",
      "###-> Validation accuracy: 21.5 | Best: 21.5\n",
      "Minibatch total loss at step 754: 0.503653 | Best: 0.437466\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.47607356  0.52392644  1.          0.        ]\n",
      " [ 0.09960645  0.90039349  0.          1.        ]]\n",
      "Minibatch total loss at step 767: 0.506957 | Best: 0.437466\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.84576273  0.15423732  1.          0.        ]\n",
      " [ 0.20589891  0.79410112  0.          1.        ]]\n",
      "Minibatch total loss at step 780: 0.472753 | Best: 0.437466\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.30072674  0.69927329  1.          0.        ]\n",
      " [ 0.17365767  0.8263424   0.          1.        ]]\n",
      "Minibatch total loss at step 793: 0.501872 | Best: 0.437466\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.85738987  0.14261012  1.          0.        ]\n",
      " [ 0.68858832  0.31141168  0.          1.        ]]\n",
      "###-> Validation accuracy: 21.5 | Best: 21.5\n",
      "Minibatch total loss at step 806: 0.447714 | Best: 0.437466\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.17923178  0.82076818  0.          1.        ]\n",
      " [ 0.55623919  0.44376084  1.          0.        ]]\n",
      "Minibatch total loss at step 819: 0.632317 | Best: 0.437466\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.82414949  0.1758505   1.          0.        ]\n",
      " [ 0.19265258  0.80734742  0.          1.        ]]\n",
      "Minibatch total loss at step 832: 0.719134 | Best: 0.437466\n",
      "Minibatch accuracy: 43.75\n",
      "Predictions | Labels:\n",
      " [[ 0.17521821  0.82478178  0.          1.        ]\n",
      " [ 0.17927626  0.82072371  0.          1.        ]]\n",
      "Minibatch total loss at step 845: 0.602425 | Best: 0.437466\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.10171884  0.89828122  0.          1.        ]\n",
      " [ 0.22012956  0.77987045  0.          1.        ]]\n",
      "###-> Validation accuracy: 21.5 | Best: 21.5\n",
      "Minibatch total loss at step 858: 0.470605 | Best: 0.437466\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.49762204  0.50237799  1.          0.        ]\n",
      " [ 0.05385837  0.9461416   0.          1.        ]]\n",
      "Minibatch total loss at step 871: 0.376768 | Best: 0.376768\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.61369377  0.3863062   1.          0.        ]\n",
      " [ 0.32605195  0.67394799  0.          1.        ]]\n",
      "Minibatch total loss at step 884: 0.326350 | Best: 0.32635\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.42678791  0.57321209  0.          1.        ]\n",
      " [ 0.096648    0.90335202  0.          1.        ]]\n",
      "Minibatch total loss at step 897: 0.481986 | Best: 0.32635\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.27426448  0.72573555  0.          1.        ]\n",
      " [ 0.51338643  0.48661357  1.          0.        ]]\n",
      "###-> Validation accuracy: 21.5 | Best: 21.5\n",
      "Minibatch total loss at step 910: 0.669100 | Best: 0.32635\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.15520351  0.84479654  0.          1.        ]\n",
      " [ 0.17726798  0.82273197  0.          1.        ]]\n",
      "Minibatch total loss at step 923: 0.612146 | Best: 0.32635\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.6606257   0.3393743   1.          0.        ]\n",
      " [ 0.70725316  0.29274681  0.          1.        ]]\n",
      "Minibatch total loss at step 936: 0.618519 | Best: 0.32635\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.51510155  0.48489848  0.          1.        ]\n",
      " [ 0.16738924  0.83261073  0.          1.        ]]\n",
      "Minibatch total loss at step 949: 0.658409 | Best: 0.32635\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.03865579  0.96134424  0.          1.        ]\n",
      " [ 0.33879697  0.66120303  0.          1.        ]]\n",
      "###-> Validation accuracy: 78.5 | Best: 78.5\n",
      "Minibatch total loss at step 962: 0.641074 | Best: 0.32635\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.54585159  0.45414847  1.          0.        ]\n",
      " [ 0.00810883  0.99189115  0.          1.        ]]\n",
      "Minibatch total loss at step 975: 0.498609 | Best: 0.32635\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.59966719  0.40033284  1.          0.        ]\n",
      " [ 0.39482361  0.60517639  1.          0.        ]]\n",
      "Minibatch total loss at step 988: 0.483143 | Best: 0.32635\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.2740469   0.72595316  0.          1.        ]\n",
      " [ 0.289269    0.71073097  1.          0.        ]]\n",
      "###-> Validation accuracy: 21.5 | Best: 78.5\n",
      "Minibatch total loss at step 1001: 0.501583 | Best: 0.32635\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.08240136  0.91759866  0.          1.        ]\n",
      " [ 0.28654486  0.71345514  0.          1.        ]]\n",
      "Minibatch total loss at step 1014: 0.351961 | Best: 0.32635\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.26745388  0.73254609  0.          1.        ]\n",
      " [ 0.50321144  0.49678856  1.          0.        ]]\n",
      "Minibatch total loss at step 1027: 0.576282 | Best: 0.32635\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.07272586  0.92727417  0.          1.        ]\n",
      " [ 0.89516371  0.10483622  0.          1.        ]]\n",
      "Minibatch total loss at step 1040: 0.607137 | Best: 0.32635\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.39281791  0.60718209  0.          1.        ]\n",
      " [ 0.15716293  0.84283704  0.          1.        ]]\n",
      "###-> Validation accuracy: 78.5 | Best: 78.5\n",
      "Minibatch total loss at step 1053: 0.521662 | Best: 0.32635\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.32435805  0.67564195  1.          0.        ]\n",
      " [ 0.20784897  0.79215097  0.          1.        ]]\n",
      "Minibatch total loss at step 1066: 0.363447 | Best: 0.32635\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.14011873  0.85988128  0.          1.        ]\n",
      " [ 0.0156616   0.98433846  0.          1.        ]]\n",
      "Minibatch total loss at step 1079: 0.644361 | Best: 0.32635\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.51624888  0.48375106  0.          1.        ]\n",
      " [ 0.84873128  0.15126874  1.          0.        ]]\n",
      "Minibatch total loss at step 1092: 0.582974 | Best: 0.32635\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.27250114  0.72749889  0.          1.        ]\n",
      " [ 0.90924233  0.09075766  1.          0.        ]]\n",
      "###-> Validation accuracy: 78.5 | Best: 78.5\n",
      "Minibatch total loss at step 1105: 0.428361 | Best: 0.32635\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.19008215  0.80991781  0.          1.        ]\n",
      " [ 0.46622682  0.53377312  0.          1.        ]]\n",
      "Minibatch total loss at step 1118: 0.551763 | Best: 0.32635\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.33755401  0.66244602  0.          1.        ]\n",
      " [ 0.63764721  0.36235279  1.          0.        ]]\n",
      "Minibatch total loss at step 1131: 0.513108 | Best: 0.32635\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.87721723  0.1227828   1.          0.        ]\n",
      " [ 0.03077178  0.96922821  0.          1.        ]]\n",
      "Minibatch total loss at step 1144: 0.540699 | Best: 0.32635\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.17190701  0.82809293  0.          1.        ]\n",
      " [ 0.04874549  0.95125449  0.          1.        ]]\n",
      "###-> Validation accuracy: 21.5 | Best: 78.5\n",
      "Minibatch total loss at step 1157: 0.565733 | Best: 0.32635\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.50822711  0.49177289  1.          0.        ]\n",
      " [ 0.14160843  0.85839158  0.          1.        ]]\n",
      "Minibatch total loss at step 1170: 0.384775 | Best: 0.32635\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.16481827  0.83518171  0.          1.        ]\n",
      " [ 0.07439109  0.92560899  0.          1.        ]]\n",
      "Minibatch total loss at step 1183: 0.507198 | Best: 0.32635\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.41838124  0.58161873  0.          1.        ]\n",
      " [ 0.10203131  0.89796871  0.          1.        ]]\n",
      "Minibatch total loss at step 1196: 0.537781 | Best: 0.32635\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.28919634  0.71080369  0.          1.        ]\n",
      " [ 0.77857798  0.22142205  1.          0.        ]]\n",
      "###-> Validation accuracy: 78.5 | Best: 78.5\n",
      "Minibatch total loss at step 1209: 0.516640 | Best: 0.32635\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.40278065  0.59721935  1.          0.        ]\n",
      " [ 0.33759996  0.66240001  0.          1.        ]]\n",
      "Minibatch total loss at step 1222: 0.577038 | Best: 0.32635\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.06373895  0.93626106  0.          1.        ]\n",
      " [ 0.52112681  0.47887319  1.          0.        ]]\n",
      "Minibatch total loss at step 1235: 0.839096 | Best: 0.32635\n",
      "Minibatch accuracy: 43.75\n",
      "Predictions | Labels:\n",
      " [[ 0.35507596  0.64492404  1.          0.        ]\n",
      " [ 0.05968935  0.94031066  0.          1.        ]]\n",
      "Minibatch total loss at step 1248: 0.666006 | Best: 0.32635\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.50972193  0.49027801  1.          0.        ]\n",
      " [ 0.50094867  0.49905136  1.          0.        ]]\n",
      "###-> Validation accuracy: 78.5 | Best: 78.5\n",
      "Minibatch total loss at step 1261: 0.468673 | Best: 0.32635\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.89009964  0.10990035  1.          0.        ]\n",
      " [ 0.77713513  0.22286487  1.          0.        ]]\n",
      "Minibatch total loss at step 1274: 0.349073 | Best: 0.32635\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.5904671   0.4095329   1.          0.        ]\n",
      " [ 0.01562908  0.98437089  0.          1.        ]]\n",
      "Minibatch total loss at step 1287: 0.697674 | Best: 0.32635\n",
      "Minibatch accuracy: 56.25\n",
      "Predictions | Labels:\n",
      " [[ 0.7887876   0.21121235  1.          0.        ]\n",
      " [ 0.72170919  0.27829081  1.          0.        ]]\n",
      "Minibatch total loss at step 1300: 0.383025 | Best: 0.32635\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.15887989  0.84112012  0.          1.        ]\n",
      " [ 0.68163371  0.31836626  0.          1.        ]]\n",
      "###-> Validation accuracy: 78.5 | Best: 78.5\n",
      "Minibatch total loss at step 1313: 0.596613 | Best: 0.32635\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.27481061  0.72518939  0.          1.        ]\n",
      " [ 0.13811174  0.86188823  0.          1.        ]]\n",
      "Minibatch total loss at step 1326: 0.513234 | Best: 0.32635\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.85027385  0.14972612  1.          0.        ]\n",
      " [ 0.3938089   0.6061911   0.          1.        ]]\n",
      "Minibatch total loss at step 1339: 0.390515 | Best: 0.32635\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.27021873  0.72978127  0.          1.        ]\n",
      " [ 0.69821906  0.30178091  0.          1.        ]]\n",
      "###-> Validation accuracy: 78.5 | Best: 78.5\n",
      "Minibatch total loss at step 1352: 0.427045 | Best: 0.32635\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.8068586  0.1931414  1.         0.       ]\n",
      " [ 0.7530126  0.2469874  1.         0.       ]]\n",
      "Minibatch total loss at step 1365: 0.391716 | Best: 0.32635\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.11962681  0.88037324  0.          1.        ]\n",
      " [ 0.05767503  0.942325    0.          1.        ]]\n",
      "Minibatch total loss at step 1378: 0.472570 | Best: 0.32635\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.48366004  0.5163399   0.          1.        ]\n",
      " [ 0.00301889  0.99698108  0.          1.        ]]\n",
      "Minibatch total loss at step 1391: 0.516908 | Best: 0.32635\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.57522589  0.42477414  0.          1.        ]\n",
      " [ 0.67947507  0.32052493  0.          1.        ]]\n",
      "###-> Validation accuracy: 78.5 | Best: 78.5\n",
      "Minibatch total loss at step 1404: 0.753819 | Best: 0.32635\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.31706056  0.68293947  1.          0.        ]\n",
      " [ 0.90168267  0.09831731  1.          0.        ]]\n",
      "Minibatch total loss at step 1417: 0.443260 | Best: 0.32635\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.23716496  0.76283509  0.          1.        ]\n",
      " [ 0.54594398  0.45405599  1.          0.        ]]\n",
      "Minibatch total loss at step 1430: 0.462146 | Best: 0.32635\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.18797734  0.81202263  0.          1.        ]\n",
      " [ 0.90173036  0.09826969  1.          0.        ]]\n",
      "Minibatch total loss at step 1443: 0.484373 | Best: 0.32635\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.87853628  0.12146374  1.          0.        ]\n",
      " [ 0.2337524   0.76624757  0.          1.        ]]\n",
      "###-> Validation accuracy: 78.5 | Best: 78.5\n",
      "Minibatch total loss at step 1456: 0.321621 | Best: 0.321621\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.09321524  0.90678477  0.          1.        ]\n",
      " [ 0.6172049   0.38279516  0.          1.        ]]\n",
      "Minibatch total loss at step 1469: 0.512941 | Best: 0.321621\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.27987602  0.72012401  0.          1.        ]\n",
      " [ 0.3590388   0.64096123  0.          1.        ]]\n",
      "Minibatch total loss at step 1482: 0.553174 | Best: 0.321621\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.34511465  0.65488529  1.          0.        ]\n",
      " [ 0.16883671  0.83116329  0.          1.        ]]\n",
      "Minibatch total loss at step 1495: 0.475666 | Best: 0.321621\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.46021363  0.5397864   0.          1.        ]\n",
      " [ 0.4880465   0.51195359  0.          1.        ]]\n",
      "###-> Validation accuracy: 78.5 | Best: 78.5\n",
      "Minibatch total loss at step 1508: 0.377878 | Best: 0.321621\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.04606136  0.9539386   0.          1.        ]\n",
      " [ 0.18264572  0.81735426  1.          0.        ]]\n",
      "Minibatch total loss at step 1521: 0.442964 | Best: 0.321621\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.25869289  0.74130714  0.          1.        ]\n",
      " [ 0.41400427  0.58599573  0.          1.        ]]\n",
      "Minibatch total loss at step 1534: 0.363252 | Best: 0.321621\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.09816296  0.90183705  0.          1.        ]\n",
      " [ 0.9007988   0.09920119  1.          0.        ]]\n",
      "Minibatch total loss at step 1547: 0.362321 | Best: 0.321621\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.01797343  0.98202652  0.          1.        ]\n",
      " [ 0.97697097  0.02302898  1.          0.        ]]\n",
      "###-> Validation accuracy: 78.5 | Best: 78.5\n",
      "Minibatch total loss at step 1560: 0.184045 | Best: 0.184045\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.19674768  0.80325228  0.          1.        ]\n",
      " [ 0.02331969  0.97668034  0.          1.        ]]\n",
      "Minibatch total loss at step 1573: 0.458861 | Best: 0.184045\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.34962252  0.65037751  1.          0.        ]\n",
      " [ 0.81818205  0.18181792  1.          0.        ]]\n",
      "Minibatch total loss at step 1586: 0.445056 | Best: 0.184045\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.41319868  0.58680123  1.          0.        ]\n",
      " [ 0.79722434  0.20277563  0.          1.        ]]\n",
      "Minibatch total loss at step 1599: 0.423333 | Best: 0.184045\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.71193486  0.28806514  1.          0.        ]\n",
      " [ 0.56732988  0.43267015  0.          1.        ]]\n",
      "###-> Validation accuracy: 78.5 | Best: 78.5\n",
      "Minibatch total loss at step 1612: 0.354428 | Best: 0.184045\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.0177514   0.98224854  0.          1.        ]\n",
      " [ 0.68687242  0.31312755  1.          0.        ]]\n",
      "Minibatch total loss at step 1625: 0.529401 | Best: 0.184045\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.09170175  0.90829819  0.          1.        ]\n",
      " [ 0.076824    0.92317605  0.          1.        ]]\n",
      "Minibatch total loss at step 1638: 0.387348 | Best: 0.184045\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.79074889  0.20925111  1.          0.        ]\n",
      " [ 0.39547366  0.60452634  1.          0.        ]]\n",
      "###-> Validation accuracy: 78.5 | Best: 78.5\n",
      "Minibatch total loss at step 1651: 0.283311 | Best: 0.184045\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.71191239  0.28808761  1.          0.        ]\n",
      " [ 0.00124582  0.99875414  0.          1.        ]]\n",
      "Minibatch total loss at step 1664: 0.351401 | Best: 0.184045\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.93021333  0.06978673  1.          0.        ]\n",
      " [ 0.5116778   0.48832214  1.          0.        ]]\n",
      "Minibatch total loss at step 1677: 0.507753 | Best: 0.184045\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.23000692  0.76999307  1.          0.        ]\n",
      " [ 0.630849    0.369151    1.          0.        ]]\n",
      "Minibatch total loss at step 1690: 0.518948 | Best: 0.184045\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.35217753  0.6478225   0.          1.        ]\n",
      " [ 0.28824493  0.71175504  0.          1.        ]]\n",
      "###-> Validation accuracy: 78.5 | Best: 78.5\n",
      "Minibatch total loss at step 1703: 0.552728 | Best: 0.184045\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.49489483  0.50510514  0.          1.        ]\n",
      " [ 0.01658724  0.98341274  0.          1.        ]]\n",
      "Minibatch total loss at step 1716: 0.428072 | Best: 0.184045\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.49370494  0.50629503  1.          0.        ]\n",
      " [ 0.03033062  0.9696694   0.          1.        ]]\n",
      "Minibatch total loss at step 1729: 0.343699 | Best: 0.184045\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.51237291  0.48762712  0.          1.        ]\n",
      " [ 0.36363977  0.63636023  0.          1.        ]]\n",
      "Minibatch total loss at step 1742: 0.257911 | Best: 0.184045\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.69399095  0.30600905  1.          0.        ]\n",
      " [ 0.55973524  0.44026479  0.          1.        ]]\n",
      "###-> Validation accuracy: 78.5 | Best: 78.5\n",
      "Minibatch total loss at step 1755: 0.529615 | Best: 0.184045\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.22102848  0.77897149  1.          0.        ]\n",
      " [ 0.15718639  0.84281361  0.          1.        ]]\n",
      "Minibatch total loss at step 1768: 0.417391 | Best: 0.184045\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[  5.51857593e-05   9.99944806e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  2.97237605e-01   7.02762425e-01   1.00000000e+00   0.00000000e+00]]\n",
      "Minibatch total loss at step 1781: 0.858741 | Best: 0.184045\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.03228823  0.96771181  0.          1.        ]\n",
      " [ 0.02741127  0.97258872  1.          0.        ]]\n",
      "Minibatch total loss at step 1794: 0.508092 | Best: 0.184045\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.48581854  0.51418149  1.          0.        ]\n",
      " [ 0.62059498  0.37940502  1.          0.        ]]\n",
      "###-> Validation accuracy: 78.5 | Best: 78.5\n",
      "Minibatch total loss at step 1807: 0.631341 | Best: 0.184045\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[  4.70433742e-01   5.29566348e-01   1.00000000e+00   0.00000000e+00]\n",
      " [  7.26199942e-04   9.99273837e-01   0.00000000e+00   1.00000000e+00]]\n",
      "Minibatch total loss at step 1820: 0.384155 | Best: 0.184045\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.14438818  0.8556118   1.          0.        ]\n",
      " [ 0.00745936  0.99254066  0.          1.        ]]\n",
      "Minibatch total loss at step 1833: 0.586815 | Best: 0.184045\n",
      "Minibatch accuracy: 50.0\n",
      "Predictions | Labels:\n",
      " [[ 0.37367797  0.62632197  1.          0.        ]\n",
      " [ 0.06666968  0.93333036  0.          1.        ]]\n",
      "Minibatch total loss at step 1846: 0.421118 | Best: 0.184045\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.02201154  0.97798848  0.          1.        ]\n",
      " [ 0.19415641  0.80584359  0.          1.        ]]\n",
      "###-> Validation accuracy: 78.5 | Best: 78.5\n",
      "Minibatch total loss at step 1859: 0.428602 | Best: 0.184045\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.40431526  0.59568477  1.          0.        ]\n",
      " [ 0.64127648  0.35872358  1.          0.        ]]\n",
      "Minibatch total loss at step 1872: 0.447411 | Best: 0.184045\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.4602865   0.53971356  0.          1.        ]\n",
      " [ 0.00337516  0.99662483  0.          1.        ]]\n",
      "Minibatch total loss at step 1885: 0.534439 | Best: 0.184045\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.55041623  0.44958371  0.          1.        ]\n",
      " [ 0.26651204  0.73348796  0.          1.        ]]\n",
      "Minibatch total loss at step 1898: 0.253104 | Best: 0.184045\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.86346251  0.13653754  1.          0.        ]\n",
      " [ 0.58292162  0.41707844  0.          1.        ]]\n",
      "###-> Validation accuracy: 78.5 | Best: 78.5\n",
      "Minibatch total loss at step 1911: 0.422121 | Best: 0.184045\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.15085405  0.84914601  0.          1.        ]\n",
      " [ 0.03832579  0.96167421  0.          1.        ]]\n",
      "Minibatch total loss at step 1924: 0.345787 | Best: 0.184045\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.38405123  0.6159488   1.          0.        ]\n",
      " [ 0.2248674   0.7751326   0.          1.        ]]\n",
      "Minibatch total loss at step 1937: 0.313778 | Best: 0.184045\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.10393538  0.8960647   0.          1.        ]\n",
      " [ 0.90474582  0.09525412  1.          0.        ]]\n",
      "Minibatch total loss at step 1950: 0.272688 | Best: 0.184045\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.93389595  0.06610407  1.          0.        ]\n",
      " [ 0.31863582  0.68136418  1.          0.        ]]\n",
      "###-> Validation accuracy: 78.5 | Best: 78.5\n",
      "Minibatch total loss at step 1963: 0.429766 | Best: 0.184045\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.22361217  0.77638781  0.          1.        ]\n",
      " [ 0.04149143  0.95850855  0.          1.        ]]\n",
      "Minibatch total loss at step 1976: 0.326996 | Best: 0.184045\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.94543016  0.05456987  1.          0.        ]\n",
      " [ 0.61635906  0.38364094  1.          0.        ]]\n",
      "Minibatch total loss at step 1989: 0.510099 | Best: 0.184045\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[  5.46737552e-01   4.53262448e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  4.01852682e-04   9.99598205e-01   0.00000000e+00   1.00000000e+00]]\n",
      "###-> Validation accuracy: 78.5 | Best: 78.5\n",
      "Minibatch total loss at step 2002: 0.234825 | Best: 0.184045\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.88304573  0.11695429  1.          0.        ]\n",
      " [ 0.11196618  0.88803381  0.          1.        ]]\n",
      "Minibatch total loss at step 2015: 0.265562 | Best: 0.184045\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.72813332  0.27186665  1.          0.        ]\n",
      " [ 0.04941645  0.95058358  0.          1.        ]]\n",
      "Minibatch total loss at step 2028: 0.272874 | Best: 0.184045\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.16107656  0.83892345  0.          1.        ]\n",
      " [ 0.23036233  0.76963776  0.          1.        ]]\n",
      "Minibatch total loss at step 2041: 0.707738 | Best: 0.184045\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[  5.76288939e-01   4.23711091e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  1.01954392e-05   9.99989748e-01   0.00000000e+00   1.00000000e+00]]\n",
      "###-> Validation accuracy: 78.5 | Best: 78.5\n",
      "Minibatch total loss at step 2054: 0.469857 | Best: 0.184045\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.10344379  0.89655614  0.          1.        ]\n",
      " [ 0.10966206  0.89033788  0.          1.        ]]\n",
      "Minibatch total loss at step 2067: 0.296264 | Best: 0.184045\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.72846049  0.27153951  0.          1.        ]\n",
      " [ 0.08248807  0.91751194  0.          1.        ]]\n",
      "Minibatch total loss at step 2080: 0.324716 | Best: 0.184045\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.07255519  0.92744476  0.          1.        ]\n",
      " [ 0.36837992  0.63162005  1.          0.        ]]\n",
      "Minibatch total loss at step 2093: 0.180825 | Best: 0.180825\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.85626286  0.14373711  1.          0.        ]\n",
      " [ 0.91637945  0.08362052  1.          0.        ]]\n",
      "###-> Validation accuracy: 78.5 | Best: 78.5\n",
      "Minibatch total loss at step 2106: 0.315955 | Best: 0.180825\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.40204296  0.59795707  0.          1.        ]\n",
      " [ 0.17712928  0.82287067  0.          1.        ]]\n",
      "Minibatch total loss at step 2119: 0.348008 | Best: 0.180825\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.03242161  0.96757841  0.          1.        ]\n",
      " [ 0.85467452  0.1453255   1.          0.        ]]\n",
      "Minibatch total loss at step 2132: 0.725509 | Best: 0.180825\n",
      "Minibatch accuracy: 62.5\n",
      "Predictions | Labels:\n",
      " [[ 0.55998868  0.44001135  1.          0.        ]\n",
      " [ 0.03277302  0.96722698  1.          0.        ]]\n",
      "Minibatch total loss at step 2145: 0.209973 | Best: 0.180825\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.15633422  0.84366584  0.          1.        ]\n",
      " [ 0.97072923  0.02927079  1.          0.        ]]\n",
      "###-> Validation accuracy: 78.5 | Best: 78.5\n",
      "Minibatch total loss at step 2158: 0.322681 | Best: 0.180825\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.00572265  0.99427742  0.          1.        ]\n",
      " [ 0.57180059  0.42819941  1.          0.        ]]\n",
      "Minibatch total loss at step 2171: 0.118089 | Best: 0.118089\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.01495598  0.985044    0.          1.        ]\n",
      " [ 0.06686551  0.93313444  0.          1.        ]]\n",
      "Minibatch total loss at step 2184: 0.377636 | Best: 0.118089\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.84560341  0.15439658  1.          0.        ]\n",
      " [ 0.18377367  0.81622636  1.          0.        ]]\n",
      "Minibatch total loss at step 2197: 0.240518 | Best: 0.118089\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[  4.93598403e-04   9.99506354e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  3.35170209e-01   6.64829791e-01   0.00000000e+00   1.00000000e+00]]\n",
      "###-> Validation accuracy: 78.5 | Best: 78.5\n",
      "Minibatch total loss at step 2210: 0.575572 | Best: 0.118089\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.9536047   0.04639534  1.          0.        ]\n",
      " [ 0.00323624  0.99676371  0.          1.        ]]\n",
      "Minibatch total loss at step 2223: 0.238508 | Best: 0.118089\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.00356739  0.99643254  0.          1.        ]\n",
      " [ 0.31257379  0.68742615  0.          1.        ]]\n",
      "Minibatch total loss at step 2236: 0.433404 | Best: 0.118089\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.08762983  0.9123702   0.          1.        ]\n",
      " [ 0.90062785  0.09937214  1.          0.        ]]\n",
      "Minibatch total loss at step 2249: 0.323289 | Best: 0.118089\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.72774982  0.27225012  1.          0.        ]\n",
      " [ 0.97654599  0.02345397  1.          0.        ]]\n",
      "###-> Validation accuracy: 78.5 | Best: 78.5\n",
      "Minibatch total loss at step 2262: 0.313846 | Best: 0.118089\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.86566669  0.13433336  0.          1.        ]\n",
      " [ 0.0071451   0.99285489  0.          1.        ]]\n",
      "Minibatch total loss at step 2275: 0.297466 | Best: 0.118089\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.03244328  0.96755666  0.          1.        ]\n",
      " [ 0.92780781  0.07219221  0.          1.        ]]\n",
      "Minibatch total loss at step 2288: 0.304060 | Best: 0.118089\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.9426899   0.05731007  1.          0.        ]\n",
      " [ 0.94282347  0.05717655  1.          0.        ]]\n",
      "###-> Validation accuracy: 78.5 | Best: 78.5\n",
      "Minibatch total loss at step 2301: 0.353258 | Best: 0.118089\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.21579942  0.78420061  1.          0.        ]\n",
      " [ 0.09084066  0.90915936  0.          1.        ]]\n",
      "Minibatch total loss at step 2314: 0.288008 | Best: 0.118089\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.65570873  0.34429118  1.          0.        ]\n",
      " [ 0.04069811  0.95930195  0.          1.        ]]\n",
      "Minibatch total loss at step 2327: 0.145872 | Best: 0.118089\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.86031216  0.13968781  1.          0.        ]\n",
      " [ 0.04575088  0.95424908  0.          1.        ]]\n",
      "Minibatch total loss at step 2340: 0.519016 | Best: 0.118089\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.6198085   0.38019148  1.          0.        ]\n",
      " [ 0.45425776  0.54574227  0.          1.        ]]\n",
      "###-> Validation accuracy: 78.5 | Best: 78.5\n",
      "Minibatch total loss at step 2353: 0.279450 | Best: 0.118089\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.06284557  0.93715441  0.          1.        ]\n",
      " [ 0.72570217  0.27429789  1.          0.        ]]\n",
      "Minibatch total loss at step 2366: 0.360968 | Best: 0.118089\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.59268051  0.40731949  1.          0.        ]\n",
      " [ 0.25132978  0.74867022  0.          1.        ]]\n",
      "Minibatch total loss at step 2379: 0.251131 | Best: 0.118089\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.00544089  0.99455911  0.          1.        ]\n",
      " [ 0.29638287  0.70361716  0.          1.        ]]\n",
      "Minibatch total loss at step 2392: 0.211963 | Best: 0.118089\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.24094881  0.7590512   0.          1.        ]\n",
      " [ 0.06720332  0.93279672  0.          1.        ]]\n",
      "###-> Validation accuracy: 78.5 | Best: 78.5\n",
      "Minibatch total loss at step 2405: 0.257539 | Best: 0.118089\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.92833924  0.07166073  1.          0.        ]\n",
      " [ 0.09214447  0.90785557  0.          1.        ]]\n",
      "Minibatch total loss at step 2418: 0.326384 | Best: 0.118089\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.67794913  0.3220509   1.          0.        ]\n",
      " [ 0.61741358  0.38258645  0.          1.        ]]\n",
      "Minibatch total loss at step 2431: 0.247329 | Best: 0.118089\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.08465175  0.91534823  0.          1.        ]\n",
      " [ 0.71030498  0.28969502  0.          1.        ]]\n",
      "Minibatch total loss at step 2444: 0.456819 | Best: 0.118089\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.00325446  0.99674547  0.          1.        ]\n",
      " [ 0.99368483  0.00631509  1.          0.        ]]\n",
      "###-> Validation accuracy: 78.5 | Best: 78.5\n",
      "Minibatch total loss at step 2457: 0.450610 | Best: 0.118089\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.00428015  0.99571985  0.          1.        ]\n",
      " [ 0.28730813  0.71269184  0.          1.        ]]\n",
      "Minibatch total loss at step 2470: 0.251959 | Best: 0.118089\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[  9.04565632e-01   9.54343081e-02   1.00000000e+00   0.00000000e+00]\n",
      " [  4.21366160e-04   9.99578655e-01   0.00000000e+00   1.00000000e+00]]\n",
      "Minibatch total loss at step 2483: 0.197879 | Best: 0.118089\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.42793292  0.57206708  0.          1.        ]\n",
      " [ 0.22771476  0.77228516  0.          1.        ]]\n",
      "Minibatch total loss at step 2496: 0.498163 | Best: 0.118089\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[  6.53680007e-04   9.99346316e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  5.47020696e-02   9.45297956e-01   0.00000000e+00   1.00000000e+00]]\n",
      "###-> Validation accuracy: 78.5 | Best: 78.5\n",
      "Minibatch total loss at step 2509: 0.533666 | Best: 0.118089\n",
      "Minibatch accuracy: 68.75\n",
      "Predictions | Labels:\n",
      " [[ 0.99070776  0.00929219  1.          0.        ]\n",
      " [ 0.00134514  0.99865484  0.          1.        ]]\n",
      "Minibatch total loss at step 2522: 0.142709 | Best: 0.118089\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.97324139  0.02675864  1.          0.        ]\n",
      " [ 0.80940098  0.19059908  1.          0.        ]]\n",
      "Minibatch total loss at step 2535: 0.198302 | Best: 0.118089\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.14199002  0.85801005  0.          1.        ]\n",
      " [ 0.81697613  0.18302388  1.          0.        ]]\n",
      "Minibatch total loss at step 2548: 0.366682 | Best: 0.118089\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.63642651  0.36357349  0.          1.        ]\n",
      " [ 0.04982306  0.95017695  0.          1.        ]]\n",
      "###-> Validation accuracy: 78.5 | Best: 78.5\n",
      "Minibatch total loss at step 2561: 0.316460 | Best: 0.118089\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.05280961  0.9471904   0.          1.        ]\n",
      " [ 0.30783117  0.69216883  1.          0.        ]]\n",
      "Minibatch total loss at step 2574: 0.524897 | Best: 0.118089\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[  2.33401377e-02   9.76659894e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  9.99171615e-01   8.28411838e-04   0.00000000e+00   1.00000000e+00]]\n",
      "Minibatch total loss at step 2587: 0.278413 | Best: 0.118089\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.14414886  0.85585117  0.          1.        ]\n",
      " [ 0.09081769  0.90918231  0.          1.        ]]\n",
      "Minibatch total loss at step 2600: 0.225323 | Best: 0.118089\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.89856803  0.101432    1.          0.        ]\n",
      " [ 0.15897696  0.84102309  0.          1.        ]]\n",
      "###-> Validation accuracy: 78.5 | Best: 78.5\n",
      "Minibatch total loss at step 2613: 0.172238 | Best: 0.118089\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[  3.74566065e-03   9.96254325e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  2.25643475e-06   9.99997735e-01   0.00000000e+00   1.00000000e+00]]\n",
      "Minibatch total loss at step 2626: 0.398194 | Best: 0.118089\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.61673248  0.38326755  0.          1.        ]\n",
      " [ 0.99437684  0.00562318  1.          0.        ]]\n",
      "Minibatch total loss at step 2639: 0.337585 | Best: 0.118089\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.00477034  0.99522972  0.          1.        ]\n",
      " [ 0.99583     0.00416999  1.          0.        ]]\n",
      "###-> Validation accuracy: 78.5 | Best: 78.5\n",
      "Minibatch total loss at step 2652: 0.308930 | Best: 0.118089\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.18518116  0.81481886  0.          1.        ]\n",
      " [ 0.02606214  0.97393793  0.          1.        ]]\n",
      "Minibatch total loss at step 2665: 0.254682 | Best: 0.118089\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.33837736  0.66162264  0.          1.        ]\n",
      " [ 0.76384747  0.23615259  1.          0.        ]]\n",
      "Minibatch total loss at step 2678: 0.309290 | Best: 0.118089\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.94575822  0.05424179  1.          0.        ]\n",
      " [ 0.00100316  0.99899679  0.          1.        ]]\n",
      "Minibatch total loss at step 2691: 0.373460 | Best: 0.118089\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.10368498  0.89631504  0.          1.        ]\n",
      " [ 0.04319589  0.95680416  0.          1.        ]]\n",
      "###-> Validation accuracy: 78.5 | Best: 78.5\n",
      "Minibatch total loss at step 2704: 0.327332 | Best: 0.118089\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.8649376   0.13506231  1.          0.        ]\n",
      " [ 0.07999535  0.92000467  0.          1.        ]]\n",
      "Minibatch total loss at step 2717: 0.170761 | Best: 0.118089\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.01055888  0.98944104  0.          1.        ]\n",
      " [ 0.00635812  0.99364191  0.          1.        ]]\n",
      "Minibatch total loss at step 2730: 0.204193 | Best: 0.118089\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.02429521  0.97570473  0.          1.        ]\n",
      " [ 0.02195039  0.97804958  0.          1.        ]]\n",
      "Minibatch total loss at step 2743: 0.199987 | Best: 0.118089\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.65676302  0.34323695  0.          1.        ]\n",
      " [ 0.88788813  0.11211184  1.          0.        ]]\n",
      "###-> Validation accuracy: 78.5 | Best: 78.5\n",
      "Minibatch total loss at step 2756: 0.165490 | Best: 0.118089\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.94611406  0.05388593  1.          0.        ]\n",
      " [ 0.10202506  0.89797497  0.          1.        ]]\n",
      "Minibatch total loss at step 2769: 0.245406 | Best: 0.118089\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.10080201  0.899198    0.          1.        ]\n",
      " [ 0.96166825  0.03833176  1.          0.        ]]\n",
      "Minibatch total loss at step 2782: 0.267457 | Best: 0.118089\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[  4.40725803e-01   5.59274137e-01   1.00000000e+00   0.00000000e+00]\n",
      " [  1.08613902e-04   9.99891400e-01   0.00000000e+00   1.00000000e+00]]\n",
      "Minibatch total loss at step 2795: 0.391472 | Best: 0.118089\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.75784171  0.24215834  1.          0.        ]\n",
      " [ 0.16393772  0.83606231  1.          0.        ]]\n",
      "###-> Validation accuracy: 78.5 | Best: 78.5\n",
      "Minibatch total loss at step 2808: 0.237919 | Best: 0.118089\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.99659181  0.00340817  1.          0.        ]\n",
      " [ 0.66708696  0.33291301  1.          0.        ]]\n",
      "Minibatch total loss at step 2821: 0.139279 | Best: 0.118089\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[  9.05012131e-01   9.49878395e-02   1.00000000e+00   0.00000000e+00]\n",
      " [  3.51567956e-04   9.99648452e-01   0.00000000e+00   1.00000000e+00]]\n",
      "Minibatch total loss at step 2834: 0.240796 | Best: 0.118089\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.49636605  0.50363392  1.          0.        ]\n",
      " [ 0.92976409  0.07023584  1.          0.        ]]\n",
      "Minibatch total loss at step 2847: 0.111434 | Best: 0.111434\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.00123898  0.99876106  0.          1.        ]\n",
      " [ 0.18200825  0.81799173  0.          1.        ]]\n",
      "###-> Validation accuracy: 78.5 | Best: 78.5\n",
      "Minibatch total loss at step 2860: 0.078989 | Best: 0.0789895\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.14563446  0.85436559  0.          1.        ]\n",
      " [ 0.01400673  0.98599327  0.          1.        ]]\n",
      "Minibatch total loss at step 2873: 0.069791 | Best: 0.069791\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.97462893  0.02537104  1.          0.        ]\n",
      " [ 0.0026202   0.99737972  0.          1.        ]]\n",
      "Minibatch total loss at step 2886: 0.124746 | Best: 0.069791\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.01296696  0.98703301  0.          1.        ]\n",
      " [ 0.16644947  0.83355051  0.          1.        ]]\n",
      "Minibatch total loss at step 2899: 0.225213 | Best: 0.069791\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.98629051  0.01370948  1.          0.        ]\n",
      " [ 0.52441108  0.47558889  1.          0.        ]]\n",
      "###-> Validation accuracy: 78.5 | Best: 78.5\n",
      "Minibatch total loss at step 2912: 0.232732 | Best: 0.069791\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.13337795  0.86662197  0.          1.        ]\n",
      " [ 0.0025077   0.99749231  0.          1.        ]]\n",
      "Minibatch total loss at step 2925: 0.280067 | Best: 0.069791\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[  9.96517949e-03   9.90034878e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  3.86951911e-07   9.99999642e-01   0.00000000e+00   1.00000000e+00]]\n",
      "Minibatch total loss at step 2938: 0.126338 | Best: 0.069791\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.02049871  0.97950131  0.          1.        ]\n",
      " [ 0.18617198  0.81382799  0.          1.        ]]\n",
      "###-> Validation accuracy: 78.5 | Best: 78.5\n",
      "Minibatch total loss at step 2951: 0.208072 | Best: 0.069791\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.86959505  0.13040492  1.          0.        ]\n",
      " [ 0.99659061  0.00340937  1.          0.        ]]\n",
      "Minibatch total loss at step 2964: 0.114074 | Best: 0.069791\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.0011816   0.99881834  0.          1.        ]\n",
      " [ 0.94686145  0.05313853  1.          0.        ]]\n",
      "Minibatch total loss at step 2977: 0.209250 | Best: 0.069791\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.06992464  0.93007535  0.          1.        ]\n",
      " [ 0.72386128  0.27613878  1.          0.        ]]\n",
      "Minibatch total loss at step 2990: 0.367901 | Best: 0.069791\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.94105655  0.05894342  1.          0.        ]\n",
      " [ 0.05324268  0.94675732  0.          1.        ]]\n",
      "###-> Validation accuracy: 78.5 | Best: 78.5\n",
      "Minibatch total loss at step 3003: 0.207902 | Best: 0.069791\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.00225912  0.99774086  0.          1.        ]\n",
      " [ 0.57309479  0.42690518  0.          1.        ]]\n",
      "Minibatch total loss at step 3016: 0.346130 | Best: 0.069791\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.00300233  0.99699771  0.          1.        ]\n",
      " [ 0.15134364  0.84865636  0.          1.        ]]\n",
      "Minibatch total loss at step 3029: 0.174405 | Best: 0.069791\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.77572173  0.22427835  1.          0.        ]\n",
      " [ 0.07034772  0.92965233  0.          1.        ]]\n",
      "Minibatch total loss at step 3042: 0.262590 | Best: 0.069791\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.37898701  0.62101305  0.          1.        ]\n",
      " [ 0.08825093  0.91174901  0.          1.        ]]\n",
      "###-> Validation accuracy: 78.5 | Best: 78.5\n",
      "Minibatch total loss at step 3055: 0.367003 | Best: 0.069791\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.00803345  0.99196661  0.          1.        ]\n",
      " [ 0.43596277  0.5640372   1.          0.        ]]\n",
      "Minibatch total loss at step 3068: 0.272589 | Best: 0.069791\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.73826587  0.26173407  0.          1.        ]\n",
      " [ 0.01591565  0.98408443  0.          1.        ]]\n",
      "Minibatch total loss at step 3081: 0.524028 | Best: 0.069791\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.00409121  0.99590874  0.          1.        ]\n",
      " [ 0.90899819  0.09100179  1.          0.        ]]\n",
      "Minibatch total loss at step 3094: 0.260338 | Best: 0.069791\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[  8.48860964e-06   9.99991536e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  9.98401582e-01   1.59842905e-03   1.00000000e+00   0.00000000e+00]]\n",
      "###-> Validation accuracy: 78.5 | Best: 78.5\n",
      "Minibatch total loss at step 3107: 0.098318 | Best: 0.069791\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[  9.53569892e-04   9.99046385e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  1.16259605e-02   9.88373995e-01   0.00000000e+00   1.00000000e+00]]\n",
      "Minibatch total loss at step 3120: 0.316205 | Best: 0.069791\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.97768438  0.02231569  1.          0.        ]\n",
      " [ 0.3042334   0.69576657  1.          0.        ]]\n",
      "Minibatch total loss at step 3133: 0.228568 | Best: 0.069791\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.98646551  0.01353443  1.          0.        ]\n",
      " [ 0.43193111  0.56806892  0.          1.        ]]\n",
      "Minibatch total loss at step 3146: 0.111075 | Best: 0.069791\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.90991914  0.09008088  1.          0.        ]\n",
      " [ 0.35876489  0.64123511  0.          1.        ]]\n",
      "###-> Validation accuracy: 78.5 | Best: 78.5\n",
      "Minibatch total loss at step 3159: 0.465595 | Best: 0.069791\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.00500277  0.99499726  0.          1.        ]\n",
      " [ 0.16293013  0.83706987  1.          0.        ]]\n",
      "Minibatch total loss at step 3172: 0.134074 | Best: 0.069791\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.28726643  0.71273363  0.          1.        ]\n",
      " [ 0.24896826  0.75103176  0.          1.        ]]\n",
      "Minibatch total loss at step 3185: 0.274985 | Best: 0.069791\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.98250145  0.01749858  1.          0.        ]\n",
      " [ 0.95116979  0.0488302   1.          0.        ]]\n",
      "Minibatch total loss at step 3198: 0.062681 | Best: 0.0626815\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[  9.33975041e-01   6.60250038e-02   1.00000000e+00   0.00000000e+00]\n",
      " [  3.31959920e-04   9.99668002e-01   0.00000000e+00   1.00000000e+00]]\n",
      "###-> Validation accuracy: 78.5 | Best: 78.5\n",
      "Minibatch total loss at step 3211: 0.253036 | Best: 0.0626815\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.97083068  0.02916937  1.          0.        ]\n",
      " [ 0.89300525  0.10699474  1.          0.        ]]\n",
      "Minibatch total loss at step 3224: 0.275836 | Best: 0.0626815\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.31259263  0.68740743  1.          0.        ]\n",
      " [ 0.88724035  0.1127597   1.          0.        ]]\n",
      "Minibatch total loss at step 3237: 0.172125 | Best: 0.0626815\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.11009596  0.88990402  0.          1.        ]\n",
      " [ 0.72471422  0.27528575  0.          1.        ]]\n",
      "Minibatch total loss at step 3250: 0.383989 | Best: 0.0626815\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[  6.94235563e-01   3.05764437e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  9.89881344e-04   9.99010086e-01   0.00000000e+00   1.00000000e+00]]\n",
      "###-> Validation accuracy: 78.5 | Best: 78.5\n",
      "Minibatch total loss at step 3263: 0.396398 | Best: 0.0626815\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.42345929  0.57654071  1.          0.        ]\n",
      " [ 0.16584229  0.83415765  0.          1.        ]]\n",
      "Minibatch total loss at step 3276: 0.423134 | Best: 0.0626815\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.04441366  0.95558631  0.          1.        ]\n",
      " [ 0.14495188  0.85504812  0.          1.        ]]\n",
      "Minibatch total loss at step 3289: 0.204122 | Best: 0.0626815\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.69951957  0.30048046  1.          0.        ]\n",
      " [ 0.66367656  0.33632347  0.          1.        ]]\n",
      "###-> Validation accuracy: 78.5 | Best: 78.5\n",
      "Minibatch total loss at step 3302: 0.199988 | Best: 0.0626815\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.98034084  0.0196592   1.          0.        ]\n",
      " [ 0.22018565  0.77981436  0.          1.        ]]\n",
      "Minibatch total loss at step 3315: 0.336547 | Best: 0.0626815\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[  1.69720110e-07   9.99999881e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  4.35006291e-01   5.64993680e-01   1.00000000e+00   0.00000000e+00]]\n",
      "Minibatch total loss at step 3328: 0.263367 | Best: 0.0626815\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.01446805  0.98553199  0.          1.        ]\n",
      " [ 0.95227796  0.04772206  1.          0.        ]]\n",
      "Minibatch total loss at step 3341: 0.303702 | Best: 0.0626815\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.94285053  0.05714951  1.          0.        ]\n",
      " [ 0.7308054   0.26919457  1.          0.        ]]\n",
      "###-> Validation accuracy: 78.5 | Best: 78.5\n",
      "Minibatch total loss at step 3354: 0.233358 | Best: 0.0626815\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[  9.88599896e-01   1.14000905e-02   1.00000000e+00   0.00000000e+00]\n",
      " [  1.41408066e-07   9.99999881e-01   0.00000000e+00   1.00000000e+00]]\n",
      "Minibatch total loss at step 3367: 0.274665 | Best: 0.0626815\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.92966646  0.07033358  1.          0.        ]\n",
      " [ 0.00898593  0.99101406  0.          1.        ]]\n",
      "Minibatch total loss at step 3380: 0.262817 | Best: 0.0626815\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.7615729   0.23842715  1.          0.        ]\n",
      " [ 0.00128833  0.99871171  0.          1.        ]]\n",
      "Minibatch total loss at step 3393: 0.179005 | Best: 0.0626815\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.00209256  0.99790752  0.          1.        ]\n",
      " [ 0.02520505  0.97479498  0.          1.        ]]\n",
      "###-> Validation accuracy: 78.5 | Best: 78.5\n",
      "Minibatch total loss at step 3406: 0.198014 | Best: 0.0626815\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.96342337  0.03657661  1.          0.        ]\n",
      " [ 0.69554198  0.30445799  1.          0.        ]]\n",
      "Minibatch total loss at step 3419: 0.058823 | Best: 0.0588233\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[  1.21737927e-01   8.78262103e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  5.40922665e-05   9.99945879e-01   0.00000000e+00   1.00000000e+00]]\n",
      "Minibatch total loss at step 3432: 0.147070 | Best: 0.0588233\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.09972131  0.90027869  0.          1.        ]\n",
      " [ 0.0010637   0.99893636  0.          1.        ]]\n",
      "Minibatch total loss at step 3445: 0.040951 | Best: 0.0409513\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.96544594  0.03455405  1.          0.        ]\n",
      " [ 0.01734119  0.98265874  0.          1.        ]]\n",
      "###-> Validation accuracy: 78.5 | Best: 78.5\n",
      "Minibatch total loss at step 3458: 0.197759 | Best: 0.0409513\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.0596641   0.94033593  0.          1.        ]\n",
      " [ 0.00176113  0.9982388   0.          1.        ]]\n",
      "Minibatch total loss at step 3471: 0.204193 | Best: 0.0409513\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.99829     0.00170995  1.          0.        ]\n",
      " [ 0.10391134  0.8960886   0.          1.        ]]\n",
      "Minibatch total loss at step 3484: 0.204333 | Best: 0.0409513\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.00159788  0.99840218  0.          1.        ]\n",
      " [ 0.78262365  0.21737629  1.          0.        ]]\n",
      "Minibatch total loss at step 3497: 0.142613 | Best: 0.0409513\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.99853575  0.00146429  1.          0.        ]\n",
      " [ 0.98619044  0.01380952  1.          0.        ]]\n",
      "###-> Validation accuracy: 78.5 | Best: 78.5\n",
      "Minibatch total loss at step 3510: 0.287281 | Best: 0.0409513\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.00383875  0.99616122  0.          1.        ]\n",
      " [ 0.18861072  0.81138927  0.          1.        ]]\n",
      "Minibatch total loss at step 3523: 0.127408 | Best: 0.0409513\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.97572052  0.02427944  1.          0.        ]\n",
      " [ 0.99630004  0.00370004  1.          0.        ]]\n",
      "Minibatch total loss at step 3536: 0.260399 | Best: 0.0409513\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[  2.95682758e-01   7.04317272e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  5.92196666e-05   9.99940753e-01   0.00000000e+00   1.00000000e+00]]\n",
      "Minibatch total loss at step 3549: 0.046468 | Best: 0.0409513\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[  5.76343477e-01   4.23656493e-01   1.00000000e+00   0.00000000e+00]\n",
      " [  1.12247471e-04   9.99887705e-01   0.00000000e+00   1.00000000e+00]]\n",
      "###-> Validation accuracy: 78.5 | Best: 78.5\n",
      "Minibatch total loss at step 3562: 0.028094 | Best: 0.0280935\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.94718099  0.05281904  1.          0.        ]\n",
      " [ 0.11334395  0.88665611  0.          1.        ]]\n",
      "Minibatch total loss at step 3575: 0.100614 | Best: 0.0280935\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.08470169  0.91529828  0.          1.        ]\n",
      " [ 0.00610317  0.9938969   0.          1.        ]]\n",
      "Minibatch total loss at step 3588: 0.193145 | Best: 0.0280935\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[  6.75424188e-02   9.32457507e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  2.90332054e-07   9.99999762e-01   0.00000000e+00   1.00000000e+00]]\n",
      "###-> Validation accuracy: 78.5 | Best: 78.5\n",
      "Minibatch total loss at step 3601: 0.157349 | Best: 0.0280935\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[  1.32476824e-04   9.99867558e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  1.74055863e-02   9.82594430e-01   0.00000000e+00   1.00000000e+00]]\n",
      "Minibatch total loss at step 3614: 0.132536 | Best: 0.0280935\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.19613385  0.80386609  0.          1.        ]\n",
      " [ 0.07972032  0.92027968  0.          1.        ]]\n",
      "Minibatch total loss at step 3627: 0.279928 | Best: 0.0280935\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[  2.92707904e-04   9.99707282e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  9.76109266e-01   2.38907319e-02   1.00000000e+00   0.00000000e+00]]\n",
      "Minibatch total loss at step 3640: 0.155887 | Best: 0.0280935\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.84090006  0.15909994  1.          0.        ]\n",
      " [ 0.95762444  0.04237555  1.          0.        ]]\n",
      "###-> Validation accuracy: 79.5 | Best: 79.5\n",
      "Minibatch total loss at step 3653: 0.134438 | Best: 0.0280935\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.21047707  0.78952295  0.          1.        ]\n",
      " [ 0.05803676  0.9419632   0.          1.        ]]\n",
      "Minibatch total loss at step 3666: 0.181665 | Best: 0.0280935\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.00575475  0.99424523  0.          1.        ]\n",
      " [ 0.98091596  0.01908397  1.          0.        ]]\n",
      "Minibatch total loss at step 3679: 0.193528 | Best: 0.0280935\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.96467185  0.03532812  1.          0.        ]\n",
      " [ 0.44785237  0.55214763  1.          0.        ]]\n",
      "Minibatch total loss at step 3692: 0.158486 | Best: 0.0280935\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.45985404  0.54014599  0.          1.        ]\n",
      " [ 0.49293578  0.50706422  1.          0.        ]]\n",
      "###-> Validation accuracy: 78.5 | Best: 79.5\n",
      "Minibatch total loss at step 3705: 0.248422 | Best: 0.0280935\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[  1.60855343e-04   9.99839067e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  9.98648584e-01   1.35148515e-03   1.00000000e+00   0.00000000e+00]]\n",
      "Minibatch total loss at step 3718: 0.038506 | Best: 0.0280935\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[  8.71199183e-04   9.99128759e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  1.87780056e-03   9.98122156e-01   0.00000000e+00   1.00000000e+00]]\n",
      "Minibatch total loss at step 3731: 0.148278 | Best: 0.0280935\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.99309921  0.00690081  1.          0.        ]\n",
      " [ 0.99136961  0.00863036  1.          0.        ]]\n",
      "Minibatch total loss at step 3744: 0.039557 | Best: 0.0280935\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[  1.07236365e-05   9.99989271e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  4.90586124e-02   9.50941443e-01   0.00000000e+00   1.00000000e+00]]\n",
      "###-> Validation accuracy: 78.5 | Best: 79.5\n",
      "Minibatch total loss at step 3757: 0.056959 | Best: 0.0280935\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[  9.99066412e-01   9.33654082e-04   1.00000000e+00   0.00000000e+00]\n",
      " [  3.14369012e-04   9.99685645e-01   0.00000000e+00   1.00000000e+00]]\n",
      "Minibatch total loss at step 3770: 0.261624 | Best: 0.0280935\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[  3.26441824e-02   9.67355847e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  7.53260567e-04   9.99246716e-01   0.00000000e+00   1.00000000e+00]]\n",
      "Minibatch total loss at step 3783: 0.039557 | Best: 0.0280935\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.00154276  0.99845719  0.          1.        ]\n",
      " [ 0.9746455   0.02535457  1.          0.        ]]\n",
      "Minibatch total loss at step 3796: 0.289070 | Best: 0.0280935\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.80876064  0.19123934  1.          0.        ]\n",
      " [ 0.8282519   0.17174816  1.          0.        ]]\n",
      "###-> Validation accuracy: 79.5 | Best: 79.5\n",
      "Minibatch total loss at step 3809: 0.079878 | Best: 0.0280935\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[  5.57482839e-01   4.42517191e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  2.82277237e-04   9.99717653e-01   0.00000000e+00   1.00000000e+00]]\n",
      "Minibatch total loss at step 3822: 0.144525 | Best: 0.0280935\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.00959177  0.99040818  0.          1.        ]\n",
      " [ 0.73829263  0.2617074   0.          1.        ]]\n",
      "Minibatch total loss at step 3835: 0.090403 | Best: 0.0280935\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.97901571  0.02098429  1.          0.        ]\n",
      " [ 0.86460733  0.13539268  1.          0.        ]]\n",
      "Minibatch total loss at step 3848: 0.044572 | Best: 0.0280935\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.97088683  0.0291132   1.          0.        ]\n",
      " [ 0.04134314  0.95865685  0.          1.        ]]\n",
      "###-> Validation accuracy: 78.5 | Best: 79.5\n",
      "Minibatch total loss at step 3861: 0.035003 | Best: 0.0280935\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.99123693  0.00876313  1.          0.        ]\n",
      " [ 0.00261208  0.99738795  0.          1.        ]]\n",
      "Minibatch total loss at step 3874: 0.223497 | Best: 0.0280935\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.95151967  0.04848029  1.          0.        ]\n",
      " [ 0.27010515  0.72989482  0.          1.        ]]\n",
      "Minibatch total loss at step 3887: 0.172403 | Best: 0.0280935\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.31866908  0.68133098  1.          0.        ]\n",
      " [ 0.22718757  0.77281249  0.          1.        ]]\n",
      "Minibatch total loss at step 3900: 0.178831 | Best: 0.0280935\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.03164789  0.96835214  0.          1.        ]\n",
      " [ 0.76244962  0.23755038  1.          0.        ]]\n",
      "###-> Validation accuracy: 79.0 | Best: 79.5\n",
      "Minibatch total loss at step 3913: 0.304138 | Best: 0.0280935\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[  5.15550971e-01   4.84449029e-01   1.00000000e+00   0.00000000e+00]\n",
      " [  1.31731440e-05   9.99986768e-01   0.00000000e+00   1.00000000e+00]]\n",
      "Minibatch total loss at step 3926: 0.200994 | Best: 0.0280935\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.35107344  0.64892656  0.          1.        ]\n",
      " [ 0.07926096  0.920739    0.          1.        ]]\n",
      "Minibatch total loss at step 3939: 0.131108 | Best: 0.0280935\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.00172433  0.99827564  0.          1.        ]\n",
      " [ 0.13639088  0.86360908  0.          1.        ]]\n",
      "###-> Validation accuracy: 79.0 | Best: 79.5\n",
      "Minibatch total loss at step 3952: 0.357170 | Best: 0.0280935\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[  8.71817172e-01   1.28182843e-01   1.00000000e+00   0.00000000e+00]\n",
      " [  3.09143419e-04   9.99690890e-01   0.00000000e+00   1.00000000e+00]]\n",
      "Minibatch total loss at step 3965: 0.204288 | Best: 0.0280935\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[  9.99952435e-01   4.75347297e-05   1.00000000e+00   0.00000000e+00]\n",
      " [  9.80917364e-03   9.90190864e-01   0.00000000e+00   1.00000000e+00]]\n",
      "Minibatch total loss at step 3978: 0.253905 | Best: 0.0280935\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.26489636  0.73510367  0.          1.        ]\n",
      " [ 0.64257479  0.35742524  0.          1.        ]]\n",
      "Minibatch total loss at step 3991: 0.298326 | Best: 0.0280935\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[  6.21267827e-04   9.99378681e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  9.49006379e-01   5.09935766e-02   1.00000000e+00   0.00000000e+00]]\n",
      "###-> Validation accuracy: 79.5 | Best: 79.5\n",
      "Minibatch total loss at step 4004: 0.323468 | Best: 0.0280935\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.00102665  0.99897337  0.          1.        ]\n",
      " [ 0.00507367  0.99492633  0.          1.        ]]\n",
      "Minibatch total loss at step 4017: 0.096242 | Best: 0.0280935\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[  8.02671075e-01   1.97328866e-01   1.00000000e+00   0.00000000e+00]\n",
      " [  1.81925832e-04   9.99818146e-01   0.00000000e+00   1.00000000e+00]]\n",
      "Minibatch total loss at step 4030: 0.101556 | Best: 0.0280935\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.69692343  0.3030766   0.          1.        ]\n",
      " [ 0.00404329  0.99595672  0.          1.        ]]\n",
      "Minibatch total loss at step 4043: 0.190955 | Best: 0.0280935\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[  7.56954891e-04   9.99243021e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  4.27692443e-01   5.72307587e-01   0.00000000e+00   1.00000000e+00]]\n",
      "###-> Validation accuracy: 78.5 | Best: 79.5\n",
      "Minibatch total loss at step 4056: 0.147852 | Best: 0.0280935\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.98137569  0.01862432  1.          0.        ]\n",
      " [ 0.40946266  0.59053737  0.          1.        ]]\n",
      "Minibatch total loss at step 4069: 0.245764 | Best: 0.0280935\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.98347753  0.01652243  1.          0.        ]\n",
      " [ 0.39572862  0.60427141  1.          0.        ]]\n",
      "Minibatch total loss at step 4082: 0.190331 | Best: 0.0280935\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.01755383  0.98244619  0.          1.        ]\n",
      " [ 0.63992536  0.36007467  1.          0.        ]]\n",
      "Minibatch total loss at step 4095: 0.123259 | Best: 0.0280935\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[  3.17309946e-01   6.82690084e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  7.07185536e-04   9.99292731e-01   0.00000000e+00   1.00000000e+00]]\n",
      "###-> Validation accuracy: 78.5 | Best: 79.5\n",
      "Minibatch total loss at step 4108: 0.219152 | Best: 0.0280935\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.01554887  0.98445117  0.          1.        ]\n",
      " [ 0.95718294  0.04281701  1.          0.        ]]\n",
      "Minibatch total loss at step 4121: 0.438680 | Best: 0.0280935\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.01637751  0.98362249  0.          1.        ]\n",
      " [ 0.98867851  0.0113215   0.          1.        ]]\n",
      "Minibatch total loss at step 4134: 0.051901 | Best: 0.0280935\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.00234986  0.99765009  0.          1.        ]\n",
      " [ 0.02224583  0.97775424  0.          1.        ]]\n",
      "Minibatch total loss at step 4147: 0.193458 | Best: 0.0280935\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.98287612  0.0171239   1.          0.        ]\n",
      " [ 0.10350817  0.89649177  0.          1.        ]]\n",
      "###-> Validation accuracy: 78.5 | Best: 79.5\n",
      "Minibatch total loss at step 4160: 0.118781 | Best: 0.0280935\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[  5.51231345e-03   9.94487703e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  4.10042003e-06   9.99995947e-01   0.00000000e+00   1.00000000e+00]]\n",
      "Minibatch total loss at step 4173: 0.233031 | Best: 0.0280935\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.0029659   0.99703407  0.          1.        ]\n",
      " [ 0.99747145  0.00252859  1.          0.        ]]\n",
      "Minibatch total loss at step 4186: 0.118941 | Best: 0.0280935\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.00307322  0.99692684  0.          1.        ]\n",
      " [ 0.96551448  0.03448549  1.          0.        ]]\n",
      "Minibatch total loss at step 4199: 0.055681 | Best: 0.0280935\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.25970569  0.74029434  0.          1.        ]\n",
      " [ 0.00151913  0.99848086  0.          1.        ]]\n",
      "###-> Validation accuracy: 78.5 | Best: 79.5\n",
      "Minibatch total loss at step 4212: 0.275896 | Best: 0.0280935\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.35972053  0.64027947  0.          1.        ]\n",
      " [ 0.69787532  0.30212468  1.          0.        ]]\n",
      "Minibatch total loss at step 4225: 0.085265 | Best: 0.0280935\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[  9.79401052e-01   2.05989610e-02   1.00000000e+00   0.00000000e+00]\n",
      " [  4.83459706e-04   9.99516487e-01   0.00000000e+00   1.00000000e+00]]\n",
      "Minibatch total loss at step 4238: 0.047399 | Best: 0.0280935\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.00288787  0.99711215  0.          1.        ]\n",
      " [ 0.03830636  0.96169358  0.          1.        ]]\n",
      "###-> Validation accuracy: 78.5 | Best: 79.5\n",
      "Minibatch total loss at step 4251: 0.390614 | Best: 0.0280935\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[  9.93206739e-01   6.79333229e-03   1.00000000e+00   0.00000000e+00]\n",
      " [  2.65126873e-04   9.99734819e-01   0.00000000e+00   1.00000000e+00]]\n",
      "Minibatch total loss at step 4264: 0.109360 | Best: 0.0280935\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[  1.62847445e-03   9.98371542e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  1.22487572e-05   9.99987721e-01   0.00000000e+00   1.00000000e+00]]\n",
      "Minibatch total loss at step 4277: 0.042506 | Best: 0.0280935\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.07862884  0.92137116  0.          1.        ]\n",
      " [ 0.00317498  0.99682498  0.          1.        ]]\n",
      "Minibatch total loss at step 4290: 0.143377 | Best: 0.0280935\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.24291067  0.75708932  0.          1.        ]\n",
      " [ 0.93674946  0.06325053  1.          0.        ]]\n",
      "###-> Validation accuracy: 79.0 | Best: 79.5\n",
      "Minibatch total loss at step 4303: 0.120264 | Best: 0.0280935\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.68796831  0.31203172  1.          0.        ]\n",
      " [ 0.01660428  0.9833957   0.          1.        ]]\n",
      "Minibatch total loss at step 4316: 0.085772 | Best: 0.0280935\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[  5.22446586e-04   9.99477565e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  8.89116228e-01   1.10883802e-01   1.00000000e+00   0.00000000e+00]]\n",
      "Minibatch total loss at step 4329: 0.167525 | Best: 0.0280935\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[  9.97626722e-01   2.37329467e-03   1.00000000e+00   0.00000000e+00]\n",
      " [  3.09488223e-06   9.99996901e-01   0.00000000e+00   1.00000000e+00]]\n",
      "Minibatch total loss at step 4342: 0.144708 | Best: 0.0280935\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.96157312  0.03842685  1.          0.        ]\n",
      " [ 0.83160621  0.16839375  1.          0.        ]]\n",
      "###-> Validation accuracy: 78.5 | Best: 79.5\n",
      "Minibatch total loss at step 4355: 0.312380 | Best: 0.0280935\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[  9.99392629e-01   6.07376220e-04   1.00000000e+00   0.00000000e+00]\n",
      " [  1.38483137e-01   8.61516893e-01   1.00000000e+00   0.00000000e+00]]\n",
      "Minibatch total loss at step 4368: 0.028182 | Best: 0.0280935\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[  9.81703818e-01   1.82961840e-02   1.00000000e+00   0.00000000e+00]\n",
      " [  2.33226339e-04   9.99766767e-01   0.00000000e+00   1.00000000e+00]]\n",
      "Minibatch total loss at step 4381: 0.091241 | Best: 0.0280935\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.9189108   0.08108921  1.          0.        ]\n",
      " [ 0.95286596  0.04713402  1.          0.        ]]\n",
      "Minibatch total loss at step 4394: 0.073526 | Best: 0.0280935\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.2379716   0.7620284   0.          1.        ]\n",
      " [ 0.00666293  0.99333704  0.          1.        ]]\n",
      "###-> Validation accuracy: 80.5 | Best: 80.5\n",
      "Minibatch total loss at step 4407: 0.110196 | Best: 0.0280935\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[  6.77502132e-04   9.99322534e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  2.45412276e-03   9.97545898e-01   0.00000000e+00   1.00000000e+00]]\n",
      "Minibatch total loss at step 4420: 0.047141 | Best: 0.0280935\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.97581428  0.02418569  1.          0.        ]\n",
      " [ 0.04463444  0.95536554  0.          1.        ]]\n",
      "Minibatch total loss at step 4433: 0.180977 | Best: 0.0280935\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.5012092   0.4987908   0.          1.        ]\n",
      " [ 0.00793795  0.99206209  0.          1.        ]]\n",
      "Minibatch total loss at step 4446: 0.109150 | Best: 0.0280935\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.92275298  0.07724697  1.          0.        ]\n",
      " [ 0.97745717  0.0225428   1.          0.        ]]\n",
      "###-> Validation accuracy: 80.0 | Best: 80.5\n",
      "Minibatch total loss at step 4459: 0.126189 | Best: 0.0280935\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.0260631   0.97393692  0.          1.        ]\n",
      " [ 0.00137985  0.99862015  0.          1.        ]]\n",
      "Minibatch total loss at step 4472: 0.108168 | Best: 0.0280935\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[  2.64042197e-03   9.97359574e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  3.45193901e-10   1.00000000e+00   0.00000000e+00   1.00000000e+00]]\n",
      "Minibatch total loss at step 4485: 0.137081 | Best: 0.0280935\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.20120648  0.79879349  0.          1.        ]\n",
      " [ 0.00669468  0.99330527  0.          1.        ]]\n",
      "Minibatch total loss at step 4498: 0.101844 | Best: 0.0280935\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[  7.76097655e-01   2.23902404e-01   1.00000000e+00   0.00000000e+00]\n",
      " [  9.99426723e-01   5.73307800e-04   1.00000000e+00   0.00000000e+00]]\n",
      "###-> Validation accuracy: 78.0 | Best: 80.5\n",
      "Minibatch total loss at step 4511: 0.477401 | Best: 0.0280935\n",
      "Minibatch accuracy: 75.0\n",
      "Predictions | Labels:\n",
      " [[ 0.01675384  0.98324615  0.          1.        ]\n",
      " [ 0.98428625  0.01571371  1.          0.        ]]\n",
      "Minibatch total loss at step 4524: 0.127614 | Best: 0.0280935\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.70578188  0.29421812  0.          1.        ]\n",
      " [ 0.95426035  0.04573961  1.          0.        ]]\n",
      "Minibatch total loss at step 4537: 0.346754 | Best: 0.0280935\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.99839407  0.00160591  1.          0.        ]\n",
      " [ 0.01907821  0.9809218   0.          1.        ]]\n",
      "Minibatch total loss at step 4550: 0.090127 | Best: 0.0280935\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.00322705  0.99677294  0.          1.        ]\n",
      " [ 0.15256315  0.84743685  0.          1.        ]]\n",
      "###-> Validation accuracy: 80.0 | Best: 80.5\n",
      "Minibatch total loss at step 4563: 0.040241 | Best: 0.0280935\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[  3.45280190e-04   9.99654770e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  1.03062771e-01   8.96937191e-01   0.00000000e+00   1.00000000e+00]]\n",
      "Minibatch total loss at step 4576: 0.049099 | Best: 0.0280935\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.84035665  0.15964338  1.          0.        ]\n",
      " [ 0.01886574  0.9811343   0.          1.        ]]\n",
      "Minibatch total loss at step 4589: 0.121290 | Best: 0.0280935\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.01362995  0.98637009  0.          1.        ]\n",
      " [ 0.27377081  0.72622919  0.          1.        ]]\n",
      "###-> Validation accuracy: 78.0 | Best: 80.5\n",
      "Minibatch total loss at step 4602: 0.024318 | Best: 0.0243175\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[  5.71998127e-04   9.99427974e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  8.65749061e-01   1.34250969e-01   1.00000000e+00   0.00000000e+00]]\n",
      "Minibatch total loss at step 4615: 0.455760 | Best: 0.0243175\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[  1.03249309e-04   9.99896765e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  8.41618553e-02   9.15838063e-01   0.00000000e+00   1.00000000e+00]]\n",
      "Minibatch total loss at step 4628: 0.035806 | Best: 0.0243175\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[  1.78653215e-06   9.99998212e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  8.45399261e-01   1.54600754e-01   1.00000000e+00   0.00000000e+00]]\n",
      "Minibatch total loss at step 4641: 0.031246 | Best: 0.0243175\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[  4.56250973e-06   9.99995470e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  9.97899532e-01   2.10045045e-03   1.00000000e+00   0.00000000e+00]]\n",
      "###-> Validation accuracy: 78.5 | Best: 80.5\n",
      "Minibatch total loss at step 4654: 0.024840 | Best: 0.0243175\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[  2.30754988e-04   9.99769270e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  7.21124743e-05   9.99927878e-01   0.00000000e+00   1.00000000e+00]]\n",
      "Minibatch total loss at step 4667: 0.110514 | Best: 0.0243175\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.9938572   0.00614283  1.          0.        ]\n",
      " [ 0.93469626  0.06530372  1.          0.        ]]\n",
      "Minibatch total loss at step 4680: 0.098101 | Best: 0.0243175\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.99471384  0.00528608  1.          0.        ]\n",
      " [ 0.0078469   0.99215311  0.          1.        ]]\n",
      "Minibatch total loss at step 4693: 0.233594 | Best: 0.0243175\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.89985448  0.10014551  1.          0.        ]\n",
      " [ 0.05845227  0.94154769  0.          1.        ]]\n",
      "###-> Validation accuracy: 75.5 | Best: 80.5\n",
      "Minibatch total loss at step 4706: 0.320955 | Best: 0.0243175\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[  2.61561858e-04   9.99738395e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  8.85688365e-01   1.14311583e-01   1.00000000e+00   0.00000000e+00]]\n",
      "Minibatch total loss at step 4719: 0.027153 | Best: 0.0243175\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.03000157  0.96999836  0.          1.        ]\n",
      " [ 0.03690774  0.96309227  0.          1.        ]]\n",
      "Minibatch total loss at step 4732: 0.064750 | Best: 0.0243175\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.98829913  0.01170081  1.          0.        ]\n",
      " [ 0.98732799  0.01267202  1.          0.        ]]\n",
      "Minibatch total loss at step 4745: 0.033161 | Best: 0.0243175\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[  8.23141456e-01   1.76858544e-01   1.00000000e+00   0.00000000e+00]\n",
      " [  2.83702548e-07   9.99999762e-01   0.00000000e+00   1.00000000e+00]]\n",
      "###-> Validation accuracy: 69.0 | Best: 80.5\n",
      "Minibatch total loss at step 4758: 0.028585 | Best: 0.0243175\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[  9.99526501e-01   4.73512308e-04   1.00000000e+00   0.00000000e+00]\n",
      " [  9.55419064e-01   4.45809476e-02   1.00000000e+00   0.00000000e+00]]\n",
      "Minibatch total loss at step 4771: 0.416113 | Best: 0.0243175\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.42985797  0.57014203  1.          0.        ]\n",
      " [ 0.98364532  0.01635462  1.          0.        ]]\n",
      "Minibatch total loss at step 4784: 0.396116 | Best: 0.0243175\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.11854248  0.88145751  0.          1.        ]\n",
      " [ 0.0081679   0.99183208  0.          1.        ]]\n",
      "Minibatch total loss at step 4797: 0.152237 | Best: 0.0243175\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[  2.34519085e-03   9.97654796e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  2.01012426e-05   9.99979854e-01   0.00000000e+00   1.00000000e+00]]\n",
      "###-> Validation accuracy: 78.0 | Best: 80.5\n",
      "Minibatch total loss at step 4810: 0.089285 | Best: 0.0243175\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[  9.87728000e-01   1.22720096e-02   1.00000000e+00   0.00000000e+00]\n",
      " [  5.53073769e-04   9.99446929e-01   0.00000000e+00   1.00000000e+00]]\n",
      "Minibatch total loss at step 4823: 0.015874 | Best: 0.0158743\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[  2.68911186e-04   9.99731123e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  4.78884394e-05   9.99952078e-01   0.00000000e+00   1.00000000e+00]]\n",
      "Minibatch total loss at step 4836: 0.155328 | Best: 0.0158743\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.66188711  0.33811289  1.          0.        ]\n",
      " [ 0.62337309  0.37662691  0.          1.        ]]\n",
      "Minibatch total loss at step 4849: 0.054430 | Best: 0.0158743\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[  9.99382138e-01   6.17851911e-04   1.00000000e+00   0.00000000e+00]\n",
      " [  3.02990491e-04   9.99697089e-01   0.00000000e+00   1.00000000e+00]]\n",
      "###-> Validation accuracy: 78.5 | Best: 80.5\n",
      "Minibatch total loss at step 4862: 0.080157 | Best: 0.0158743\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[  3.74935673e-12   1.00000000e+00   0.00000000e+00   1.00000000e+00]\n",
      " [  8.74239445e-01   1.25760525e-01   1.00000000e+00   0.00000000e+00]]\n",
      "Minibatch total loss at step 4875: 0.074266 | Best: 0.0158743\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[  4.20518889e-04   9.99579489e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  9.73305285e-01   2.66947094e-02   1.00000000e+00   0.00000000e+00]]\n",
      "Minibatch total loss at step 4888: 0.156061 | Best: 0.0158743\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.95936733  0.04063269  1.          0.        ]\n",
      " [ 0.91782373  0.08217625  1.          0.        ]]\n",
      "###-> Validation accuracy: 78.5 | Best: 80.5\n",
      "Minibatch total loss at step 4901: 0.071663 | Best: 0.0158743\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[  8.68334770e-01   1.31665200e-01   1.00000000e+00   0.00000000e+00]\n",
      " [  4.26721300e-07   9.99999523e-01   0.00000000e+00   1.00000000e+00]]\n",
      "Minibatch total loss at step 4914: 0.035903 | Best: 0.0158743\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.9940995   0.00590057  1.          0.        ]\n",
      " [ 0.00169718  0.99830282  0.          1.        ]]\n",
      "Minibatch total loss at step 4927: 0.410066 | Best: 0.0158743\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.50479913  0.49520087  1.          0.        ]\n",
      " [ 0.00837191  0.99162805  0.          1.        ]]\n",
      "Minibatch total loss at step 4940: 0.033597 | Best: 0.0158743\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.04970518  0.95029485  0.          1.        ]\n",
      " [ 0.15760992  0.84239006  0.          1.        ]]\n",
      "###-> Validation accuracy: 78.5 | Best: 80.5\n",
      "Minibatch total loss at step 4953: 0.119227 | Best: 0.0158743\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.97217631  0.02782366  1.          0.        ]\n",
      " [ 0.98103297  0.01896705  1.          0.        ]]\n",
      "Minibatch total loss at step 4966: 0.087878 | Best: 0.0158743\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[  2.12899163e-01   7.87100792e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  6.68600569e-06   9.99993324e-01   0.00000000e+00   1.00000000e+00]]\n",
      "Minibatch total loss at step 4979: 0.370793 | Best: 0.0158743\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.81814522  0.18185475  0.          1.        ]\n",
      " [ 0.00879446  0.99120551  0.          1.        ]]\n",
      "Minibatch total loss at step 4992: 0.137542 | Best: 0.0158743\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[  9.99038219e-01   9.61866172e-04   1.00000000e+00   0.00000000e+00]\n",
      " [  4.69910741e-01   5.30089319e-01   0.00000000e+00   1.00000000e+00]]\n",
      "###-> Validation accuracy: 76.0 | Best: 80.5\n",
      "Minibatch total loss at step 5005: 0.283301 | Best: 0.0158743\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[  3.02591231e-02   9.69740927e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  1.95483182e-04   9.99804556e-01   0.00000000e+00   1.00000000e+00]]\n",
      "Minibatch total loss at step 5018: 0.334246 | Best: 0.0158743\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[  9.75129485e-01   2.48704832e-02   1.00000000e+00   0.00000000e+00]\n",
      " [  1.42780254e-05   9.99985695e-01   0.00000000e+00   1.00000000e+00]]\n",
      "Minibatch total loss at step 5031: 0.237303 | Best: 0.0158743\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[  1.30675195e-04   9.99869347e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  9.96877909e-01   3.12203798e-03   1.00000000e+00   0.00000000e+00]]\n",
      "Minibatch total loss at step 5044: 0.084977 | Best: 0.0158743\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[  9.99906301e-01   9.37295408e-05   1.00000000e+00   0.00000000e+00]\n",
      " [  9.99454200e-01   5.45798161e-04   1.00000000e+00   0.00000000e+00]]\n",
      "###-> Validation accuracy: 51.5 | Best: 80.5\n",
      "Minibatch total loss at step 5057: 0.068093 | Best: 0.0158743\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[  3.09859548e-04   9.99690175e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  2.11498350e-01   7.88501620e-01   0.00000000e+00   1.00000000e+00]]\n",
      "Minibatch total loss at step 5070: 0.095823 | Best: 0.0158743\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.87708026  0.12291974  1.          0.        ]\n",
      " [ 0.87256795  0.12743203  1.          0.        ]]\n",
      "Minibatch total loss at step 5083: 0.226816 | Best: 0.0158743\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[  5.33880115e-01   4.66119856e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  1.25089991e-06   9.99998808e-01   0.00000000e+00   1.00000000e+00]]\n",
      "Minibatch total loss at step 5096: 0.004781 | Best: 0.00478117\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.99481189  0.00518813  1.          0.        ]\n",
      " [ 0.00635087  0.99364918  0.          1.        ]]\n",
      "###-> Validation accuracy: 55.0 | Best: 80.5\n",
      "Minibatch total loss at step 5109: 0.066936 | Best: 0.00478117\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[  9.89155471e-01   1.08444989e-02   1.00000000e+00   0.00000000e+00]\n",
      " [  3.52182862e-04   9.99647856e-01   0.00000000e+00   1.00000000e+00]]\n",
      "Minibatch total loss at step 5122: 0.045464 | Best: 0.00478117\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.03991763  0.96008235  0.          1.        ]\n",
      " [ 0.00209197  0.997908    0.          1.        ]]\n",
      "Minibatch total loss at step 5135: 0.119204 | Best: 0.00478117\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[  1.04253218e-02   9.89574730e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  1.28239469e-06   9.99998689e-01   0.00000000e+00   1.00000000e+00]]\n",
      "Minibatch total loss at step 5148: 0.130187 | Best: 0.00478117\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[  4.66740821e-05   9.99953270e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  3.84936866e-04   9.99615073e-01   0.00000000e+00   1.00000000e+00]]\n",
      "###-> Validation accuracy: 68.0 | Best: 80.5\n",
      "Minibatch total loss at step 5161: 0.078919 | Best: 0.00478117\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.00377146  0.99622858  0.          1.        ]\n",
      " [ 0.00447104  0.995529    0.          1.        ]]\n",
      "Minibatch total loss at step 5174: 0.139851 | Best: 0.00478117\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[  1.50666077e-04   9.99849319e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  7.18531251e-01   2.81468719e-01   1.00000000e+00   0.00000000e+00]]\n",
      "Minibatch total loss at step 5187: 0.033590 | Best: 0.00478117\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.95165902  0.04834097  1.          0.        ]\n",
      " [ 0.79354036  0.20645961  1.          0.        ]]\n",
      "Minibatch total loss at step 5200: 0.035066 | Best: 0.00478117\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.16384126  0.83615875  0.          1.        ]\n",
      " [ 0.00294519  0.99705482  0.          1.        ]]\n",
      "###-> Validation accuracy: 51.5 | Best: 80.5\n",
      "Minibatch total loss at step 5213: 0.175540 | Best: 0.00478117\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.00328046  0.99671954  0.          1.        ]\n",
      " [ 0.99658823  0.00341172  1.          0.        ]]\n",
      "Minibatch total loss at step 5226: 0.127741 | Best: 0.00478117\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[  9.99972105e-01   2.79519190e-05   1.00000000e+00   0.00000000e+00]\n",
      " [  9.99792755e-01   2.07211036e-04   1.00000000e+00   0.00000000e+00]]\n",
      "Minibatch total loss at step 5239: 0.007486 | Best: 0.00478117\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.0185619   0.98143816  0.          1.        ]\n",
      " [ 0.97343767  0.02656234  1.          0.        ]]\n",
      "###-> Validation accuracy: 58.0 | Best: 80.5\n",
      "Minibatch total loss at step 5252: 0.056325 | Best: 0.00478117\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[  1.15505347e-04   9.99884486e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  9.81816173e-01   1.81838516e-02   1.00000000e+00   0.00000000e+00]]\n",
      "Minibatch total loss at step 5265: 0.041070 | Best: 0.00478117\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.37970403  0.620296    0.          1.        ]\n",
      " [ 0.01933217  0.98066783  0.          1.        ]]\n",
      "Minibatch total loss at step 5278: 0.137803 | Best: 0.00478117\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.94611305  0.05388692  1.          0.        ]\n",
      " [ 0.99057615  0.00942387  1.          0.        ]]\n",
      "Minibatch total loss at step 5291: 0.194512 | Best: 0.00478117\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[  5.81469340e-06   9.99994159e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  4.41928767e-02   9.55807149e-01   0.00000000e+00   1.00000000e+00]]\n",
      "###-> Validation accuracy: 35.0 | Best: 80.5\n",
      "Minibatch total loss at step 5304: 0.028109 | Best: 0.00478117\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[  9.87870157e-01   1.21298628e-02   1.00000000e+00   0.00000000e+00]\n",
      " [  2.43493150e-06   9.99997616e-01   0.00000000e+00   1.00000000e+00]]\n",
      "Minibatch total loss at step 5317: 0.068838 | Best: 0.00478117\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.00950149  0.99049848  0.          1.        ]\n",
      " [ 0.00395758  0.99604249  0.          1.        ]]\n",
      "Minibatch total loss at step 5330: 0.160803 | Best: 0.00478117\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[  6.10158604e-04   9.99389768e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  9.41994369e-01   5.80056235e-02   1.00000000e+00   0.00000000e+00]]\n",
      "Minibatch total loss at step 5343: 0.079862 | Best: 0.00478117\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.87551886  0.12448111  1.          0.        ]\n",
      " [ 0.81510496  0.18489499  1.          0.        ]]\n",
      "###-> Validation accuracy: 54.5 | Best: 80.5\n",
      "Minibatch total loss at step 5356: 0.017678 | Best: 0.00478117\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[  9.62367561e-03   9.90376294e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  1.22520362e-07   9.99999881e-01   0.00000000e+00   1.00000000e+00]]\n",
      "Minibatch total loss at step 5369: 0.108946 | Best: 0.00478117\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.01172371  0.9882763   0.          1.        ]\n",
      " [ 0.02560713  0.97439289  0.          1.        ]]\n",
      "Minibatch total loss at step 5382: 0.065708 | Best: 0.00478117\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.99886608  0.001134    1.          0.        ]\n",
      " [ 0.5110268   0.48897317  1.          0.        ]]\n",
      "Minibatch total loss at step 5395: 0.011413 | Best: 0.00478117\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[  9.99233127e-01   7.66906422e-04   1.00000000e+00   0.00000000e+00]\n",
      " [  7.92833710e-09   1.00000000e+00   0.00000000e+00   1.00000000e+00]]\n",
      "###-> Validation accuracy: 51.0 | Best: 80.5\n",
      "Minibatch total loss at step 5408: 0.194071 | Best: 0.00478117\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[  9.99790251e-01   2.09793856e-04   1.00000000e+00   0.00000000e+00]\n",
      " [  7.37036462e-05   9.99926329e-01   0.00000000e+00   1.00000000e+00]]\n",
      "Minibatch total loss at step 5421: 0.184451 | Best: 0.00478117\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.9968155   0.00318457  1.          0.        ]\n",
      " [ 0.11937422  0.88062578  0.          1.        ]]\n",
      "Minibatch total loss at step 5434: 0.288000 | Best: 0.00478117\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.99880433  0.00119565  1.          0.        ]\n",
      " [ 0.00228771  0.99771225  0.          1.        ]]\n",
      "Minibatch total loss at step 5447: 0.048094 | Best: 0.00478117\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[  3.75823176e-04   9.99624133e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  9.70096648e-01   2.99033653e-02   1.00000000e+00   0.00000000e+00]]\n",
      "###-> Validation accuracy: 21.5 | Best: 80.5\n",
      "Minibatch total loss at step 5460: 0.064167 | Best: 0.00478117\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[  9.98033822e-01   1.96621986e-03   1.00000000e+00   0.00000000e+00]\n",
      " [  1.16674761e-04   9.99883294e-01   0.00000000e+00   1.00000000e+00]]\n",
      "Minibatch total loss at step 5473: 0.293846 | Best: 0.00478117\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[ 0.03258318  0.96741688  0.          1.        ]\n",
      " [ 0.01113634  0.98886371  0.          1.        ]]\n",
      "Minibatch total loss at step 5486: 0.371832 | Best: 0.00478117\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[  5.75510167e-05   9.99942422e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  3.77638787e-01   6.22361243e-01   0.00000000e+00   1.00000000e+00]]\n",
      "Minibatch total loss at step 5499: 0.039110 | Best: 0.00478117\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[  7.85727859e-01   2.14272216e-01   1.00000000e+00   0.00000000e+00]\n",
      " [  3.33121861e-04   9.99666929e-01   0.00000000e+00   1.00000000e+00]]\n",
      "###-> Validation accuracy: 25.0 | Best: 80.5\n",
      "Minibatch total loss at step 5512: 0.131256 | Best: 0.00478117\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[  9.99555647e-01   4.44336096e-04   1.00000000e+00   0.00000000e+00]\n",
      " [  1.53219011e-02   9.84678149e-01   0.00000000e+00   1.00000000e+00]]\n",
      "Minibatch total loss at step 5525: 0.046659 | Best: 0.00478117\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[  5.24351140e-04   9.99475658e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  8.18604417e-03   9.91814017e-01   0.00000000e+00   1.00000000e+00]]\n",
      "Minibatch total loss at step 5538: 0.079111 | Best: 0.00478117\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[  1.47701353e-02   9.85229909e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  9.99624372e-01   3.75599688e-04   1.00000000e+00   0.00000000e+00]]\n",
      "###-> Validation accuracy: 21.5 | Best: 80.5\n",
      "Minibatch total loss at step 5551: 0.235070 | Best: 0.00478117\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[  4.62028947e-05   9.99953747e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  1.09867260e-05   9.99989033e-01   0.00000000e+00   1.00000000e+00]]\n",
      "Minibatch total loss at step 5564: 0.025745 | Best: 0.00478117\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[  9.99868512e-01   1.31487148e-04   1.00000000e+00   0.00000000e+00]\n",
      " [  3.06168949e-04   9.99693871e-01   0.00000000e+00   1.00000000e+00]]\n",
      "Minibatch total loss at step 5577: 0.027326 | Best: 0.00478117\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.00913563  0.99086434  0.          1.        ]\n",
      " [ 0.04132716  0.95867288  0.          1.        ]]\n",
      "Minibatch total loss at step 5590: 0.036682 | Best: 0.00478117\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[  8.85374789e-07   9.99999166e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  5.30041107e-05   9.99946952e-01   0.00000000e+00   1.00000000e+00]]\n",
      "###-> Validation accuracy: 47.5 | Best: 80.5\n",
      "Minibatch total loss at step 5603: 0.085183 | Best: 0.00478117\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[  9.73934293e-01   2.60656774e-02   1.00000000e+00   0.00000000e+00]\n",
      " [  2.01606531e-06   9.99997973e-01   0.00000000e+00   1.00000000e+00]]\n",
      "Minibatch total loss at step 5616: 0.064697 | Best: 0.00478117\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[  9.99295950e-01   7.03979749e-04   1.00000000e+00   0.00000000e+00]\n",
      " [  9.97554839e-01   2.44516786e-03   1.00000000e+00   0.00000000e+00]]\n",
      "Minibatch total loss at step 5629: 0.043244 | Best: 0.00478117\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.21379383  0.78620619  0.          1.        ]\n",
      " [ 0.99898213  0.0010179   1.          0.        ]]\n",
      "Minibatch total loss at step 5642: 0.181020 | Best: 0.00478117\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.00497628  0.99502373  0.          1.        ]\n",
      " [ 0.00323998  0.99676007  0.          1.        ]]\n",
      "###-> Validation accuracy: 32.0 | Best: 80.5\n",
      "Minibatch total loss at step 5655: 0.132566 | Best: 0.00478117\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[  7.31333603e-06   9.99992728e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  9.94384408e-01   5.61562786e-03   1.00000000e+00   0.00000000e+00]]\n",
      "Minibatch total loss at step 5668: 0.139573 | Best: 0.00478117\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.00887647  0.99112356  0.          1.        ]\n",
      " [ 0.80536717  0.19463281  0.          1.        ]]\n",
      "Minibatch total loss at step 5681: 0.064360 | Best: 0.00478117\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.0640833   0.93591666  0.          1.        ]\n",
      " [ 0.15929857  0.84070146  0.          1.        ]]\n",
      "Minibatch total loss at step 5694: 0.075514 | Best: 0.00478117\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[  9.99998450e-01   1.50281971e-06   1.00000000e+00   0.00000000e+00]\n",
      " [  7.59835988e-02   9.24016416e-01   0.00000000e+00   1.00000000e+00]]\n",
      "###-> Validation accuracy: 40.5 | Best: 80.5\n",
      "Minibatch total loss at step 5707: 0.034402 | Best: 0.00478117\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[  1.93496086e-02   9.80650365e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  7.43850294e-12   1.00000000e+00   0.00000000e+00   1.00000000e+00]]\n",
      "Minibatch total loss at step 5720: 0.120762 | Best: 0.00478117\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[  3.02170607e-04   9.99697804e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  9.99994874e-01   5.12752922e-06   1.00000000e+00   0.00000000e+00]]\n",
      "Minibatch total loss at step 5733: 0.024395 | Best: 0.00478117\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[  1.81161182e-03   9.98188436e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  1.00000000e+00   3.19929114e-08   1.00000000e+00   0.00000000e+00]]\n",
      "Minibatch total loss at step 5746: 0.101539 | Best: 0.00478117\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[  1.99654505e-01   8.00345540e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  4.77638650e-06   9.99995232e-01   0.00000000e+00   1.00000000e+00]]\n",
      "###-> Validation accuracy: 38.5 | Best: 80.5\n",
      "Minibatch total loss at step 5759: 0.114407 | Best: 0.00478117\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.71744895  0.28255105  0.          1.        ]\n",
      " [ 0.97886842  0.02113151  1.          0.        ]]\n",
      "Minibatch total loss at step 5772: 0.012312 | Best: 0.00478117\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[  9.99924421e-01   7.55483125e-05   1.00000000e+00   0.00000000e+00]\n",
      " [  1.25732459e-02   9.87426758e-01   0.00000000e+00   1.00000000e+00]]\n",
      "Minibatch total loss at step 5785: 0.084387 | Best: 0.00478117\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.00805392  0.99194604  0.          1.        ]\n",
      " [ 0.02673755  0.97326243  0.          1.        ]]\n",
      "Minibatch total loss at step 5798: 0.040008 | Best: 0.00478117\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.97598529  0.02401471  1.          0.        ]\n",
      " [ 0.02253288  0.97746712  0.          1.        ]]\n",
      "###-> Validation accuracy: 51.5 | Best: 80.5\n",
      "Minibatch total loss at step 5811: 0.177286 | Best: 0.00478117\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[  9.72796624e-06   9.99990225e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  2.77199870e-04   9.99722779e-01   0.00000000e+00   1.00000000e+00]]\n",
      "Minibatch total loss at step 5824: 0.176084 | Best: 0.00478117\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.00529531  0.99470466  0.          1.        ]\n",
      " [ 0.00337233  0.99662763  0.          1.        ]]\n",
      "Minibatch total loss at step 5837: 0.012262 | Best: 0.00478117\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.00394721  0.99605274  0.          1.        ]\n",
      " [ 0.96712595  0.03287399  1.          0.        ]]\n",
      "Minibatch total loss at step 5850: 0.072877 | Best: 0.00478117\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.9973948   0.00260521  1.          0.        ]\n",
      " [ 0.05293558  0.9470644   0.          1.        ]]\n",
      "###-> Validation accuracy: 69.0 | Best: 80.5\n",
      "Minibatch total loss at step 5863: 0.304760 | Best: 0.00478117\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.36040184  0.63959819  0.          1.        ]\n",
      " [ 0.60318369  0.39681637  1.          0.        ]]\n",
      "Minibatch total loss at step 5876: 0.175630 | Best: 0.00478117\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[  2.90251255e-01   7.09748685e-01   1.00000000e+00   0.00000000e+00]\n",
      " [  3.04892637e-06   9.99996901e-01   0.00000000e+00   1.00000000e+00]]\n",
      "Minibatch total loss at step 5889: 0.055115 | Best: 0.00478117\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.95075995  0.04924008  1.          0.        ]\n",
      " [ 0.95140153  0.04859849  1.          0.        ]]\n",
      "###-> Validation accuracy: 39.0 | Best: 80.5\n",
      "Minibatch total loss at step 5902: 0.221074 | Best: 0.00478117\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[  9.99541283e-01   4.58803814e-04   1.00000000e+00   0.00000000e+00]\n",
      " [  9.84086692e-01   1.59132425e-02   1.00000000e+00   0.00000000e+00]]\n",
      "Minibatch total loss at step 5915: 0.022819 | Best: 0.00478117\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.98944622  0.01055374  1.          0.        ]\n",
      " [ 0.00128682  0.99871325  0.          1.        ]]\n",
      "Minibatch total loss at step 5928: 0.461909 | Best: 0.00478117\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.8450008   0.15499917  1.          0.        ]\n",
      " [ 0.9965089   0.00349104  1.          0.        ]]\n",
      "Minibatch total loss at step 5941: 0.010930 | Best: 0.00478117\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[  7.31108245e-04   9.99268949e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  2.45750882e-02   9.75424886e-01   0.00000000e+00   1.00000000e+00]]\n",
      "###-> Validation accuracy: 21.5 | Best: 80.5\n",
      "Minibatch total loss at step 5954: 0.137519 | Best: 0.00478117\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.00308453  0.99691546  0.          1.        ]\n",
      " [ 0.00107209  0.99892789  0.          1.        ]]\n",
      "Minibatch total loss at step 5967: 0.200923 | Best: 0.00478117\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.93300825  0.06699175  1.          0.        ]\n",
      " [ 0.00499908  0.9950009   0.          1.        ]]\n",
      "Minibatch total loss at step 5980: 0.187711 | Best: 0.00478117\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.31855676  0.68144327  0.          1.        ]\n",
      " [ 0.00163228  0.99836773  0.          1.        ]]\n",
      "Minibatch total loss at step 5993: 0.015201 | Best: 0.00478117\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.99434775  0.00565223  1.          0.        ]\n",
      " [ 0.96201092  0.03798906  1.          0.        ]]\n",
      "###-> Validation accuracy: 21.5 | Best: 80.5\n",
      "Minibatch total loss at step 6006: 0.060392 | Best: 0.00478117\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[  1.45050958e-01   8.54949057e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  1.33619265e-04   9.99866366e-01   0.00000000e+00   1.00000000e+00]]\n",
      "Minibatch total loss at step 6019: 0.080490 | Best: 0.00478117\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[  2.01340517e-05   9.99979854e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  1.48460383e-10   1.00000000e+00   0.00000000e+00   1.00000000e+00]]\n",
      "Minibatch total loss at step 6032: 0.201690 | Best: 0.00478117\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.08029998  0.91970003  0.          1.        ]\n",
      " [ 0.00264448  0.99735558  0.          1.        ]]\n",
      "Minibatch total loss at step 6045: 0.211271 | Best: 0.00478117\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[  8.96349728e-01   1.03650279e-01   1.00000000e+00   0.00000000e+00]\n",
      " [  9.99889493e-01   1.10478250e-04   1.00000000e+00   0.00000000e+00]]\n",
      "###-> Validation accuracy: 21.5 | Best: 80.5\n",
      "Minibatch total loss at step 6058: 0.029756 | Best: 0.00478117\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.00300268  0.99699736  0.          1.        ]\n",
      " [ 0.99432832  0.00567175  1.          0.        ]]\n",
      "Minibatch total loss at step 6071: 0.093800 | Best: 0.00478117\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.03568228  0.9643178   0.          1.        ]\n",
      " [ 0.97882092  0.02117901  1.          0.        ]]\n",
      "Minibatch total loss at step 6084: 0.070042 | Best: 0.00478117\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[  9.99572337e-01   4.27624298e-04   1.00000000e+00   0.00000000e+00]\n",
      " [  9.47606168e-06   9.99990582e-01   0.00000000e+00   1.00000000e+00]]\n",
      "Minibatch total loss at step 6097: 0.034769 | Best: 0.00478117\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[  3.05914413e-03   9.96940792e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  5.48579264e-05   9.99945164e-01   0.00000000e+00   1.00000000e+00]]\n",
      "###-> Validation accuracy: 21.5 | Best: 80.5\n",
      "Minibatch total loss at step 6110: 0.240012 | Best: 0.00478117\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.92689651  0.07310347  0.          1.        ]\n",
      " [ 0.03917797  0.96082211  0.          1.        ]]\n",
      "Minibatch total loss at step 6123: 0.141019 | Best: 0.00478117\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[  9.79036510e-01   2.09634975e-02   1.00000000e+00   0.00000000e+00]\n",
      " [  8.41302317e-05   9.99915838e-01   0.00000000e+00   1.00000000e+00]]\n",
      "Minibatch total loss at step 6136: 0.038377 | Best: 0.00478117\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.32394555  0.67605448  0.          1.        ]\n",
      " [ 0.00496706  0.99503291  0.          1.        ]]\n",
      "Minibatch total loss at step 6149: 0.275754 | Best: 0.00478117\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[  5.84505033e-04   9.99415517e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  3.66122454e-01   6.33877516e-01   1.00000000e+00   0.00000000e+00]]\n",
      "###-> Validation accuracy: 21.5 | Best: 80.5\n",
      "Minibatch total loss at step 6162: 0.137796 | Best: 0.00478117\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[  8.57313513e-04   9.99142647e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  2.49605660e-06   9.99997497e-01   0.00000000e+00   1.00000000e+00]]\n",
      "Minibatch total loss at step 6175: 0.144530 | Best: 0.00478117\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[  6.10568153e-04   9.99389410e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  9.61825132e-01   3.81748676e-02   1.00000000e+00   0.00000000e+00]]\n",
      "Minibatch total loss at step 6188: 0.098355 | Best: 0.00478117\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[  3.87656655e-07   9.99999642e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  9.99994397e-01   5.57478643e-06   1.00000000e+00   0.00000000e+00]]\n",
      "###-> Validation accuracy: 21.5 | Best: 80.5\n",
      "Minibatch total loss at step 6201: 0.043907 | Best: 0.00478117\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[  8.04315787e-04   9.99195635e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  4.25177335e-04   9.99574840e-01   0.00000000e+00   1.00000000e+00]]\n",
      "Minibatch total loss at step 6214: 0.078077 | Best: 0.00478117\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[  9.99922037e-01   7.79149996e-05   1.00000000e+00   0.00000000e+00]\n",
      " [  9.99994993e-01   5.06618790e-06   1.00000000e+00   0.00000000e+00]]\n",
      "Minibatch total loss at step 6227: 0.175613 | Best: 0.00478117\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[  9.99931335e-01   6.86330241e-05   1.00000000e+00   0.00000000e+00]\n",
      " [  1.17192802e-03   9.98828113e-01   0.00000000e+00   1.00000000e+00]]\n",
      "Minibatch total loss at step 6240: 0.022960 | Best: 0.00478117\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.97651166  0.02348834  1.          0.        ]\n",
      " [ 0.04714697  0.95285296  0.          1.        ]]\n",
      "###-> Validation accuracy: 21.5 | Best: 80.5\n",
      "Minibatch total loss at step 6253: 0.078015 | Best: 0.00478117\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[  6.46380795e-06   9.99993563e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  9.98624682e-01   1.37533818e-03   1.00000000e+00   0.00000000e+00]]\n",
      "Minibatch total loss at step 6266: 0.015973 | Best: 0.00478117\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[  4.63749003e-03   9.95362520e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  2.78751832e-04   9.99721229e-01   0.00000000e+00   1.00000000e+00]]\n",
      "Minibatch total loss at step 6279: 0.032935 | Best: 0.00478117\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.9985708   0.00142919  1.          0.        ]\n",
      " [ 0.83900392  0.16099608  1.          0.        ]]\n",
      "Minibatch total loss at step 6292: 0.011393 | Best: 0.00478117\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[  9.92233992e-01   7.76601676e-03   1.00000000e+00   0.00000000e+00]\n",
      " [  6.16297902e-10   1.00000000e+00   0.00000000e+00   1.00000000e+00]]\n",
      "###-> Validation accuracy: 21.5 | Best: 80.5\n",
      "Minibatch total loss at step 6305: 0.005563 | Best: 0.00478117\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.99872774  0.00127222  1.          0.        ]\n",
      " [ 0.99757701  0.002423    1.          0.        ]]\n",
      "Minibatch total loss at step 6318: 0.031738 | Best: 0.00478117\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[  8.57621968e-01   1.42378002e-01   1.00000000e+00   0.00000000e+00]\n",
      " [  9.99531269e-01   4.68743208e-04   1.00000000e+00   0.00000000e+00]]\n",
      "Minibatch total loss at step 6331: 0.015642 | Best: 0.00478117\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.00381268  0.99618727  0.          1.        ]\n",
      " [ 0.0246008   0.97539926  0.          1.        ]]\n",
      "Minibatch total loss at step 6344: 0.112126 | Best: 0.00478117\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[  7.22301662e-01   2.77698308e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  1.73852342e-04   9.99826133e-01   0.00000000e+00   1.00000000e+00]]\n",
      "###-> Validation accuracy: 21.5 | Best: 80.5\n",
      "Minibatch total loss at step 6357: 0.041497 | Best: 0.00478117\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[  9.98905063e-01   1.09494128e-03   1.00000000e+00   0.00000000e+00]\n",
      " [  2.87037201e-05   9.99971271e-01   0.00000000e+00   1.00000000e+00]]\n",
      "Minibatch total loss at step 6370: 0.023231 | Best: 0.00478117\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[  5.73845318e-05   9.99942660e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  2.31960975e-03   9.97680306e-01   0.00000000e+00   1.00000000e+00]]\n",
      "Minibatch total loss at step 6383: 0.063640 | Best: 0.00478117\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.7380718   0.26192817  1.          0.        ]\n",
      " [ 0.13303971  0.86696029  0.          1.        ]]\n",
      "Minibatch total loss at step 6396: 0.011690 | Best: 0.00478117\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[  9.99685049e-01   3.14988254e-04   1.00000000e+00   0.00000000e+00]\n",
      " [  2.23940115e-05   9.99977589e-01   0.00000000e+00   1.00000000e+00]]\n",
      "###-> Validation accuracy: 21.5 | Best: 80.5\n",
      "Minibatch total loss at step 6409: 0.261147 | Best: 0.00478117\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[  2.62132122e-11   1.00000000e+00   0.00000000e+00   1.00000000e+00]\n",
      " [  9.97453868e-01   2.54614372e-03   1.00000000e+00   0.00000000e+00]]\n",
      "Minibatch total loss at step 6422: 0.070574 | Best: 0.00478117\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.01303907  0.98696089  0.          1.        ]\n",
      " [ 0.98017943  0.01982056  1.          0.        ]]\n",
      "Minibatch total loss at step 6435: 0.170001 | Best: 0.00478117\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.82857841  0.17142162  1.          0.        ]\n",
      " [ 0.9962852   0.0037148   1.          0.        ]]\n",
      "Minibatch total loss at step 6448: 0.022839 | Best: 0.00478117\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[  9.99915838e-01   8.42117879e-05   1.00000000e+00   0.00000000e+00]\n",
      " [  1.09038240e-06   9.99998927e-01   0.00000000e+00   1.00000000e+00]]\n",
      "###-> Validation accuracy: 21.5 | Best: 80.5\n",
      "Minibatch total loss at step 6461: 0.039278 | Best: 0.00478117\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.94488811  0.05511184  1.          0.        ]\n",
      " [ 0.00179938  0.99820065  0.          1.        ]]\n",
      "Minibatch total loss at step 6474: 0.258038 | Best: 0.00478117\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[  9.74066377e-01   2.59336531e-02   1.00000000e+00   0.00000000e+00]\n",
      " [  8.09901176e-05   9.99919057e-01   0.00000000e+00   1.00000000e+00]]\n",
      "Minibatch total loss at step 6487: 0.012728 | Best: 0.00478117\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[  4.11412520e-06   9.99995828e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  4.80697863e-02   9.51930225e-01   0.00000000e+00   1.00000000e+00]]\n",
      "Minibatch total loss at step 6500: 0.021454 | Best: 0.00478117\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[  9.95618641e-01   4.38128831e-03   1.00000000e+00   0.00000000e+00]\n",
      " [  9.99828458e-01   1.71474865e-04   1.00000000e+00   0.00000000e+00]]\n",
      "###-> Validation accuracy: 21.5 | Best: 80.5\n",
      "Minibatch total loss at step 6513: 0.051402 | Best: 0.00478117\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[  1.33691006e-04   9.99866247e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  1.60393810e-07   9.99999881e-01   0.00000000e+00   1.00000000e+00]]\n",
      "Minibatch total loss at step 6526: 0.062637 | Best: 0.00478117\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[  3.90230902e-02   9.60976958e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  5.96463869e-05   9.99940395e-01   0.00000000e+00   1.00000000e+00]]\n",
      "Minibatch total loss at step 6539: 0.023723 | Best: 0.00478117\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[  9.99936104e-01   6.39376376e-05   1.00000000e+00   0.00000000e+00]\n",
      " [  1.48966847e-05   9.99985099e-01   0.00000000e+00   1.00000000e+00]]\n",
      "###-> Validation accuracy: 21.5 | Best: 80.5\n",
      "Minibatch total loss at step 6552: 0.012588 | Best: 0.00478117\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[  3.55736192e-05   9.99964476e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  1.85120199e-03   9.98148799e-01   0.00000000e+00   1.00000000e+00]]\n",
      "Minibatch total loss at step 6565: 0.015250 | Best: 0.00478117\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[  9.99642253e-01   3.57724173e-04   1.00000000e+00   0.00000000e+00]\n",
      " [  3.09891948e-06   9.99996901e-01   0.00000000e+00   1.00000000e+00]]\n",
      "Minibatch total loss at step 6578: 0.009522 | Best: 0.00478117\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[  3.35700549e-02   9.66429949e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  9.99217391e-01   7.82620569e-04   1.00000000e+00   0.00000000e+00]]\n",
      "Minibatch total loss at step 6591: 0.089622 | Best: 0.00478117\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[  9.99978185e-01   2.17897341e-05   1.00000000e+00   0.00000000e+00]\n",
      " [  9.62601244e-01   3.73987108e-02   1.00000000e+00   0.00000000e+00]]\n",
      "###-> Validation accuracy: 21.5 | Best: 80.5\n",
      "Minibatch total loss at step 6604: 0.004474 | Best: 0.00447359\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[  2.96828570e-04   9.99703109e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  4.31296369e-03   9.95687068e-01   0.00000000e+00   1.00000000e+00]]\n",
      "Minibatch total loss at step 6617: 0.021224 | Best: 0.00447359\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.95640606  0.04359398  1.          0.        ]\n",
      " [ 0.87673837  0.12326161  1.          0.        ]]\n",
      "Minibatch total loss at step 6630: 0.337152 | Best: 0.00447359\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[  4.48635638e-01   5.51364422e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  2.57424603e-07   9.99999762e-01   0.00000000e+00   1.00000000e+00]]\n",
      "Minibatch total loss at step 6643: 0.052654 | Best: 0.00447359\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[  9.99539971e-01   4.60008509e-04   1.00000000e+00   0.00000000e+00]\n",
      " [  3.22093765e-05   9.99967813e-01   0.00000000e+00   1.00000000e+00]]\n",
      "###-> Validation accuracy: 21.5 | Best: 80.5\n",
      "Minibatch total loss at step 6656: 0.006904 | Best: 0.00447359\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[  9.99945998e-01   5.39722096e-05   1.00000000e+00   0.00000000e+00]\n",
      " [  2.25694547e-03   9.97743130e-01   0.00000000e+00   1.00000000e+00]]\n",
      "Minibatch total loss at step 6669: 0.101181 | Best: 0.00447359\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[  2.13258713e-03   9.97867346e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  4.83286480e-04   9.99516726e-01   0.00000000e+00   1.00000000e+00]]\n",
      "Minibatch total loss at step 6682: 0.016036 | Best: 0.00447359\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[  9.83966165e-04   9.99016047e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  1.03369806e-10   1.00000000e+00   0.00000000e+00   1.00000000e+00]]\n",
      "Minibatch total loss at step 6695: 0.052633 | Best: 0.00447359\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[  1.43961530e-04   9.99855995e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  7.11736362e-03   9.92882609e-01   0.00000000e+00   1.00000000e+00]]\n",
      "###-> Validation accuracy: 21.5 | Best: 80.5\n",
      "Minibatch total loss at step 6708: 0.072528 | Best: 0.00447359\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[  2.74728482e-05   9.99972582e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  4.29712440e-04   9.99570310e-01   0.00000000e+00   1.00000000e+00]]\n",
      "Minibatch total loss at step 6721: 0.340726 | Best: 0.00447359\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[  6.32745102e-02   9.36725497e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  9.99825537e-01   1.74468456e-04   1.00000000e+00   0.00000000e+00]]\n",
      "Minibatch total loss at step 6734: 0.060941 | Best: 0.00447359\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.9914453   0.00855466  1.          0.        ]\n",
      " [ 0.89459181  0.10540822  1.          0.        ]]\n",
      "Minibatch total loss at step 6747: 0.343360 | Best: 0.00447359\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[  7.79359616e-05   9.99922037e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  5.03493957e-02   9.49650645e-01   0.00000000e+00   1.00000000e+00]]\n",
      "###-> Validation accuracy: 21.5 | Best: 80.5\n",
      "Minibatch total loss at step 6760: 0.254993 | Best: 0.00447359\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[  2.41827845e-04   9.99758184e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  9.99634385e-01   3.65567859e-04   1.00000000e+00   0.00000000e+00]]\n",
      "Minibatch total loss at step 6773: 0.272119 | Best: 0.00447359\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.07811617  0.92188382  1.          0.        ]\n",
      " [ 0.54245174  0.45754823  1.          0.        ]]\n",
      "Minibatch total loss at step 6786: 0.037720 | Best: 0.00447359\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[  8.94265773e-04   9.99105752e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  8.07510138e-01   1.92489922e-01   1.00000000e+00   0.00000000e+00]]\n",
      "Minibatch total loss at step 6799: 0.695829 | Best: 0.00447359\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[  1.59524888e-07   9.99999881e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  9.99948502e-01   5.15315733e-05   1.00000000e+00   0.00000000e+00]]\n",
      "###-> Validation accuracy: 21.5 | Best: 80.5\n",
      "Minibatch total loss at step 6812: 0.152268 | Best: 0.00447359\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.01758044  0.98241961  0.          1.        ]\n",
      " [ 0.0060434   0.99395657  0.          1.        ]]\n",
      "Minibatch total loss at step 6825: 0.106963 | Best: 0.00447359\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.95480907  0.04519089  1.          0.        ]\n",
      " [ 0.99831188  0.00168808  1.          0.        ]]\n",
      "Minibatch total loss at step 6838: 0.038131 | Best: 0.00447359\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[  2.52197623e-08   1.00000000e+00   0.00000000e+00   1.00000000e+00]\n",
      " [  3.97713826e-04   9.99602258e-01   0.00000000e+00   1.00000000e+00]]\n",
      "###-> Validation accuracy: 21.5 | Best: 80.5\n",
      "Minibatch total loss at step 6851: 0.099158 | Best: 0.00447359\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.5790363   0.4209637   1.          0.        ]\n",
      " [ 0.01353537  0.98646462  0.          1.        ]]\n",
      "Minibatch total loss at step 6864: 0.023196 | Best: 0.00447359\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[  1.35376642e-03   9.98646200e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  1.43494326e-04   9.99856472e-01   0.00000000e+00   1.00000000e+00]]\n",
      "Minibatch total loss at step 6877: 0.160240 | Best: 0.00447359\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.00755617  0.99244386  0.          1.        ]\n",
      " [ 0.80127299  0.198727    1.          0.        ]]\n",
      "Minibatch total loss at step 6890: 0.004593 | Best: 0.00447359\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.99554563  0.00445431  1.          0.        ]\n",
      " [ 0.9963876   0.00361243  1.          0.        ]]\n",
      "###-> Validation accuracy: 21.5 | Best: 80.5\n",
      "Minibatch total loss at step 6903: 0.162833 | Best: 0.00447359\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[  3.56210582e-02   9.64378953e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  1.23240989e-05   9.99987721e-01   0.00000000e+00   1.00000000e+00]]\n",
      "Minibatch total loss at step 6916: 0.172869 | Best: 0.00447359\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[  1.43713296e-05   9.99985576e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  4.91179675e-02   9.50882018e-01   0.00000000e+00   1.00000000e+00]]\n",
      "Minibatch total loss at step 6929: 0.172913 | Best: 0.00447359\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[  9.79398727e-01   2.06013154e-02   1.00000000e+00   0.00000000e+00]\n",
      " [  9.99867439e-01   1.32504370e-04   1.00000000e+00   0.00000000e+00]]\n",
      "Minibatch total loss at step 6942: 0.084230 | Best: 0.00447359\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.98416936  0.01583058  1.          0.        ]\n",
      " [ 0.00245406  0.9975459   0.          1.        ]]\n",
      "###-> Validation accuracy: 21.5 | Best: 80.5\n",
      "Minibatch total loss at step 6955: 0.270011 | Best: 0.00447359\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[  9.98395264e-01   1.60467124e-03   1.00000000e+00   0.00000000e+00]\n",
      " [  2.18142624e-04   9.99781907e-01   0.00000000e+00   1.00000000e+00]]\n",
      "Minibatch total loss at step 6968: 0.096474 | Best: 0.00447359\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.96227676  0.03772328  1.          0.        ]\n",
      " [ 0.00136809  0.99863189  0.          1.        ]]\n",
      "Minibatch total loss at step 6981: 0.073987 | Best: 0.00447359\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.99292809  0.00707187  1.          0.        ]\n",
      " [ 0.03157436  0.96842557  0.          1.        ]]\n",
      "Minibatch total loss at step 6994: 0.064778 | Best: 0.00447359\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[  2.15357232e-07   9.99999762e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  9.99999404e-01   5.77701883e-07   1.00000000e+00   0.00000000e+00]]\n",
      "###-> Validation accuracy: 21.5 | Best: 80.5\n",
      "Minibatch total loss at step 7007: 0.083749 | Best: 0.00447359\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.99457932  0.00542065  1.          0.        ]\n",
      " [ 0.48528472  0.51471531  0.          1.        ]]\n",
      "Minibatch total loss at step 7020: 0.221267 | Best: 0.00447359\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.00370596  0.99629396  0.          1.        ]\n",
      " [ 0.29026058  0.70973945  0.          1.        ]]\n",
      "Minibatch total loss at step 7033: 0.011117 | Best: 0.00447359\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[  9.18574817e-03   9.90814269e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  9.66132575e-06   9.99990344e-01   0.00000000e+00   1.00000000e+00]]\n",
      "Minibatch total loss at step 7046: 0.037931 | Best: 0.00447359\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[  9.93663669e-01   6.33631507e-03   1.00000000e+00   0.00000000e+00]\n",
      " [  4.11317924e-05   9.99958873e-01   0.00000000e+00   1.00000000e+00]]\n",
      "###-> Validation accuracy: 21.5 | Best: 80.5\n",
      "Minibatch total loss at step 7059: 0.073344 | Best: 0.00447359\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[  9.92342770e-01   7.65716657e-03   1.00000000e+00   0.00000000e+00]\n",
      " [  6.81215024e-05   9.99931931e-01   0.00000000e+00   1.00000000e+00]]\n",
      "Minibatch total loss at step 7072: 0.131329 | Best: 0.00447359\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[  8.70085903e-04   9.99129951e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  5.48210800e-01   4.51789230e-01   0.00000000e+00   1.00000000e+00]]\n",
      "Minibatch total loss at step 7085: 0.289299 | Best: 0.00447359\n",
      "Minibatch accuracy: 81.25\n",
      "Predictions | Labels:\n",
      " [[  1.45963713e-04   9.99853969e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  9.99934077e-01   6.58760691e-05   1.00000000e+00   0.00000000e+00]]\n",
      "Minibatch total loss at step 7098: 0.098729 | Best: 0.00447359\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[  2.53012720e-02   9.74698722e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  6.95964764e-06   9.99993086e-01   0.00000000e+00   1.00000000e+00]]\n",
      "###-> Validation accuracy: 27.0 | Best: 80.5\n",
      "Minibatch total loss at step 7111: 0.125916 | Best: 0.00447359\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[  9.99944568e-01   5.54544386e-05   1.00000000e+00   0.00000000e+00]\n",
      " [  2.00635932e-05   9.99979973e-01   0.00000000e+00   1.00000000e+00]]\n",
      "Minibatch total loss at step 7124: 0.021948 | Best: 0.00447359\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.00469922  0.99530077  0.          1.        ]\n",
      " [ 0.0838684   0.91613156  0.          1.        ]]\n",
      "Minibatch total loss at step 7137: 0.026878 | Best: 0.00447359\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[  6.97557567e-08   9.99999881e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  5.62055875e-03   9.94379461e-01   0.00000000e+00   1.00000000e+00]]\n",
      "Minibatch total loss at step 7150: 0.231944 | Best: 0.00447359\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[  9.94893670e-01   5.10631595e-03   1.00000000e+00   0.00000000e+00]\n",
      " [  5.19389346e-07   9.99999523e-01   0.00000000e+00   1.00000000e+00]]\n",
      "###-> Validation accuracy: 23.0 | Best: 80.5\n",
      "Minibatch total loss at step 7163: 0.003582 | Best: 0.00358214\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[  9.99877214e-01   1.22761761e-04   1.00000000e+00   0.00000000e+00]\n",
      " [  9.84651804e-01   1.53481802e-02   1.00000000e+00   0.00000000e+00]]\n",
      "Minibatch total loss at step 7176: 0.047950 | Best: 0.00358214\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[  1.80108622e-02   9.81989145e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  9.99924779e-01   7.52403866e-05   1.00000000e+00   0.00000000e+00]]\n",
      "Minibatch total loss at step 7189: 0.095099 | Best: 0.00358214\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.00547913  0.99452078  0.          1.        ]\n",
      " [ 0.005363    0.99463695  0.          1.        ]]\n",
      "###-> Validation accuracy: 22.0 | Best: 80.5\n",
      "Minibatch total loss at step 7202: 0.012716 | Best: 0.00358214\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[  4.26749684e-05   9.99957323e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  9.72158849e-01   2.78411452e-02   1.00000000e+00   0.00000000e+00]]\n",
      "Minibatch total loss at step 7215: 0.100631 | Best: 0.00358214\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.01698976  0.98301017  0.          1.        ]\n",
      " [ 0.70870918  0.29129079  0.          1.        ]]\n",
      "Minibatch total loss at step 7228: 0.009430 | Best: 0.00358214\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[  4.56640089e-04   9.99543369e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  9.74963419e-03   9.90250349e-01   0.00000000e+00   1.00000000e+00]]\n",
      "Minibatch total loss at step 7241: 0.012321 | Best: 0.00358214\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[  9.99887228e-01   1.12700931e-04   1.00000000e+00   0.00000000e+00]\n",
      " [  7.87043246e-04   9.99212980e-01   0.00000000e+00   1.00000000e+00]]\n",
      "###-> Validation accuracy: 21.5 | Best: 80.5\n",
      "Minibatch total loss at step 7254: 0.082760 | Best: 0.00358214\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[  3.95629974e-03   9.96043682e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  1.33496651e-11   1.00000000e+00   0.00000000e+00   1.00000000e+00]]\n",
      "Minibatch total loss at step 7267: 0.047162 | Best: 0.00358214\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[  1.84651883e-03   9.98153508e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  9.99931097e-01   6.89412045e-05   1.00000000e+00   0.00000000e+00]]\n",
      "Minibatch total loss at step 7280: 0.156948 | Best: 0.00358214\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[  2.39395886e-04   9.99760568e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  9.99992728e-01   7.30331340e-06   1.00000000e+00   0.00000000e+00]]\n",
      "Minibatch total loss at step 7293: 0.009027 | Best: 0.00358214\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[  1.01580843e-03   9.98984158e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  9.37884557e-04   9.99062121e-01   0.00000000e+00   1.00000000e+00]]\n",
      "###-> Validation accuracy: 21.5 | Best: 80.5\n",
      "Minibatch total loss at step 7306: 0.060260 | Best: 0.00358214\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.07139113  0.92860889  0.          1.        ]\n",
      " [ 0.99577701  0.00422297  1.          0.        ]]\n",
      "Minibatch total loss at step 7319: 0.007262 | Best: 0.00358214\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[  9.99976993e-01   2.30323258e-05   1.00000000e+00   0.00000000e+00]\n",
      " [  6.87725318e-04   9.99312282e-01   0.00000000e+00   1.00000000e+00]]\n",
      "Minibatch total loss at step 7332: 0.002510 | Best: 0.00250975\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[  1.46286693e-04   9.99853730e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  3.25157521e-06   9.99996781e-01   0.00000000e+00   1.00000000e+00]]\n",
      "Minibatch total loss at step 7345: 0.222286 | Best: 0.00250975\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[  9.99992132e-01   7.82938332e-06   1.00000000e+00   0.00000000e+00]\n",
      " [  7.89510857e-09   1.00000000e+00   0.00000000e+00   1.00000000e+00]]\n",
      "###-> Validation accuracy: 21.5 | Best: 80.5\n",
      "Minibatch total loss at step 7358: 0.012751 | Best: 0.00250975\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[  9.01993248e-04   9.99098063e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  2.16187891e-05   9.99978423e-01   0.00000000e+00   1.00000000e+00]]\n",
      "Minibatch total loss at step 7371: 0.040747 | Best: 0.00250975\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[  4.89352853e-04   9.99510646e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  5.63904736e-03   9.94360983e-01   0.00000000e+00   1.00000000e+00]]\n",
      "Minibatch total loss at step 7384: 0.071733 | Best: 0.00250975\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.53123999  0.46875995  0.          1.        ]\n",
      " [ 0.86555886  0.13444115  1.          0.        ]]\n",
      "Minibatch total loss at step 7397: 0.015668 | Best: 0.00250975\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[  9.99975085e-01   2.49136210e-05   1.00000000e+00   0.00000000e+00]\n",
      " [  1.13601338e-04   9.99886394e-01   0.00000000e+00   1.00000000e+00]]\n",
      "###-> Validation accuracy: 21.5 | Best: 80.5\n",
      "Minibatch total loss at step 7410: 0.206144 | Best: 0.00250975\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.00162412  0.99837589  0.          1.        ]\n",
      " [ 0.99778181  0.00221816  1.          0.        ]]\n",
      "Minibatch total loss at step 7423: 0.170365 | Best: 0.00250975\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[  9.99998927e-01   1.09935479e-06   1.00000000e+00   0.00000000e+00]\n",
      " [  9.81269186e-05   9.99901891e-01   0.00000000e+00   1.00000000e+00]]\n",
      "Minibatch total loss at step 7436: 0.195599 | Best: 0.00250975\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.99713409  0.00286585  1.          0.        ]\n",
      " [ 0.99416542  0.00583458  1.          0.        ]]\n",
      "Minibatch total loss at step 7449: 0.084656 | Best: 0.00250975\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.98754555  0.0124544   1.          0.        ]\n",
      " [ 0.86398292  0.13601711  1.          0.        ]]\n",
      "###-> Validation accuracy: 21.5 | Best: 80.5\n",
      "Minibatch total loss at step 7462: 0.023312 | Best: 0.00250975\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[  9.83449876e-01   1.65501814e-02   1.00000000e+00   0.00000000e+00]\n",
      " [  2.07982215e-04   9.99792039e-01   0.00000000e+00   1.00000000e+00]]\n",
      "Minibatch total loss at step 7475: 0.049534 | Best: 0.00250975\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.540636    0.45936397  1.          0.        ]\n",
      " [ 0.99864739  0.00135262  1.          0.        ]]\n",
      "Minibatch total loss at step 7488: 0.021459 | Best: 0.00250975\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[  2.49944910e-01   7.50055015e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  3.27118760e-05   9.99967337e-01   0.00000000e+00   1.00000000e+00]]\n",
      "###-> Validation accuracy: 21.5 | Best: 80.5\n",
      "Minibatch total loss at step 7501: 0.036448 | Best: 0.00250975\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[  2.19454087e-05   9.99978065e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  2.12361658e-04   9.99787629e-01   0.00000000e+00   1.00000000e+00]]\n",
      "Minibatch total loss at step 7514: 0.011568 | Best: 0.00250975\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.98672152  0.01327844  1.          0.        ]\n",
      " [ 0.0622336   0.93776637  0.          1.        ]]\n",
      "Minibatch total loss at step 7527: 0.037142 | Best: 0.00250975\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[ 0.00173984  0.9982602   0.          1.        ]\n",
      " [ 0.24420999  0.75578994  0.          1.        ]]\n",
      "Minibatch total loss at step 7540: 0.033117 | Best: 0.00250975\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[  9.99998569e-01   1.47298397e-06   1.00000000e+00   0.00000000e+00]\n",
      " [  9.97921050e-01   2.07894063e-03   1.00000000e+00   0.00000000e+00]]\n",
      "###-> Validation accuracy: 21.5 | Best: 80.5\n",
      "Minibatch total loss at step 7553: 0.074833 | Best: 0.00250975\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.64456969  0.35543031  0.          1.        ]\n",
      " [ 0.0017227   0.99827731  0.          1.        ]]\n",
      "Minibatch total loss at step 7566: 0.047673 | Best: 0.00250975\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[  1.84965611e-05   9.99981523e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  1.08840326e-10   1.00000000e+00   0.00000000e+00   1.00000000e+00]]\n",
      "Minibatch total loss at step 7579: 0.067527 | Best: 0.00250975\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[  3.30607980e-01   6.69392049e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  3.17716214e-04   9.99682307e-01   0.00000000e+00   1.00000000e+00]]\n",
      "Minibatch total loss at step 7592: 0.027371 | Best: 0.00250975\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[  9.17361915e-01   8.26380625e-02   1.00000000e+00   0.00000000e+00]\n",
      " [  9.99704778e-01   2.95248319e-04   1.00000000e+00   0.00000000e+00]]\n",
      "###-> Validation accuracy: 21.5 | Best: 80.5\n",
      "Minibatch total loss at step 7605: 0.372083 | Best: 0.00250975\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.00537224  0.99462777  0.          1.        ]\n",
      " [ 0.06038132  0.93961865  1.          0.        ]]\n",
      "Minibatch total loss at step 7618: 0.052915 | Best: 0.00250975\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[  9.69015819e-04   9.99030948e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  6.49867475e-01   3.50132525e-01   1.00000000e+00   0.00000000e+00]]\n",
      "Minibatch total loss at step 7631: 0.021023 | Best: 0.00250975\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[  9.99757588e-01   2.42375594e-04   1.00000000e+00   0.00000000e+00]\n",
      " [  9.00221244e-03   9.90997791e-01   0.00000000e+00   1.00000000e+00]]\n",
      "Minibatch total loss at step 7644: 0.200777 | Best: 0.00250975\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[ 0.02696839  0.97303164  0.          1.        ]\n",
      " [ 0.01423316  0.98576689  0.          1.        ]]\n",
      "###-> Validation accuracy: 21.5 | Best: 80.5\n",
      "Minibatch total loss at step 7657: 0.280236 | Best: 0.00250975\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[  5.03601390e-04   9.99496341e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  3.85531746e-02   9.61446822e-01   0.00000000e+00   1.00000000e+00]]\n",
      "Minibatch total loss at step 7670: 0.004367 | Best: 0.00250975\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[  9.99999881e-01   9.55957375e-08   1.00000000e+00   0.00000000e+00]\n",
      " [  2.76087871e-04   9.99723852e-01   0.00000000e+00   1.00000000e+00]]\n",
      "Minibatch total loss at step 7683: 0.249032 | Best: 0.00250975\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[  3.71153583e-04   9.99628782e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  2.72107143e-02   9.72789288e-01   0.00000000e+00   1.00000000e+00]]\n",
      "Minibatch total loss at step 7696: 0.216713 | Best: 0.00250975\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[  2.51283240e-07   9.99999762e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  9.99999404e-01   6.16487227e-07   1.00000000e+00   0.00000000e+00]]\n",
      "###-> Validation accuracy: 21.5 | Best: 80.5\n",
      "Minibatch total loss at step 7709: 0.029147 | Best: 0.00250975\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[  6.20110892e-02   9.37988877e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  6.85197801e-06   9.99993205e-01   0.00000000e+00   1.00000000e+00]]\n",
      "Minibatch total loss at step 7722: 0.006087 | Best: 0.00250975\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[  4.20911419e-05   9.99957919e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  9.63808894e-01   3.61911468e-02   1.00000000e+00   0.00000000e+00]]\n",
      "Minibatch total loss at step 7735: 0.121840 | Best: 0.00250975\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[  1.68543909e-06   9.99998331e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  9.99130189e-01   8.69877869e-04   1.00000000e+00   0.00000000e+00]]\n",
      "Minibatch total loss at step 7748: 0.006384 | Best: 0.00250975\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[  3.88054505e-05   9.99961138e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  3.38656925e-09   1.00000000e+00   0.00000000e+00   1.00000000e+00]]\n",
      "###-> Validation accuracy: 21.5 | Best: 80.5\n",
      "Minibatch total loss at step 7761: 0.236129 | Best: 0.00250975\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[  9.91949677e-01   8.05032440e-03   1.00000000e+00   0.00000000e+00]\n",
      " [  9.99999642e-01   3.65966201e-07   1.00000000e+00   0.00000000e+00]]\n",
      "Minibatch total loss at step 7774: 0.172009 | Best: 0.00250975\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[  9.99350011e-01   6.49997324e-04   1.00000000e+00   0.00000000e+00]\n",
      " [  3.80002826e-01   6.19997144e-01   0.00000000e+00   1.00000000e+00]]\n",
      "Minibatch total loss at step 7787: 0.012918 | Best: 0.00250975\n",
      "Minibatch accuracy: 100.0\n",
      "Predictions | Labels:\n",
      " [[  9.96833622e-01   3.16642178e-03   1.00000000e+00   0.00000000e+00]\n",
      " [  1.80567968e-05   9.99981999e-01   0.00000000e+00   1.00000000e+00]]\n",
      "Minibatch total loss at step 7800: 0.181069 | Best: 0.00250975\n",
      "Minibatch accuracy: 87.5\n",
      "Predictions | Labels:\n",
      " [[  6.39879454e-06   9.99993563e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  9.97895837e-01   2.10419507e-03   1.00000000e+00   0.00000000e+00]]\n",
      "###-> Validation accuracy: 21.5 | Best: 80.5\n",
      "Minibatch total loss at step 7813: 0.121380 | Best: 0.00250975\n",
      "Minibatch accuracy: 93.75\n",
      "Predictions | Labels:\n",
      " [[ 0.11822316  0.88177687  0.          1.        ]\n",
      " [ 0.16513437  0.83486563  0.          1.        ]]\n"
     ]
    }
   ],
   "source": [
    "num_steps = 10001\n",
    "\n",
    "trace_file = open('./tracing/timeline.json', 'w')\n",
    "save_path = './checkpoints/model.ckpt'\n",
    "\n",
    "best_loss = 99.0\n",
    "val_accu = 0.0\n",
    "best_val_accu = 0.0\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    tf.initialize_local_variables().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_dataset.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels}\n",
    "        _, l, trprob, traccu, summary = session.run(\n",
    "            [optimizer, loss, train_probabilities, train_accuracy, merged_summs], \n",
    "            feed_dict=feed_dict)\n",
    "        results_writer.add_summary(summary, step)\n",
    "        if (step % 13 == 0):\n",
    "            best_loss = l if l < best_loss else best_loss\n",
    "            print('Minibatch total loss at step %d: %f' % (step, l), '| Best:', best_loss)\n",
    "            print('Minibatch accuracy:', traccu)\n",
    "            print('Predictions | Labels:\\n', np.concatenate((trprob[:2], batch_labels[:2]), axis=1))\n",
    "        if (step % 50 == 0):\n",
    "            val_accu = valid_accuracy.eval()\n",
    "            best_val_accu = val_accu if val_accu > best_val_accu else best_val_accu\n",
    "            print('###-> Validation accuracy:', val_accu, '| Best:', best_val_accu)\n",
    "            \n",
    "    # Save tracing into disl\n",
    "    #trace = timeline.Timeline(step_stats=run_metadata.step_stats)\n",
    "    #trace_file.write(trace.generate_chrome_trace_format(show_memory=True))\n",
    "            \n",
    "    # Save the variables to disk.\n",
    "    saver.save(session, save_path)\n",
    "    print(\"Model saved in file: %s\" % save_path)\n",
    "            \n",
    "    results_writer.flush()\n",
    "    results_writer.close()\n",
    "\n",
    "    print('Finished training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "valid_batch_size = 1\n",
    "\n",
    "def accuracy_notpercent(predictions, labels):\n",
    "  return np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    saver.restore(session, save_path)\n",
    "    print('Model Loaded')\n",
    "    data_split = np.array_split(valid_dataset, valid_dataset.shape[0]//valid_batch_size, axis=0)\n",
    "    labels_split = np.array_split(valid_labels, valid_labels.shape[0]//valid_batch_size, axis=0)\n",
    "    correct_predictions = 0\n",
    "    for idx, batch_data in enumerate(data_split):\n",
    "        correct_predictions += accuracy_notpercent(\n",
    "            train_prediction.eval(feed_dict={tf_train_dataset: batch_data}), \n",
    "            labels_split[idx])\n",
    "        print('accuracy:', (100.0*correct_predictions)/((idx+1)*valid_batch_size))\n",
    "        \n",
    "        \n",
    "    print('Finished validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
