{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "import os\n",
    "import glob\n",
    "from six.moves import cPickle as pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import timeline\n",
    "\n",
    "print(\"import done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load pickled dataset into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "name_pickle = './data/trainsh1_norm.pickle'\n",
    "\n",
    "with open(name_pickle, 'rb') as f:\n",
    "    print('Unpickling ' + name_pickle)\n",
    "    load = pickle.load(f)\n",
    "    dataset = load['data']\n",
    "    labels = load['labels']\n",
    "    del load\n",
    "    print('dataset shape:', dataset.shape)\n",
    "    print('labels shape:', labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reformat data for training\n",
    "- Divide each file with 240000 samples into smaller batch_samples ~= size of receptive field of eegnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Output size of the layer = 2: one for preictal and other for interictal\n",
    "num_labels = 2\n",
    "\n",
    "#60% for train and 40% for validation\n",
    "split_idx = int(dataset.shape[0]*0.8)\n",
    "#nr of splits\n",
    "nrOfSplits = 200\n",
    "\n",
    "def format_data(data, labels, nr_splits):\n",
    "    shape = data.shape\n",
    "    #stack 3D array into 2D\n",
    "    data = np.reshape(data, (shape[0]*shape[1], shape[2]))\n",
    "    #3D array from 2D array by splitting 2D array into the desired smaller chuncks\n",
    "    data = np.asarray(np.split(data, shape[0]*nr_splits, axis=0))\n",
    "    #data has to be 4D for tensorflow (insert an empty dimension)\n",
    "    data = data[:,None,:,:]\n",
    "    #labels are obtaining by repeating original labels nr_splits times\n",
    "    labels = np.repeat((np.arange(num_labels) == labels[:,None]).astype(np.float32), nr_splits, axis=0)\n",
    "    return data, labels\n",
    "    \n",
    "train_dataset, train_labels = format_data(dataset[:split_idx,:,:], labels[:split_idx], nrOfSplits)\n",
    "valid_dataset, valid_labels = format_data(dataset[split_idx:-1,:,:], labels[split_idx:-1], nrOfSplits)\n",
    "del dataset, labels\n",
    "\n",
    "print('train_dataset shape:', train_dataset.shape, 'train_labels shape:', train_labels.shape)\n",
    "print('valid_dataset shape:', valid_dataset.shape, 'valid_labels shape:', valid_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot some data to have an idea of how data looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.subplot(211)\n",
    "plt.plot(train_dataset[2,0,:,0])\n",
    "plt.plot(train_dataset[12,0,:,0])\n",
    "\n",
    "plt.figure(2)\n",
    "plt.subplot(211)\n",
    "idx = 4\n",
    "plt.plot(valid_dataset[idx,0,:,0])\n",
    "plt.plot(valid_dataset[idx,0,:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EEGNET implementation\n",
    "\n",
    "Ideas\n",
    "  - Condition classification based on sensor?\n",
    "  \n",
    "Part of https://arxiv.org/pdf/1609.03499.pdf that most concerns classification:\n",
    "\"As a last experiment we looked at speech recognition with WaveNets on the TIMIT (Garofolo et al., 1993) dataset. For this task we added a mean-pooling layer after the dilation convolutions that aggregated the activations to coarser frames spanning 10 milliseconds (160 x downsampling). The pooling layer was followed by a few non-causal convolutions. We trained WaveNet with two loss terms, one to predict the next sample and one to classify the frame, the model generalized better than with a single loss and achieved 18.8 PER on the test set, which is to our knowledge the best score obtained from a model trained directly on raw audio on TIMIT.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#How many files are supplied per batch.\n",
    "batch_size=14\n",
    "#Number of samples in each batch entry\n",
    "batch_samples=1200\n",
    "#A list with the dilation factor for each layer.\n",
    "dilations=[1, 2, 4, 8, 16, 32, 64, 128]\n",
    "#Samples in each convolution.\n",
    "filter_width=2\n",
    "#How many filters to learn for the input.\n",
    "input_channels=16\n",
    "#How many filters to learn for the residual.\n",
    "residual_channels=64\n",
    "#How many filters to learn for the dilation convolution.\n",
    "dilation_channels=32\n",
    "#How many filters to learn for the softmax output.\n",
    "skip_channels=64\n",
    "#Samples after pooling, equal to receptive field size\n",
    "size_after_pool=75 #approx. sum(dilations)\n",
    "#Hidden layer size for fully connected layer\n",
    "hidden_size=16\n",
    "#number of steps after which learning rate is decayed\n",
    "decay_steps=100\n",
    "\n",
    "#Construct computation graph\n",
    "graph = tf.Graph()\n",
    "\n",
    "#activation function\n",
    "actfunc = tf.nn.elu\n",
    "\n",
    "with graph.as_default():\n",
    "    \n",
    "    # Input data\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, 1, \n",
    "                                                         train_dataset.shape[2], train_dataset.shape[3]))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    #tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    \n",
    "    def weights_variable(name, shape):\n",
    "        initializer = tf.contrib.layers.xavier_initializer_conv2d()\n",
    "        return tf.Variable(initializer(shape=shape), name=name)\n",
    "    \n",
    "    def bias_variable(name, shape):\n",
    "        return tf.Variable(tf.zeros(shape=shape), name=name)\n",
    "    \n",
    "    #Variables\n",
    "    var = dict()\n",
    "    with tf.variable_scope('eegnet'):\n",
    "        var['causal'] = dict()\n",
    "        with tf.variable_scope('causal'):\n",
    "            var['causal']['wfilter'] = weights_variable(\"wfilter\", [1, filter_width, input_channels, \n",
    "                                                                    residual_channels])\n",
    "            var['causal']['bfilter'] = bias_variable(\"bfilter\", [residual_channels])\n",
    "            \n",
    "            tf.histogram_summary('causal_wfilter', var['causal']['wfilter'])\n",
    "            tf.histogram_summary('causal_bfilter', var['causal']['bfilter'])\n",
    "        \n",
    "        var['dilation'] = list()\n",
    "        with tf.variable_scope('dilation'):\n",
    "            for idx, dilation in enumerate(dilations):\n",
    "                with tf.variable_scope('layer{}'.format(idx)):\n",
    "                    var['dilation'].append(dict())\n",
    "                    #Filter\n",
    "                    var['dilation'][idx]['wfilter'] = weights_variable(\"wfilter\", [1, filter_width, \n",
    "                                                                                   residual_channels, \n",
    "                                                                                   dilation_channels])\n",
    "                    var['dilation'][idx]['bfilter'] = bias_variable(\"bfilter\", [dilation_channels])\n",
    "                    #Gate\n",
    "                    var['dilation'][idx]['wgate'] = weights_variable(\"wgate\", [1, filter_width, \n",
    "                                                                               residual_channels, \n",
    "                                                                               dilation_channels])\n",
    "                    var['dilation'][idx]['bgate'] = bias_variable(\"bgate\", [dilation_channels])    \n",
    "                    # 1x1 conv for dense contribution\n",
    "                    var['dilation'][idx]['wdense'] = weights_variable(\"wdense\", [1, 1, dilation_channels, \n",
    "                                                                                 residual_channels])\n",
    "                    var['dilation'][idx]['bdense'] = bias_variable(\"bdense\", [residual_channels])\n",
    "                    # 1x1 conv for skip contribution\n",
    "                    var['dilation'][idx]['wskip'] = weights_variable(\"wskip\", [1, 1, dilation_channels, \n",
    "                                                                               skip_channels])\n",
    "                    var['dilation'][idx]['bskip'] = bias_variable(\"bskip\", [skip_channels])\n",
    "                    \n",
    "                    tf.histogram_summary('dilation_{}'.format(idx) + '_wfilter', var['dilation'][idx]['wfilter'])\n",
    "                    tf.histogram_summary('dilation_{}'.format(idx) + '_bfilter', var['dilation'][idx]['bfilter'])\n",
    "                    tf.histogram_summary('dilation_{}'.format(idx) + '_wgate', var['dilation'][idx]['wgate'])\n",
    "                    tf.histogram_summary('dilation_{}'.format(idx) + '_bgate', var['dilation'][idx]['bgate'])\n",
    "                    tf.histogram_summary('dilation_{}'.format(idx) + '_wdense', var['dilation'][idx]['wdense'])\n",
    "                    tf.histogram_summary('dilation_{}'.format(idx) + '_bdense', var['dilation'][idx]['bdense'])\n",
    "                    tf.histogram_summary('dilation_{}'.format(idx) + '_wskip', var['dilation'][idx]['wskip'])\n",
    "                    tf.histogram_summary('dilation_{}'.format(idx) + '_bskip', var['dilation'][idx]['bskip'])\n",
    "            \n",
    "        var['skipsum'] = dict()\n",
    "        with tf.variable_scope('output'):\n",
    "            var['skipsum']['wsum'] = weights_variable(\"wsum\", [1, 1, skip_channels, skip_channels])\n",
    "            var['skipsum']['bsum'] = bias_variable(\"bsum\", [skip_channels])\n",
    "            \n",
    "            tf.histogram_summary('skipsum_wsum', var['skipsum']['wsum'])\n",
    "            tf.histogram_summary('skipsum_bsum', var['skipsum']['bsum'])\n",
    "            \n",
    "        var['output'] = dict()\n",
    "        with tf.variable_scope('output'):\n",
    "            var['output']['wfullcon'] = weights_variable('wfullcon', [size_after_pool * skip_channels, hidden_size])\n",
    "            var['output']['bfullcon'] = bias_variable('bfullcon', [hidden_size])\n",
    "            var['output']['wlabel'] = weights_variable('wlabel', [hidden_size, num_labels])\n",
    "            var['output']['blabel'] = bias_variable('wlabel', [num_labels])\n",
    "            \n",
    "            tf.histogram_summary('output_wfullcon', var['output']['wfullcon'])\n",
    "            tf.histogram_summary('output_bfullcon', var['output']['bfullcon'])\n",
    "            tf.histogram_summary('output_wlabel', var['output']['wlabel'])\n",
    "            tf.histogram_summary('output_blabel', var['output']['blabel'])\n",
    "\n",
    "            \n",
    "    def dilation_layer(batch_data, layer_idx, dilation):\n",
    "        '''Creates a single causal dilation convolution layer.\n",
    "        The layer contains a gated filter that connects to dense output\n",
    "        and to a skip connection:\n",
    "               |-> [gate]   -|        |-> 1x1 conv -> skip output\n",
    "               |             |-> (*) -| \n",
    "        input -|-> [filter] -|        |-> 1x1 conv -|\n",
    "               |                                    |-> (+) -> dense output\n",
    "               |------------------------------------|\n",
    "        Where `[gate]` and `[filter]` are causal convolutions with a\n",
    "        non-linear activation at the output.\n",
    "        '''\n",
    "        wfilter = var['dilation'][layer_idx]['wfilter']\n",
    "        bfilter = var['dilation'][layer_idx]['bfilter']\n",
    "        wgate = var['dilation'][layer_idx]['wgate']\n",
    "        bgate = var['dilation'][layer_idx]['bgate']\n",
    "        wdense = var['dilation'][layer_idx]['wdense']\n",
    "        bdense = var['dilation'][layer_idx]['bdense']\n",
    "        wskip = var['dilation'][layer_idx]['wskip']\n",
    "        bskip = var['dilation'][layer_idx]['bskip']\n",
    "        #Filter part\n",
    "        conv_filter = tf.nn.atrous_conv2d(batch_data, wfilter, dilation, padding='SAME', name='filter_atrous')\n",
    "        conv_filter = tf.tanh(tf.add(conv_filter, bfilter)) \n",
    "        #Gated part\n",
    "        conv_gate = tf.nn.atrous_conv2d(batch_data, wgate, dilation, padding='SAME', name='gate_atrous')\n",
    "        conv_gate = tf.sigmoid(tf.add(conv_gate, bgate))\n",
    "        # filter * gated output\n",
    "        hidden = tf.mul(conv_filter, conv_gate, name='filterXgate')\n",
    "        # The 1x1 conv to produce the dense contribution.\n",
    "        conv_dense = tf.nn.conv2d(hidden, wdense, [1, 1, 1, 1], padding=\"SAME\", name=\"dense_1x1conv\")\n",
    "        conv_dense = tf.add(tf.add(conv_dense, bdense), batch_data) # add input back (residual part)\n",
    "        # The 1x1 conv to produce the skip contribution.\n",
    "        conv_skip = tf.nn.conv2d(hidden, wskip, [1, 1, 1, 1], padding=\"SAME\", name=\"skip_1x1conv\")\n",
    "        # output to final decision softmax layer\n",
    "        conv_skip = tf.add(conv_skip, bskip)\n",
    "\n",
    "        return conv_skip, conv_dense\n",
    "     \n",
    "        \n",
    "    def network(batch_data):\n",
    "        outputs = tf.Variable(tf.zeros(shape=[batch_size, 1, train_dataset.shape[2], skip_channels]), \n",
    "                              dtype=tf.float32, trainable=False)\n",
    "        current_layer = batch_data\n",
    "\n",
    "        # Process input with causal convolution layer\n",
    "        with tf.name_scope('causal_layer'):\n",
    "            current_layer = tf.nn.conv2d(current_layer, var['causal']['wfilter'], [1, 1, 1, 1], padding='SAME')\n",
    "            current_layer = tf.add(current_layer, var['causal']['bfilter'])\n",
    "\n",
    "        # Process with dilation layers\n",
    "        with tf.name_scope('dilation_stack'):\n",
    "            tf.assign(outputs, tf.zeros(shape=[batch_size, 1, train_dataset.shape[2], skip_channels]))\n",
    "            for layer_idx, dilation in enumerate(dilations):\n",
    "                with tf.name_scope('layer{}'.format(layer_idx)):\n",
    "                    output, current_layer = dilation_layer(current_layer, layer_idx, dilation)\n",
    "                    outputs = tf.add(outputs, output)\n",
    "        \n",
    "        # Sum and process skip outputs from all dilation layers\n",
    "        with tf.name_scope('skipsum'):\n",
    "            hidden = actfunc(outputs)\n",
    "            hidden = tf.nn.conv2d(hidden, var['skipsum']['wsum'], [1, 1, 1, 1], padding=\"SAME\")\n",
    "            hidden = actfunc(tf.add(hidden, var['skipsum']['bsum']))\n",
    "        \n",
    "        # Dimension reduction and Fully Connected layer for classification output\n",
    "        with tf.name_scope('output'):\n",
    "            hidden = tf.nn.avg_pool(hidden, [1, 1, batch_samples//size_after_pool, 1], \n",
    "                                    [1, 1, batch_samples//size_after_pool, 1], padding='SAME')\n",
    "            shape = hidden.get_shape().as_list()\n",
    "            reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "            hidden = tf.matmul(reshape, var['output']['wfullcon'])\n",
    "            hidden = actfunc(tf.add(hidden, var['output']['bfullcon']))\n",
    "            return tf.add(tf.matmul(hidden, var['output']['wlabel']), var['output']['blabel'])\n",
    "        \n",
    "\n",
    "    with tf.name_scope('eegnet'):\n",
    "        logits = network(tf_train_dataset)\n",
    "        with tf.name_scope('loss'):\n",
    "            loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "            tf.scalar_summary('loss', loss)\n",
    "        with tf.name_scope('optimizer'):\n",
    "            global_step = tf.Variable(0)\n",
    "            learning_rate = tf.train.exponential_decay(1e-9, global_step, decay_steps, 0.96, staircase=True)\n",
    "            #optimizer = tf.train.MomentumOptimizer(learning_rate, 0.9).minimize(loss, global_step=global_step)\n",
    "            optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "            tf.scalar_summary('learning_rate', learning_rate)\n",
    "        with tf.name_scope('predictions'):\n",
    "            train_prediction = tf.nn.softmax(logits)\n",
    "            #valid_prediction = tf.nn.softmax(network(tf_valid_dataset))\n",
    "            \n",
    "    #Merge all summaries and write to a folder\n",
    "    merged = tf.merge_all_summaries()\n",
    "    results_writer = tf.train.SummaryWriter('./results', graph)\n",
    "    \n",
    "    # Add ops to save and restore all the variables.\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    #tracing for timeline\n",
    "    run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "    run_metadata = tf.RunMetadata()    \n",
    "    \n",
    "\n",
    "print('computational graph created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_steps = 1600\n",
    "\n",
    "trace_file = open('./tracing/timeline.json', 'w')\n",
    "save_path = './checkpoints/model.ckpt'\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_dataset.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels}\n",
    "        #_, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        _, l, predictions, summary = session.run(\n",
    "            [optimizer, loss, train_prediction, merged], \n",
    "            feed_dict=feed_dict, \n",
    "            options=run_options,\n",
    "            run_metadata=run_metadata)\n",
    "        results_writer.add_summary(summary, step)\n",
    "        if (step % 5 == 0):\n",
    "            print('Minibatch loss at step %d: %f' % (step, l))\n",
    "            print('Minibatch accuracy: %f' % accuracy(predictions, batch_labels))\n",
    "            #print('Validation accuracy: %.1f%%' % accuracy(valid_prediction.eval(), valid_labels))\n",
    "        if (step % 200 == 0) and (step != 0):\n",
    "            trace = timeline.Timeline(step_stats=run_metadata.step_stats)\n",
    "            trace_file.write(trace.generate_chrome_trace_format(show_memory=True))\n",
    "            \n",
    "    # Save the variables to disk.\n",
    "    saver.save(session, save_path)\n",
    "    print(\"Model saved in file: %s\" % save_path)\n",
    "            \n",
    "    results_writer.flush()\n",
    "    results_writer.close()\n",
    "\n",
    "    print('Finished training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "valid_batch_size = 14\n",
    "\n",
    "def accuracy_notpercent(predictions, labels):\n",
    "  return np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    saver.restore(session, save_path)\n",
    "    print('Model Loaded')\n",
    "    data_split = np.array_split(valid_dataset, valid_dataset.shape[0]//valid_batch_size, axis=0)\n",
    "    labels_split = np.array_split(valid_labels, valid_labels.shape[0]//valid_batch_size, axis=0)\n",
    "    correct_predictions = 0\n",
    "    for idx, batch_data in enumerate(data_split):\n",
    "        correct_predictions += accuracy_notpercent(\n",
    "            train_prediction.eval(feed_dict={tf_train_dataset: batch_data}), \n",
    "            labels_split[idx])\n",
    "        print('accuracy:', (100.0*correct_predictions)/((idx+1)*valid_batch_size))\n",
    "        \n",
    "        \n",
    "    print('Finished validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
